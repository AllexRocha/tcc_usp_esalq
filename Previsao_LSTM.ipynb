{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "073d0634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py==2.1.0\n",
      "adal==1.2.7\n",
      "aiodns==3.2.0\n",
      "aiohttp @ file:///C:/ci/aiohttp_1646806572557/work\n",
      "aiohttp-socks==0.8.4\n",
      "aiosignal @ file:///tmp/build/80754af9/aiosignal_1637843061372/work\n",
      "alabaster @ file:///home/ktietz/src/ci/alabaster_1611921544520/work\n",
      "alembic==1.13.1\n",
      "anaconda-client @ file:///C:/ci/anaconda-client_1635342725944/work\n",
      "anaconda-navigator==2.1.4\n",
      "anaconda-project @ file:///tmp/build/80754af9/anaconda-project_1637161053845/work\n",
      "anyio @ file:///C:/ci/anyio_1644481921011/work/dist\n",
      "appdirs==1.4.4\n",
      "argon2-cffi @ file:///opt/conda/conda-bld/argon2-cffi_1645000214183/work\n",
      "argon2-cffi-bindings @ file:///C:/ci/argon2-cffi-bindings_1644551690056/work\n",
      "arrow @ file:///opt/conda/conda-bld/arrow_1649166651673/work\n",
      "astroid @ file:///C:/ci/astroid_1628063282661/work\n",
      "astropy @ file:///C:/ci/astropy_1650634291321/work\n",
      "asttokens @ file:///opt/conda/conda-bld/asttokens_1646925590279/work\n",
      "astunparse==1.6.3\n",
      "async-timeout @ file:///tmp/build/80754af9/async-timeout_1637851218186/work\n",
      "atomicwrites==1.4.0\n",
      "attrs @ file:///opt/conda/conda-bld/attrs_1642510447205/work\n",
      "Automat @ file:///tmp/build/80754af9/automat_1600298431173/work\n",
      "autopep8 @ file:///opt/conda/conda-bld/autopep8_1639166893812/work\n",
      "Babel @ file:///tmp/build/80754af9/babel_1620871417480/work\n",
      "backcall @ file:///home/ktietz/src/ci/backcall_1611930011877/work\n",
      "backports.functools-lru-cache @ file:///tmp/build/80754af9/backports.functools_lru_cache_1618170165463/work\n",
      "backports.tempfile @ file:///home/linux1/recipes/ci/backports.tempfile_1610991236607/work\n",
      "backports.weakref==1.0.post1\n",
      "Note: you may need to restart the kernel to use updated packages.bcrypt @ file:///C:/ci/bcrypt_1607022693089/work\n",
      "beautifulsoup4 @ file:///C:/ci/beautifulsoup4_1650293025093/work\n",
      "binaryornot @ file:///tmp/build/80754af9/binaryornot_1617751525010/work\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\alex\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bitarray @ file:///C:/ci/bitarray_1648739663053/work\n",
      "bkcharts==0.2\n",
      "black==19.10b0\n",
      "bleach @ file:///opt/conda/conda-bld/bleach_1641577558959/work\n",
      "bokeh @ file:///C:/ci/bokeh_1638362966927/work\n",
      "boto3 @ file:///opt/conda/conda-bld/boto3_1649078879353/work\n",
      "botocore @ file:///opt/conda/conda-bld/botocore_1649076662316/work\n",
      "Bottleneck @ file:///C:/ci/bottleneck_1648010904582/work\n",
      "branca==0.7.2\n",
      "brotlipy==0.7.0\n",
      "cachetools @ file:///tmp/build/80754af9/cachetools_1619597386817/work\n",
      "cchardet==2.1.7\n",
      "certifi==2021.10.8\n",
      "cffi @ file:///C:/ci_310/cffi_1642682485096/work\n",
      "chardet @ file:///C:/ci/chardet_1607706937985/work\n",
      "charset-normalizer @ file:///tmp/build/80754af9/charset-normalizer_1630003229654/work\n",
      "click @ file:///C:/ci/click_1646038595831/work\n",
      "cloudpickle @ file:///tmp/build/80754af9/cloudpickle_1632508026186/work\n",
      "clyent==1.2.2\n",
      "cmdstanpy==1.2.0\n",
      "colorama @ file:///tmp/build/80754af9/colorama_1607707115595/work\n",
      "colorcet @ file:///tmp/build/80754af9/colorcet_1611168489822/work\n",
      "colorlog==6.8.2\n",
      "comtypes==1.1.10\n",
      "conda==4.12.0\n",
      "conda-build==3.21.8\n",
      "conda-content-trust @ file:///tmp/build/80754af9/conda-content-trust_1617045594566/work\n",
      "conda-pack @ file:///tmp/build/80754af9/conda-pack_1611163042455/work\n",
      "conda-package-handling @ file:///C:/ci/conda-package-handling_1649106011304/work\n",
      "conda-repo-cli @ file:///tmp/build/80754af9/conda-repo-cli_1620168426516/work\n",
      "conda-token @ file:///tmp/build/80754af9/conda-token_1620076980546/work\n",
      "conda-verify==3.4.2\n",
      "constantly==15.1.0\n",
      "cookiecutter @ file:///opt/conda/conda-bld/cookiecutter_1649151442564/work\n",
      "coreforecast==0.0.8\n",
      "cryptography @ file:///C:/ci/cryptography_1633520531101/work\n",
      "cssselect==1.1.0\n",
      "cycler @ file:///tmp/build/80754af9/cycler_1637851556182/work\n",
      "Cython @ file:///C:/ci/cython_1647850559892/work\n",
      "cytoolz==0.11.0\n",
      "daal4py==2021.5.0\n",
      "dask @ file:///opt/conda/conda-bld/dask-core_1647268715755/work\n",
      "datashader @ file:///tmp/build/80754af9/datashader_1623782308369/work\n",
      "datashape==0.5.4\n",
      "DateTime==5.2\n",
      "debugpy @ file:///C:/ci/debugpy_1637091961445/work\n",
      "decorator @ file:///opt/conda/conda-bld/decorator_1643638310831/work\n",
      "defusedxml @ file:///tmp/build/80754af9/defusedxml_1615228127516/work\n",
      "diff-match-patch @ file:///Users/ktietz/demo/mc3/conda-bld/diff-match-patch_1630511840874/work\n",
      "distributed @ file:///opt/conda/conda-bld/distributed_1647271944416/work\n",
      "docutils @ file:///C:/ci/docutils_1620828264669/work\n",
      "elastic-transport==8.13.1\n",
      "elasticsearch==8.14.0\n",
      "entrypoints @ file:///C:/ci/entrypoints_1649926621128/work\n",
      "ephem==4.1.5\n",
      "et-xmlfile==1.1.0\n",
      "executing @ file:///opt/conda/conda-bld/executing_1646925071911/work\n",
      "fake-useragent==1.5.1\n",
      "fastjsonschema @ file:///tmp/build/80754af9/python-fastjsonschema_1620414857593/work/dist\n",
      "filelock @ file:///opt/conda/conda-bld/filelock_1647002191454/work\n",
      "flake8 @ file:///tmp/build/80754af9/flake8_1620776156532/work\n",
      "Flask @ file:///home/ktietz/src/ci/flask_1611932660458/work\n",
      "flatbuffers==24.3.25\n",
      "folium==0.17.0\n",
      "fonttools==4.25.0\n",
      "frozendict==2.3.10\n",
      "frozenlist @ file:///C:/ci/frozenlist_1637767271796/work\n",
      "fsspec==2024.3.1\n",
      "future @ file:///C:/ci/future_1607568713721/work\n",
      "gast==0.5.4\n",
      "gensim @ file:///C:/ci/gensim_1646825438310/work\n",
      "geographiclib==2.0\n",
      "geopy==2.4.1\n",
      "glob2 @ file:///home/linux1/recipes/ci/glob2_1610991677669/work\n",
      "google-api-core @ file:///C:/ci/google-api-core-split_1613980333946/work\n",
      "google-auth @ file:///tmp/build/80754af9/google-auth_1626320605116/work\n",
      "google-cloud-core @ file:///tmp/build/80754af9/google-cloud-core_1625077425256/work\n",
      "google-cloud-storage @ file:///tmp/build/80754af9/google-cloud-storage_1601307969662/work\n",
      "google-crc32c @ file:///C:/ci/google-crc32c_1613234249694/work\n",
      "google-pasta==0.2.0\n",
      "google-resumable-media @ file:///tmp/build/80754af9/google-resumable-media_1624367812531/work\n",
      "googleapis-common-protos @ file:///C:/ci/googleapis-common-protos-feedstock_1617957814607/work\n",
      "googletransx==2.4.2\n",
      "greenlet @ file:///C:/ci/greenlet_1628888275363/work\n",
      "grpcio==1.62.2\n",
      "h5py==3.11.0\n",
      "HeapDict @ file:///Users/ktietz/demo/mc3/conda-bld/heapdict_1630598515714/work\n",
      "holidays==0.38\n",
      "holoviews @ file:///opt/conda/conda-bld/holoviews_1645454331194/work\n",
      "html5lib==1.1\n",
      "hvplot @ file:///tmp/build/80754af9/hvplot_1627305124151/work\n",
      "hyperlink @ file:///tmp/build/80754af9/hyperlink_1610130746837/work\n",
      "idna @ file:///tmp/build/80754af9/idna_1637925883363/work\n",
      "imagecodecs @ file:///C:/ci/imagecodecs_1635511087451/work\n",
      "imageio @ file:///tmp/build/80754af9/imageio_1617700267927/work\n",
      "imagesize @ file:///tmp/build/80754af9/imagesize_1637939814114/work\n",
      "importlib-metadata @ file:///C:/ci/importlib-metadata_1648562621412/work\n",
      "importlib-resources==6.1.1\n",
      "incremental @ file:///tmp/build/80754af9/incremental_1636629750599/work\n",
      "inflection==0.5.1\n",
      "iniconfig @ file:///home/linux1/recipes/ci/iniconfig_1610983019677/work\n",
      "intake @ file:///opt/conda/conda-bld/intake_1647436631684/work\n",
      "intel-openmp==2021.4.0\n",
      "intervaltree @ file:///Users/ktietz/demo/mc3/conda-bld/intervaltree_1630511889664/work\n",
      "ipykernel @ file:///C:/ci/ipykernel_1646982785443/work/dist/ipykernel-6.9.1-py3-none-any.whl\n",
      "ipython @ file:///C:/ci/ipython_1648817223581/work\n",
      "ipython-genutils @ file:///tmp/build/80754af9/ipython_genutils_1606773439826/work\n",
      "ipywidgets @ file:///tmp/build/80754af9/ipywidgets_1634143127070/work\n",
      "isort @ file:///tmp/build/80754af9/isort_1628603791788/work\n",
      "itemadapter @ file:///tmp/build/80754af9/itemadapter_1626442940632/work\n",
      "itemloaders @ file:///opt/conda/conda-bld/itemloaders_1646805235997/work\n",
      "itsdangerous @ file:///tmp/build/80754af9/itsdangerous_1621432558163/work\n",
      "jdcal @ file:///Users/ktietz/demo/mc3/conda-bld/jdcal_1630584345063/work\n",
      "jedi @ file:///C:/ci/jedi_1644315428289/work\n",
      "Jinja2==3.1.4\n",
      "jinja2-time @ file:///opt/conda/conda-bld/jinja2-time_1649251842261/work\n",
      "jmespath @ file:///Users/ktietz/demo/mc3/conda-bld/jmespath_1630583964805/work\n",
      "joblib @ file:///tmp/build/80754af9/joblib_1635411271373/work\n",
      "json5 @ file:///tmp/build/80754af9/json5_1624432770122/work\n",
      "jsonschema @ file:///C:/ci/jsonschema_1650008058050/work\n",
      "jupyter @ file:///C:/ci/jupyter_1607685287094/work\n",
      "jupyter-client @ file:///tmp/build/80754af9/jupyter_client_1616770841739/work\n",
      "jupyter-console @ file:///tmp/build/80754af9/jupyter_console_1616615302928/work\n",
      "jupyter-core @ file:///C:/ci/jupyter_core_1646994619043/work\n",
      "jupyter-server @ file:///opt/conda/conda-bld/jupyter_server_1644494914632/work\n",
      "jupyterlab @ file:///opt/conda/conda-bld/jupyterlab_1647445413472/work\n",
      "jupyterlab-pygments @ file:///tmp/build/80754af9/jupyterlab_pygments_1601490720602/work\n",
      "jupyterlab-server @ file:///opt/conda/conda-bld/jupyterlab_server_1644500396812/work\n",
      "jupyterlab-widgets @ file:///tmp/build/80754af9/jupyterlab_widgets_1609884341231/work\n",
      "keras==3.3.3\n",
      "keyring @ file:///C:/ci/keyring_1638531673471/work\n",
      "kiwisolver @ file:///C:/ci/kiwisolver_1644962577370/work\n",
      "lazy-object-proxy @ file:///C:/ci/lazy-object-proxy_1616529288960/work\n",
      "libarchive-c @ file:///tmp/build/80754af9/python-libarchive-c_1617780486945/work\n",
      "libclang==18.1.1\n",
      "lightning-utilities==0.11.2\n",
      "llvmlite==0.38.0\n",
      "locket @ file:///C:/ci/locket_1647006279389/work\n",
      "lxml==4.9.3\n",
      "Mako==1.3.3\n",
      "Markdown @ file:///C:/ci/markdown_1614364082838/work\n",
      "markdown-it-py==3.0.0\n",
      "MarkupSafe @ file:///C:/ci/markupsafe_1621528502553/work\n",
      "matplotlib @ file:///C:/ci/matplotlib-suite_1647423638658/work\n",
      "matplotlib-inline @ file:///tmp/build/80754af9/matplotlib-inline_1628242447089/work\n",
      "mccabe==0.6.1\n",
      "mdurl==0.1.2\n",
      "menuinst @ file:///C:/ci/menuinst_1631733438520/work\n",
      "mistune @ file:///C:/ci/mistune_1607359457024/work\n",
      "mkl==2021.4.0\n",
      "mkl-fft==1.3.1\n",
      "mkl-random @ file:///C:/ci/mkl_random_1626186184308/work\n",
      "mkl-service==2.4.0\n",
      "ml-dtypes==0.3.2\n",
      "mlxtend==0.20.0\n",
      "mock @ file:///tmp/build/80754af9/mock_1607622725907/work\n",
      "mpmath==1.2.1\n",
      "msal==1.29.0\n",
      "msgpack @ file:///C:/ci/msgpack-python_1612287350784/work\n",
      "multidict @ file:///C:/ci/multidict_1607349747897/work\n",
      "multipledispatch @ file:///C:/ci/multipledispatch_1607574329826/work\n",
      "multitasking==0.0.11\n",
      "munkres==1.1.4\n",
      "mypy-extensions==0.4.3\n",
      "namex==0.0.8\n",
      "navigator-updater==0.2.1\n",
      "nbclassic @ file:///opt/conda/conda-bld/nbclassic_1644943264176/work\n",
      "nbclient @ file:///C:/ci/nbclient_1650290387259/work\n",
      "nbconvert @ file:///C:/ci/nbconvert_1649741016669/work\n",
      "nbformat @ file:///C:/ci/nbformat_1649845125000/work\n",
      "nest-asyncio @ file:///C:/ci/nest-asyncio_1649829929390/work\n",
      "networkx @ file:///opt/conda/conda-bld/networkx_1647437648384/work\n",
      "neuralforecast==1.7.1\n",
      "nltk @ file:///opt/conda/conda-bld/nltk_1645628263994/work\n",
      "nose @ file:///opt/conda/conda-bld/nose_1642704612149/work\n",
      "notebook @ file:///C:/ci/notebook_1645002729033/work\n",
      "numba @ file:///C:/ci/numba_1650394399948/work\n",
      "numexpr @ file:///C:/ci/numexpr_1640704337920/work\n",
      "numpy==1.26.4\n",
      "numpydoc @ file:///opt/conda/conda-bld/numpydoc_1643788541039/work\n",
      "oauthlib==3.2.2\n",
      "olefile @ file:///Users/ktietz/demo/mc3/conda-bld/olefile_1629805411829/work\n",
      "openpyxl @ file:///tmp/build/80754af9/openpyxl_1632777717936/work\n",
      "opt-einsum==3.3.0\n",
      "optree==0.11.0\n",
      "optuna==3.6.1\n",
      "packaging @ file:///tmp/build/80754af9/packaging_1637314298585/work\n",
      "pandas @ file:///C:/ci/pandas_1650373458095/work\n",
      "pandas-datareader==0.10.0\n",
      "pandocfilters @ file:///opt/conda/conda-bld/pandocfilters_1643405455980/work\n",
      "panel @ file:///C:/ci/panel_1650623703033/work\n",
      "param @ file:///tmp/build/80754af9/param_1636647414893/work\n",
      "paramiko @ file:///opt/conda/conda-bld/paramiko_1640109032755/work\n",
      "parsel @ file:///C:/ci/parsel_1646740216444/work\n",
      "parso @ file:///opt/conda/conda-bld/parso_1641458642106/work\n",
      "partd @ file:///opt/conda/conda-bld/partd_1647245470509/work\n",
      "pathspec==0.7.0\n",
      "patsy==0.5.2\n",
      "peewee==3.17.0\n",
      "pep8==1.7.1\n",
      "pexpect @ file:///tmp/build/80754af9/pexpect_1605563209008/work\n",
      "pickleshare @ file:///tmp/build/80754af9/pickleshare_1606932040724/work\n",
      "Pillow==9.0.1\n",
      "pkginfo @ file:///tmp/build/80754af9/pkginfo_1643162084911/work\n",
      "plotly==5.14.1\n",
      "pluggy @ file:///C:/ci/pluggy_1648024580010/work\n",
      "pmdarima==2.0.4\n",
      "poyo @ file:///tmp/build/80754af9/poyo_1617751526755/work\n",
      "prometheus-client @ file:///opt/conda/conda-bld/prometheus_client_1643788673601/work\n",
      "prompt-toolkit @ file:///tmp/build/80754af9/prompt-toolkit_1633440160888/work\n",
      "prophet==1.1.5\n",
      "Protego @ file:///tmp/build/80754af9/protego_1598657180827/work\n",
      "protobuf==4.25.3\n",
      "psutil @ file:///C:/ci/psutil_1612298199233/work\n",
      "ptyprocess @ file:///tmp/build/80754af9/ptyprocess_1609355006118/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl\n",
      "pure-eval @ file:///opt/conda/conda-bld/pure_eval_1646925070566/work\n",
      "py @ file:///opt/conda/conda-bld/py_1644396412707/work\n",
      "pyaml==23.12.0\n",
      "pyarrow==16.0.0\n",
      "pyasn1 @ file:///Users/ktietz/demo/mc3/conda-bld/pyasn1_1629708007385/work\n",
      "pyasn1-modules==0.2.8\n",
      "pycares==4.4.0\n",
      "pycodestyle @ file:///tmp/build/80754af9/pycodestyle_1615748559966/work\n",
      "pycosat==0.6.3\n",
      "pycparser @ file:///tmp/build/80754af9/pycparser_1636541352034/work\n",
      "pyct @ file:///C:/ci/pyct_1613411728548/work\n",
      "pycurl==7.44.1\n",
      "PyDispatcher==2.0.5\n",
      "pydocstyle @ file:///tmp/build/80754af9/pydocstyle_1621600989141/work\n",
      "pyerfa @ file:///C:/ci/pyerfa_1621560974055/work\n",
      "pyflakes @ file:///tmp/build/80754af9/pyflakes_1617200973297/work\n",
      "Pygments==2.17.2\n",
      "PyHamcrest @ file:///tmp/build/80754af9/pyhamcrest_1615748656804/work\n",
      "PyJWT @ file:///C:/ci/pyjwt_1619682721924/work\n",
      "pylint @ file:///C:/ci/pylint_1627536884966/work\n",
      "pyls-spyder==0.4.0\n",
      "PyMeeus==0.5.12\n",
      "PyNaCl @ file:///C:/ci/pynacl_1607612759007/work\n",
      "pyodbc @ file:///C:/ci/pyodbc_1647426110990/work\n",
      "pyOpenSSL @ file:///tmp/build/80754af9/pyopenssl_1635333100036/work\n",
      "pyparsing @ file:///tmp/build/80754af9/pyparsing_1635766073266/work\n",
      "pyreadline==2.1\n",
      "pyrsistent @ file:///C:/ci/pyrsistent_1636093225342/work\n",
      "PySocks @ file:///C:/ci/pysocks_1605307512533/work\n",
      "pytest==7.1.1\n",
      "python-dateutil @ file:///tmp/build/80754af9/python-dateutil_1626374649649/work\n",
      "python-lsp-black @ file:///tmp/build/80754af9/python-lsp-black_1634232156041/work\n",
      "python-lsp-jsonrpc==1.0.0\n",
      "python-lsp-server==1.2.4\n",
      "python-msgraph==0.2.8\n",
      "python-slugify @ file:///tmp/build/80754af9/python-slugify_1620405669636/work\n",
      "python-snappy @ file:///C:/ci/python-snappy_1610133405910/work\n",
      "python-socks==2.5.0\n",
      "pytorch-lightning==2.2.3\n",
      "pytz==2023.3.post1\n",
      "pyviz-comms @ file:///tmp/build/80754af9/pyviz_comms_1623747165329/work\n",
      "PyWavelets @ file:///C:/ci/pywavelets_1648728084106/work\n",
      "pywin32==302\n",
      "pywin32-ctypes @ file:///C:/ci/pywin32-ctypes_1607553594546/work\n",
      "pywinpty @ file:///C:/ci_310/pywinpty_1644230983541/work/target/wheels/pywinpty-2.0.2-cp39-none-win_amd64.whl\n",
      "PyYAML==6.0\n",
      "pyzmq @ file:///C:/ci/pyzmq_1638435148211/work\n",
      "QDarkStyle @ file:///tmp/build/80754af9/qdarkstyle_1617386714626/work\n",
      "qstylizer @ file:///tmp/build/80754af9/qstylizer_1617713584600/work/dist/qstylizer-0.1.10-py2.py3-none-any.whl\n",
      "QtAwesome @ file:///tmp/build/80754af9/qtawesome_1637160816833/work\n",
      "qtconsole @ file:///opt/conda/conda-bld/qtconsole_1649078897110/work\n",
      "QtPy @ file:///opt/conda/conda-bld/qtpy_1649073884068/work\n",
      "queuelib==1.5.0\n",
      "ray==2.10.0\n",
      "regex @ file:///C:/ci/regex_1648447888413/work\n",
      "requests==2.31.0\n",
      "requests-file @ file:///Users/ktietz/demo/mc3/conda-bld/requests-file_1629455781986/work\n",
      "requests-oauthlib==1.3.1\n",
      "rich==13.7.1\n",
      "rope @ file:///opt/conda/conda-bld/rope_1643788605236/work\n",
      "rsa @ file:///tmp/build/80754af9/rsa_1614366226499/work\n",
      "Rtree @ file:///C:/ci/rtree_1618421015405/work\n",
      "ruamel-yaml-conda @ file:///C:/ci/ruamel_yaml_1616016898638/work\n",
      "s3transfer @ file:///tmp/build/80754af9/s3transfer_1626435152308/work\n",
      "schedule==1.2.2\n",
      "scikit-image @ file:///C:/ci/scikit-image_1648214340990/work\n",
      "scikit-learn @ file:///C:/ci/scikit-learn_1642617276183/work\n",
      "scikit-learn-intelex==2021.20220215.102710\n",
      "scikit-optimize==0.9.0\n",
      "scipy @ file:///C:/ci/scipy_1641555170412/work\n",
      "Scrapy @ file:///C:/ci/scrapy_1646837986255/work\n",
      "seaborn @ file:///tmp/build/80754af9/seaborn_1629307859561/work\n",
      "selenium @ file:///C:/ci/selenium_1614268120390/work\n",
      "Send2Trash @ file:///tmp/build/80754af9/send2trash_1632406701022/work\n",
      "service-identity @ file:///Users/ktietz/demo/mc3/conda-bld/service_identity_1629460757137/work\n",
      "setuptools-git==1.2\n",
      "sip==4.19.13\n",
      "six @ file:///tmp/build/80754af9/six_1644875935023/work\n",
      "smart-open @ file:///tmp/build/80754af9/smart_open_1623928409369/work\n",
      "sniffio @ file:///C:/ci/sniffio_1614030527509/work\n",
      "snowballstemmer @ file:///tmp/build/80754af9/snowballstemmer_1637937080595/work\n",
      "sortedcollections @ file:///tmp/build/80754af9/sortedcollections_1611172717284/work\n",
      "sortedcontainers @ file:///tmp/build/80754af9/sortedcontainers_1623949099177/work\n",
      "soupsieve @ file:///tmp/build/80754af9/soupsieve_1636706018808/work\n",
      "Sphinx @ file:///opt/conda/conda-bld/sphinx_1643644169832/work\n",
      "sphinxcontrib-applehelp @ file:///home/ktietz/src/ci/sphinxcontrib-applehelp_1611920841464/work\n",
      "sphinxcontrib-devhelp @ file:///home/ktietz/src/ci/sphinxcontrib-devhelp_1611920923094/work\n",
      "sphinxcontrib-htmlhelp @ file:///tmp/build/80754af9/sphinxcontrib-htmlhelp_1623945626792/work\n",
      "sphinxcontrib-jsmath @ file:///home/ktietz/src/ci/sphinxcontrib-jsmath_1611920942228/work\n",
      "sphinxcontrib-qthelp @ file:///home/ktietz/src/ci/sphinxcontrib-qthelp_1611921055322/work\n",
      "sphinxcontrib-serializinghtml @ file:///tmp/build/80754af9/sphinxcontrib-serializinghtml_1624451540180/work\n",
      "spyder @ file:///C:/ci/spyder_1636480369575/work\n",
      "spyder-kernels @ file:///C:/ci/spyder-kernels_1634237096710/work\n",
      "SQLAlchemy @ file:///C:/ci/sqlalchemy_1647600017103/work\n",
      "stack-data @ file:///opt/conda/conda-bld/stack_data_1646927590127/work\n",
      "stanio==0.3.0\n",
      "statsmodels==0.14.0\n",
      "sympy @ file:///C:/ci/sympy_1647853873858/work\n",
      "tables==3.6.1\n",
      "tabulate==0.8.9\n",
      "tbb==2021.12.0\n",
      "tblib @ file:///Users/ktietz/demo/mc3/conda-bld/tblib_1629402031467/work\n",
      "tenacity @ file:///C:/ci/tenacity_1626248381338/work\n",
      "tensorboard==2.16.2\n",
      "tensorboard-data-server==0.7.2\n",
      "tensorboardX==2.6.2.2\n",
      "tensorflow==2.16.1\n",
      "tensorflow-intel==2.16.1\n",
      "tensorflow-io-gcs-filesystem==0.31.0\n",
      "termcolor==2.4.0\n",
      "terminado @ file:///C:/ci/terminado_1644322780199/work\n",
      "testpath @ file:///tmp/build/80754af9/testpath_1624638946665/work\n",
      "text-unidecode @ file:///Users/ktietz/demo/mc3/conda-bld/text-unidecode_1629401354553/work\n",
      "textdistance @ file:///tmp/build/80754af9/textdistance_1612461398012/work\n",
      "threadpoolctl @ file:///Users/ktietz/demo/mc3/conda-bld/threadpoolctl_1629802263681/work\n",
      "three-merge @ file:///tmp/build/80754af9/three-merge_1607553261110/work\n",
      "tifffile @ file:///tmp/build/80754af9/tifffile_1627275862826/work\n",
      "tinycss @ file:///tmp/build/80754af9/tinycss_1617713798712/work\n",
      "tldextract @ file:///opt/conda/conda-bld/tldextract_1646638314385/work\n",
      "toml @ file:///tmp/build/80754af9/toml_1616166611790/work\n",
      "tomli @ file:///tmp/build/80754af9/tomli_1637314251069/work\n",
      "toolz @ file:///tmp/build/80754af9/toolz_1636545406491/work\n",
      "torch==2.3.0\n",
      "torchmetrics==1.3.2\n",
      "tornado @ file:///C:/ci/tornado_1606924294691/work\n",
      "tqdm @ file:///C:/ci/tqdm_1650636210717/work\n",
      "traitlets @ file:///tmp/build/80754af9/traitlets_1636710298902/work\n",
      "tweepy==3.7.0\n",
      "twint==2.1.20\n",
      "Twisted @ file:///C:/ci/twisted_1646835413846/work\n",
      "twisted-iocpsupport @ file:///C:/ci/twisted-iocpsupport_1646798932792/work\n",
      "typed-ast @ file:///C:/ci/typed-ast_1624953797214/work\n",
      "typing_extensions==4.11.0\n",
      "ujson @ file:///C:/ci/ujson_1648044223886/work\n",
      "Unidecode @ file:///tmp/build/80754af9/unidecode_1614712377438/work\n",
      "urllib3 @ file:///C:/ci/urllib3_1650639883891/work\n",
      "utilsforecast==0.1.7\n",
      "w3lib @ file:///Users/ktietz/demo/mc3/conda-bld/w3lib_1629359764703/work\n",
      "watchdog @ file:///C:/ci/watchdog_1638367441841/work\n",
      "wcwidth @ file:///Users/ktietz/demo/mc3/conda-bld/wcwidth_1629357192024/work\n",
      "webencodings==0.5.1\n",
      "websocket-client @ file:///C:/ci/websocket-client_1614804375980/work\n",
      "Werkzeug @ file:///opt/conda/conda-bld/werkzeug_1645628268370/work\n",
      "widgetsnbextension @ file:///C:/ci/widgetsnbextension_1644991377168/work\n",
      "win-inet-pton @ file:///C:/ci/win_inet_pton_1605306162074/work\n",
      "win-unicode-console==0.5\n",
      "wincertstore==0.2\n",
      "wrapt @ file:///C:/ci/wrapt_1607574570428/work\n",
      "xarray @ file:///opt/conda/conda-bld/xarray_1639166117697/work\n",
      "xlrd @ file:///tmp/build/80754af9/xlrd_1608072521494/work\n",
      "XlsxWriter @ file:///opt/conda/conda-bld/xlsxwriter_1649073856329/work\n",
      "xlwings==0.24.9\n",
      "xyzservices==2024.6.0\n",
      "yapf @ file:///tmp/build/80754af9/yapf_1615749224965/work\n",
      "yarl @ file:///C:/ci/yarl_1606940155993/work\n",
      "yellowbrick==1.5\n",
      "yfinance==0.2.32\n",
      "zict==2.0.0\n",
      "zipp @ file:///opt/conda/conda-bld/zipp_1641824620731/work\n",
      "zope.interface @ file:///C:/ci/zope.interface_1625036252485/work\n"
     ]
    }
   ],
   "source": [
    "pip freeze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200d49d2",
   "metadata": {},
   "source": [
    "# Importação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f5338130",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c3c36",
   "metadata": {},
   "source": [
    "# Declaração de funções"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "efd93b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta classe define uma Rede Neural Recorrente LSTM \n",
    "#(Long Short-Term Memory), que é um tipo de rede neural usada principalmente para tarefas de séries temporais e sequências.\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, in_dim, hid_dim, out_dim, num_layers, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.hid_dim = hid_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.layer = num_layers\n",
    "        \n",
    "        self.lstm = nn.LSTM(self.in_dim, self.hid_dim, self.layer, dropout=dropout_rate, batch_first=True)\n",
    "        \n",
    "        self.fc = nn.Linear(self.hid_dim, self.out_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # initialize the hidden state\n",
    "        h0 = torch.zeros(self.layer, x.size(0), self.hid_dim)\n",
    "        c0 = torch.zeros(self.layer, x.size(0), self.hid_dim)\n",
    "        \n",
    "        # forward propagate\n",
    "        out, (h_,c_) = self.lstm(x,(h0,c0))\n",
    "        \n",
    "        out = self.fc(out[:,-1,:])\n",
    "        return out\n",
    "#Esta função treina o modelo LSTM usando os dados fornecidos.\n",
    "def train_model(model,\n",
    "               criterion,\n",
    "               optimizer,\n",
    "               x_train,\n",
    "               x_test,\n",
    "               y_train,\n",
    "               y_test,\n",
    "               epochs=500):\n",
    "    \n",
    "    train_loss = np.zeros(epochs)\n",
    "    test_loss = np.zeros(epochs)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # put default model grads to zero\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # predict the output\n",
    "        pred = model(x_train)\n",
    "        \n",
    "        # calculate the loss \n",
    "        error = criterion(pred,y_train)\n",
    "        \n",
    "        # backpropagate the error\n",
    "        error.backward()\n",
    "        \n",
    "        # update the model parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # save the losses \n",
    "        train_loss[epoch] = error.item()\n",
    "        \n",
    "        # test loss \n",
    "        test_pred = model(x_test)\n",
    "        test_error = criterion(y_test,test_pred)\n",
    "        test_loss[epoch] = test_error.item()\n",
    "        \n",
    "        if (epoch+1) % 5 ==0:\n",
    "           print('Epoch :{}    Train Loss :{}    Test Loss :{}'.format((epoch+1)/epochs, error.item(), test_error.item()))\n",
    "            \n",
    "    return train_loss, test_loss\n",
    "\n",
    "\n",
    "# divide the data into train and test\n",
    "def split_data(x,y, ratio):\n",
    "    assert len(x)==len(y)\n",
    "    N = len(x)\n",
    "    train_x, test_x  = x[:int(N*ratio)], x[int(N*ratio):]\n",
    "    train_y, test_y  = y[:int(N*ratio)], y[int(N*ratio):]\n",
    "    return train_x,train_y, test_x, test_y\n",
    "\n",
    "# divide the data into number of steps \n",
    "def create_data(data, seq_len):\n",
    "    N = len(data)\n",
    "    X = []\n",
    "    Y = []\n",
    "    for i in range(N-seq_len-1):\n",
    "        x = data[i:i+seq_len]\n",
    "        X.append(x)\n",
    "        y = data[i+seq_len]\n",
    "        Y.append(y)\n",
    "        \n",
    "    return X,Y\n",
    "\n",
    "\n",
    "# Realiza o pré-processamento de dados em um DataFrame.\n",
    "def df_tratamento_dados(df):\n",
    "    df['data'] = pd.to_datetime(df['data'], format='%Y-%m-%d')\n",
    "    df = df.sort_values(by='data')\n",
    "    # Completando dados\n",
    "    # Crie um novo DataFrame com um intervalo completo de datas\n",
    "    data_inicio = df['data'].min()  # Data de início\n",
    "    data_fim = df['data'].max()  \n",
    "    datas_completas = pd.date_range(start=data_inicio, end=data_fim, freq='D')\n",
    "    # Crie um novo DataFrame com o intervalo completo de datas\n",
    "    df_completo = pd.DataFrame(datas_completas, columns=['data'])# Faça um merge com o dataframe original\n",
    "\n",
    "    df = df_completo.merge(df, on='data', how='left')\n",
    "    df = df.sort_values(by='data')\n",
    "\n",
    "    \n",
    "    df['valor'] = pd.to_numeric(df['valor'], errors='coerce')\n",
    "    df.index = pd.to_datetime(df['data'])\n",
    "    # Interpolação linear\n",
    "    df['valor'] = df['valor'].interpolate(method='linear')\n",
    "    df['valor'] = df['valor'].fillna(method='bfill')\n",
    "    df.sort_index(ascending=True, inplace=True)\n",
    "    #df = df.asfreq('D')\n",
    "    df.drop('data', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def ler_csv(caminho_arquivo):\n",
    "    return pd.read_csv(caminho_arquivo, sep=',',usecols=[0,1], decimal=',', header=0)\n",
    "\n",
    "# Escalona os dados e divide em treinamento e teste\n",
    "def escalonar(df,treino_df, entradas, scale):\n",
    "    percentual = len(treino_df)/len(df)\n",
    "    entradas = entradas\n",
    "    # scale the data\n",
    "    df = scale.fit_transform(df)\n",
    "    x,y = create_data(df,entradas)\n",
    "    x, y = np.asarray(x), np.asarray(y)\n",
    "    x.reshape(-1,entradas).shape\n",
    "    train_x,train_y, test_x, test_y = split_data(x,y,percentual)\n",
    "    # convert the data from numpy to tensor\n",
    "    train_x = torch.from_numpy(train_x.astype(np.float32))\n",
    "    train_y = torch.from_numpy(train_y.astype(np.float32))\n",
    "    test_x = torch.from_numpy(test_x.astype(np.float32))\n",
    "    test_y = torch.from_numpy(test_y.astype(np.float32))\n",
    "    return x,y,train_x,train_y,test_x,test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72ce9b7",
   "metadata": {},
   "source": [
    "# Leitura e tratamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6cff554d",
   "metadata": {},
   "outputs": [],
   "source": [
    "treino = ler_csv('dados_tratados/treino.csv')\n",
    "teste = ler_csv('dados_tratados/teste.csv')\n",
    "previsao = ler_csv('dados_tratados/previsao.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "35d69d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([treino,teste])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d84bcf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "treino = df_tratamento_dados(treino)\n",
    "teste = df_tratamento_dados(teste)\n",
    "previsao = df_tratamento_dados(previsao)\n",
    "df = df_tratamento_dados(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1dcf29fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8002738654147105"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentual = len(treino)/len(df)\n",
    "percentual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f0128b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_treino = treino.index\n",
    "data_teste = teste.index\n",
    "data_previsao = previsao.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cf9b52",
   "metadata": {},
   "source": [
    "# Escalonando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e33cd6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8002738654147105"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percentual = len(treino)/len(df)\n",
    "percentual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cc991e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "entradas = 10\n",
    "scale = MinMaxScaler(feature_range=(0,1))\n",
    "x,y,train_x,train_y,test_x,test_y = escalonar(df,treino, entradas, scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e142ab",
   "metadata": {},
   "source": [
    "# Treinando o modelo e aplicando o Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0520272c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Possible Models 32\n",
      "Epoch :0.0125    Train Loss :0.03609566017985344    Test Loss :0.2337789684534073\n",
      "Epoch :0.025    Train Loss :0.02372235618531704    Test Loss :0.027363106608390808\n",
      "Epoch :0.0375    Train Loss :0.006401128135621548    Test Loss :0.09242372959852219\n",
      "Epoch :0.05    Train Loss :0.0025898751337081194    Test Loss :0.0067662750370800495\n",
      "Epoch :0.0625    Train Loss :0.005455408710986376    Test Loss :0.020782044157385826\n",
      "Epoch :0.075    Train Loss :0.0020903281401842833    Test Loss :0.005536013748496771\n",
      "Epoch :0.0875    Train Loss :0.0015111742541193962    Test Loss :0.005025507882237434\n",
      "Epoch :0.1    Train Loss :0.002060650149360299    Test Loss :0.015073130838572979\n",
      "Epoch :0.1125    Train Loss :0.0011044967686757445    Test Loss :0.004338608123362064\n",
      "Epoch :0.125    Train Loss :0.0010699898703023791    Test Loss :0.004289866425096989\n",
      "Epoch :0.1375    Train Loss :0.0010407197987660766    Test Loss :0.005262303631752729\n",
      "Epoch :0.15    Train Loss :0.0008677538717165589    Test Loss :0.002952852286398411\n",
      "Epoch :0.1625    Train Loss :0.0007773965480737388    Test Loss :0.003164333291351795\n",
      "Epoch :0.175    Train Loss :0.0006883296882733703    Test Loss :0.002581639913842082\n",
      "Epoch :0.1875    Train Loss :0.0006988673121668398    Test Loss :0.0027666452806442976\n",
      "Epoch :0.2    Train Loss :0.0006501649622805417    Test Loss :0.0033076442778110504\n",
      "Epoch :0.2125    Train Loss :0.0006235542241483927    Test Loss :0.0022071893326938152\n",
      "Epoch :0.225    Train Loss :0.0005735390586778522    Test Loss :0.002386326203122735\n",
      "Epoch :0.2375    Train Loss :0.0005592222441919148    Test Loss :0.001962157664820552\n",
      "Epoch :0.25    Train Loss :0.0005571852088905871    Test Loss :0.0020827262196689844\n",
      "Epoch :0.2625    Train Loss :0.0005384045653045177    Test Loss :0.0018763703992590308\n",
      "Epoch :0.275    Train Loss :0.0005252632545307279    Test Loss :0.0020096965599805117\n",
      "Epoch :0.2875    Train Loss :0.0005163832101970911    Test Loss :0.001671588746830821\n",
      "Epoch :0.3    Train Loss :0.0005107008037157357    Test Loss :0.0018693918827921152\n",
      "Epoch :0.3125    Train Loss :0.0004899101913906634    Test Loss :0.0016602713149040937\n",
      "Epoch :0.325    Train Loss :0.00047725599142722785    Test Loss :0.0017021745443344116\n",
      "Epoch :0.3375    Train Loss :0.0004720279248431325    Test Loss :0.001518536009825766\n",
      "Epoch :0.35    Train Loss :0.0004318252031225711    Test Loss :0.0014281576732173562\n",
      "Epoch :0.3625    Train Loss :0.00043017262942157686    Test Loss :0.001366616110317409\n",
      "Epoch :0.375    Train Loss :0.0004341414023656398    Test Loss :0.0012961121974512935\n",
      "Epoch :0.3875    Train Loss :0.00043475651182234287    Test Loss :0.0013648313470184803\n",
      "Epoch :0.4    Train Loss :0.00042603767360560596    Test Loss :0.001385045819915831\n",
      "Epoch :0.4125    Train Loss :0.00040155978058464825    Test Loss :0.0011674495181068778\n",
      "Epoch :0.425    Train Loss :0.0003851163201034069    Test Loss :0.0011253799311816692\n",
      "Epoch :0.4375    Train Loss :0.00039819537778384984    Test Loss :0.0011258999584242702\n",
      "Epoch :0.45    Train Loss :0.0003706449642777443    Test Loss :0.0010737362317740917\n",
      "Epoch :0.4625    Train Loss :0.00036605066270567477    Test Loss :0.0011955059599131346\n",
      "Epoch :0.475    Train Loss :0.0003637103654909879    Test Loss :0.0009766911389306188\n",
      "Epoch :0.4875    Train Loss :0.00035663260496221483    Test Loss :0.0009745964198373258\n",
      "Epoch :0.5    Train Loss :0.0003430879150982946    Test Loss :0.0009564539650455117\n",
      "Epoch :0.5125    Train Loss :0.0003451736702118069    Test Loss :0.0009217780316248536\n",
      "Epoch :0.525    Train Loss :0.00034805512405000627    Test Loss :0.0008084174478426576\n",
      "Epoch :0.5375    Train Loss :0.00033371217432431877    Test Loss :0.0008025771821849048\n",
      "Epoch :0.55    Train Loss :0.0006670241127721965    Test Loss :0.003820205107331276\n",
      "Epoch :0.5625    Train Loss :0.0005507000023499131    Test Loss :0.0011013703187927604\n",
      "Epoch :0.575    Train Loss :0.00043669212027452886    Test Loss :0.0008215232519432902\n",
      "Epoch :0.5875    Train Loss :0.0003781953710131347    Test Loss :0.00092844053870067\n",
      "Epoch :0.6    Train Loss :0.000333323871018365    Test Loss :0.0008484826539643109\n",
      "Epoch :0.6125    Train Loss :0.00031577711342833936    Test Loss :0.0009127723751589656\n",
      "Epoch :0.625    Train Loss :0.00031060518813319504    Test Loss :0.0007524443790316582\n",
      "Epoch :0.6375    Train Loss :0.000298083177767694    Test Loss :0.0007948484853841364\n",
      "Epoch :0.65    Train Loss :0.00029705691849812865    Test Loss :0.0007107471465133131\n",
      "Epoch :0.6625    Train Loss :0.00028982528601773083    Test Loss :0.0007063515949994326\n",
      "Epoch :0.675    Train Loss :0.00029812887078151107    Test Loss :0.0007747073541395366\n",
      "Epoch :0.6875    Train Loss :0.00028248902526684105    Test Loss :0.0007229649345390499\n",
      "Epoch :0.7    Train Loss :0.0002853535988833755    Test Loss :0.0008353955927304924\n",
      "Epoch :0.7125    Train Loss :0.0004485879617277533    Test Loss :0.0010722092119976878\n",
      "Epoch :0.725    Train Loss :0.0002691220724955201    Test Loss :0.0006569215911440551\n",
      "Epoch :0.7375    Train Loss :0.00028258460224606097    Test Loss :0.0007400737376883626\n",
      "Epoch :0.75    Train Loss :0.0002893334603868425    Test Loss :0.0006656620535068214\n",
      "Epoch :0.7625    Train Loss :0.0002968041517306119    Test Loss :0.0006811970961280167\n",
      "Epoch :0.775    Train Loss :0.0002660246391315013    Test Loss :0.0006122506456449628\n",
      "Epoch :0.7875    Train Loss :0.0002664702187757939    Test Loss :0.0006005612085573375\n",
      "Epoch :0.8    Train Loss :0.0002579611027613282    Test Loss :0.0006279919762164354\n",
      "Epoch :0.8125    Train Loss :0.00026042238459922373    Test Loss :0.0006777552189305425\n",
      "Epoch :0.825    Train Loss :0.0002938243851531297    Test Loss :0.0007565378327853978\n",
      "Epoch :0.8375    Train Loss :0.0019378593424335122    Test Loss :0.0043979487381875515\n",
      "Epoch :0.85    Train Loss :0.0005217086873017251    Test Loss :0.0010619830572977662\n",
      "Epoch :0.8625    Train Loss :0.0006807930767536163    Test Loss :0.0012081440072506666\n",
      "Epoch :0.875    Train Loss :0.0003170384734403342    Test Loss :0.0017414186149835587\n",
      "Epoch :0.8875    Train Loss :0.000446376099716872    Test Loss :0.0010104092070832849\n",
      "Epoch :0.9    Train Loss :0.00031906922231428325    Test Loss :0.0009208591072820127\n",
      "Epoch :0.9125    Train Loss :0.0003071474493481219    Test Loss :0.0007911272696219385\n",
      "Epoch :0.925    Train Loss :0.00028844820917584    Test Loss :0.0007812759140506387\n",
      "Epoch :0.9375    Train Loss :0.0002817407075781375    Test Loss :0.0007576860953122377\n",
      "Epoch :0.95    Train Loss :0.0002800824004225433    Test Loss :0.0006884201429784298\n",
      "Epoch :0.9625    Train Loss :0.00027305298135615885    Test Loss :0.0006921722670085728\n",
      "Epoch :0.975    Train Loss :0.00026365966186858714    Test Loss :0.0006861851434223354\n",
      "Epoch :0.9875    Train Loss :0.0002594332618173212    Test Loss :0.000669631059281528\n",
      "Epoch :1.0    Train Loss :0.00024350745661649853    Test Loss :0.0006606269744224846\n",
      "RMSE: 15.369733792912998\n",
      "MAE: 13.284612790975366\n",
      "MAPE: 11.415792449124563%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 50, 'lr': 0.01, 'num_layers': 2}\n",
      "total:  3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.050150543451309204    Test Loss :0.3224029242992401\n",
      "Epoch :0.025    Train Loss :0.04448748379945755    Test Loss :0.15958575904369354\n",
      "Epoch :0.0375    Train Loss :0.041576653718948364    Test Loss :0.25914257764816284\n",
      "Epoch :0.05    Train Loss :0.03895684704184532    Test Loss :0.18855984508991241\n",
      "Epoch :0.0625    Train Loss :0.01600640080869198    Test Loss :0.025889305397868156\n",
      "Epoch :0.075    Train Loss :0.010574069805443287    Test Loss :0.04654614254832268\n",
      "Epoch :0.0875    Train Loss :0.010849719867110252    Test Loss :0.052771300077438354\n",
      "Epoch :0.1    Train Loss :0.008058320730924606    Test Loss :0.013630393892526627\n",
      "Epoch :0.1125    Train Loss :0.005133682396262884    Test Loss :0.01899396814405918\n",
      "Epoch :0.125    Train Loss :0.003082232316955924    Test Loss :0.015771690756082535\n",
      "Epoch :0.1375    Train Loss :0.0023813913576304913    Test Loss :0.009839746169745922\n",
      "Epoch :0.15    Train Loss :0.0020042485557496548    Test Loss :0.007095640990883112\n",
      "Epoch :0.1625    Train Loss :0.0017453718464821577    Test Loss :0.004577727522701025\n",
      "Epoch :0.175    Train Loss :0.001478887046687305    Test Loss :0.004388229455798864\n",
      "Epoch :0.1875    Train Loss :0.0014057016232982278    Test Loss :0.004834176506847143\n",
      "Epoch :0.2    Train Loss :0.0011951684718951583    Test Loss :0.005274529103189707\n",
      "Epoch :0.2125    Train Loss :0.001146563095971942    Test Loss :0.004663178231567144\n",
      "Epoch :0.225    Train Loss :0.0011333361035212874    Test Loss :0.0033520127180963755\n",
      "Epoch :0.2375    Train Loss :0.0010499167256057262    Test Loss :0.0033558900468051434\n",
      "Epoch :0.25    Train Loss :0.0010503185912966728    Test Loss :0.003133808495476842\n",
      "Epoch :0.2625    Train Loss :0.0009820120176300406    Test Loss :0.0031753177754580975\n",
      "Epoch :0.275    Train Loss :0.0009804734727367759    Test Loss :0.0029308965895324945\n",
      "Epoch :0.2875    Train Loss :0.0009524761699140072    Test Loss :0.0026417484041303396\n",
      "Epoch :0.3    Train Loss :0.0009141218033619225    Test Loss :0.0027489301282912493\n",
      "Epoch :0.3125    Train Loss :0.0008688105153851211    Test Loss :0.0026683551259338856\n",
      "Epoch :0.325    Train Loss :0.0009052138775587082    Test Loss :0.002950392197817564\n",
      "Epoch :0.3375    Train Loss :0.0008238430600613356    Test Loss :0.0025207726284861565\n",
      "Epoch :0.35    Train Loss :0.0008349131094291806    Test Loss :0.0026550793554633856\n",
      "Epoch :0.3625    Train Loss :0.0007937558111734688    Test Loss :0.002492536325007677\n",
      "Epoch :0.375    Train Loss :0.0007508364506065845    Test Loss :0.002541945083066821\n",
      "Epoch :0.3875    Train Loss :0.0007883640355430543    Test Loss :0.0023404061794281006\n",
      "Epoch :0.4    Train Loss :0.0007338399300351739    Test Loss :0.002610839204862714\n",
      "Epoch :0.4125    Train Loss :0.0010276754619553685    Test Loss :0.003912996035069227\n",
      "Epoch :0.425    Train Loss :0.0008742272621020675    Test Loss :0.0032735324930399656\n",
      "Epoch :0.4375    Train Loss :0.0007298492128029466    Test Loss :0.0024860892444849014\n",
      "Epoch :0.45    Train Loss :0.0006626974791288376    Test Loss :0.0024316157214343548\n",
      "Epoch :0.4625    Train Loss :0.0006438101991079748    Test Loss :0.0024605479557067156\n",
      "Epoch :0.475    Train Loss :0.0006302500260062516    Test Loss :0.0023786481469869614\n",
      "Epoch :0.4875    Train Loss :0.0006157673778943717    Test Loss :0.002645910019055009\n",
      "Epoch :0.5    Train Loss :0.0007947887061163783    Test Loss :0.0034700422547757626\n",
      "Epoch :0.5125    Train Loss :0.0006523921038024127    Test Loss :0.002855300437659025\n",
      "Epoch :0.525    Train Loss :0.0006936517311260104    Test Loss :0.002551013370975852\n",
      "Epoch :0.5375    Train Loss :0.0006118602468632162    Test Loss :0.002647427609190345\n",
      "Epoch :0.55    Train Loss :0.0006030325894244015    Test Loss :0.0023878151550889015\n",
      "Epoch :0.5625    Train Loss :0.0005671087419614196    Test Loss :0.0023983290884643793\n",
      "Epoch :0.575    Train Loss :0.0005856640636920929    Test Loss :0.00234968145377934\n",
      "Epoch :0.5875    Train Loss :0.0005560818244703114    Test Loss :0.0023212116211652756\n",
      "Epoch :0.6    Train Loss :0.0005028784507885575    Test Loss :0.002228706842288375\n",
      "Epoch :0.6125    Train Loss :0.000532030884642154    Test Loss :0.002308269264176488\n",
      "Epoch :0.625    Train Loss :0.0005655789864249527    Test Loss :0.0023821683134883642\n",
      "Epoch :0.6375    Train Loss :0.0016695401864126325    Test Loss :0.004057915415614843\n",
      "Epoch :0.65    Train Loss :0.0010608574375510216    Test Loss :0.002955923555418849\n",
      "Epoch :0.6625    Train Loss :0.000660750491078943    Test Loss :0.002177185146138072\n",
      "Epoch :0.675    Train Loss :0.0006487485370598733    Test Loss :0.0037820537108927965\n",
      "Epoch :0.6875    Train Loss :0.0006986191147007048    Test Loss :0.0030457607936114073\n",
      "Epoch :0.7    Train Loss :0.0005789956776425242    Test Loss :0.002426304155960679\n",
      "Epoch :0.7125    Train Loss :0.0005954781663604081    Test Loss :0.0024872818030416965\n",
      "Epoch :0.725    Train Loss :0.0005515840603038669    Test Loss :0.00237517012283206\n",
      "Epoch :0.7375    Train Loss :0.0005070235347375274    Test Loss :0.0021352495532482862\n",
      "Epoch :0.75    Train Loss :0.000509049161337316    Test Loss :0.00216875271871686\n",
      "Epoch :0.7625    Train Loss :0.0005202250904403627    Test Loss :0.0022101786453276873\n",
      "Epoch :0.775    Train Loss :0.0005111355567350984    Test Loss :0.0023550435435026884\n",
      "Epoch :0.7875    Train Loss :0.00048803930985741317    Test Loss :0.002090448047965765\n",
      "Epoch :0.8    Train Loss :0.00048053887439891696    Test Loss :0.0021853160578757524\n",
      "Epoch :0.8125    Train Loss :0.0004666539025492966    Test Loss :0.0021238764747977257\n",
      "Epoch :0.825    Train Loss :0.0004619644023478031    Test Loss :0.001926394528709352\n",
      "Epoch :0.8375    Train Loss :0.00044746644562110305    Test Loss :0.0019991223234683275\n",
      "Epoch :0.85    Train Loss :0.0004598544619511813    Test Loss :0.002057459205389023\n",
      "Epoch :0.8625    Train Loss :0.0004383726045489311    Test Loss :0.0021439336705952883\n",
      "Epoch :0.875    Train Loss :0.00044733553659170866    Test Loss :0.0020676241256296635\n",
      "Epoch :0.8875    Train Loss :0.0004217762325424701    Test Loss :0.0021055578254163265\n",
      "Epoch :0.9    Train Loss :0.00042602370376698673    Test Loss :0.0020030876621603966\n",
      "Epoch :0.9125    Train Loss :0.00042351658339612186    Test Loss :0.001977028790861368\n",
      "Epoch :0.925    Train Loss :0.000427712919190526    Test Loss :0.0019214729545637965\n",
      "Epoch :0.9375    Train Loss :0.000401100522140041    Test Loss :0.0018954492406919599\n",
      "Epoch :0.95    Train Loss :0.000412751775002107    Test Loss :0.001895864144898951\n",
      "Epoch :0.9625    Train Loss :0.00045365028199739754    Test Loss :0.0022441556211560965\n",
      "Epoch :0.975    Train Loss :0.0004212002386339009    Test Loss :0.0020658904686570168\n",
      "Epoch :0.9875    Train Loss :0.00040035121492110193    Test Loss :0.001997855259105563\n",
      "Epoch :1.0    Train Loss :0.0003940596943721175    Test Loss :0.0019958962220698595\n",
      "RMSE: 43.72912462636507\n",
      "MAE: 35.27438797760564\n",
      "MAPE: 31.086943684659452%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 50, 'lr': 0.01, 'num_layers': 5}\n",
      "total:  6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.04112120345234871    Test Loss :0.08123306930065155\n",
      "Epoch :0.025    Train Loss :0.005291163921356201    Test Loss :0.01757633686065674\n",
      "Epoch :0.0375    Train Loss :0.0030625085346400738    Test Loss :0.02475629188120365\n",
      "Epoch :0.05    Train Loss :0.0016708479961380363    Test Loss :0.005119803827255964\n",
      "Epoch :0.0625    Train Loss :0.001990370685234666    Test Loss :0.009948063641786575\n",
      "Epoch :0.075    Train Loss :0.0012041712179780006    Test Loss :0.003722504945471883\n",
      "Epoch :0.0875    Train Loss :0.0008257701992988586    Test Loss :0.004016704857349396\n",
      "Epoch :0.1    Train Loss :0.00086224265396595    Test Loss :0.002576502040028572\n",
      "Epoch :0.1125    Train Loss :0.0007994279148988426    Test Loss :0.003181623760610819\n",
      "Epoch :0.125    Train Loss :0.0006345185684040189    Test Loss :0.002730518113821745\n",
      "Epoch :0.1375    Train Loss :0.0006442872690968215    Test Loss :0.0025540932547301054\n",
      "Epoch :0.15    Train Loss :0.000598849612288177    Test Loss :0.002609583782032132\n",
      "Epoch :0.1625    Train Loss :0.0005498572718352079    Test Loss :0.0024046071339398623\n",
      "Epoch :0.175    Train Loss :0.000532854872290045    Test Loss :0.0023455864284187555\n",
      "Epoch :0.1875    Train Loss :0.0005436114151962101    Test Loss :0.002207090612500906\n",
      "Epoch :0.2    Train Loss :0.0005287189851514995    Test Loss :0.00192757579497993\n",
      "Epoch :0.2125    Train Loss :0.0004915306926704943    Test Loss :0.002052724128589034\n",
      "Epoch :0.225    Train Loss :0.0004788813821505755    Test Loss :0.0019980340730398893\n",
      "Epoch :0.2375    Train Loss :0.00046103523345664144    Test Loss :0.0020105482544749975\n",
      "Epoch :0.25    Train Loss :0.0004375569988042116    Test Loss :0.0017077785450965166\n",
      "Epoch :0.2625    Train Loss :0.000449105107691139    Test Loss :0.0016968087293207645\n",
      "Epoch :0.275    Train Loss :0.0004471816646400839    Test Loss :0.0015746282879263163\n",
      "Epoch :0.2875    Train Loss :0.0004375432909000665    Test Loss :0.0016128933057188988\n",
      "Epoch :0.3    Train Loss :0.00040914095006883144    Test Loss :0.0017204481409862638\n",
      "Epoch :0.3125    Train Loss :0.0004001753986813128    Test Loss :0.0015080010052770376\n",
      "Epoch :0.325    Train Loss :0.00039683340582996607    Test Loss :0.0014597642002627254\n",
      "Epoch :0.3375    Train Loss :0.0003938611189369112    Test Loss :0.0014107170281931758\n",
      "Epoch :0.35    Train Loss :0.00037877896102145314    Test Loss :0.0013053885195404291\n",
      "Epoch :0.3625    Train Loss :0.00037055660504847765    Test Loss :0.0013066631508991122\n",
      "Epoch :0.375    Train Loss :0.0003646422701422125    Test Loss :0.001253248192369938\n",
      "Epoch :0.3875    Train Loss :0.0003524037019815296    Test Loss :0.0011485241120681167\n",
      "Epoch :0.4    Train Loss :0.00032159691909328103    Test Loss :0.0011098092654719949\n",
      "Epoch :0.4125    Train Loss :0.0003397262189537287    Test Loss :0.0010118884965777397\n",
      "Epoch :0.425    Train Loss :0.00032829534029588103    Test Loss :0.001024147029966116\n",
      "Epoch :0.4375    Train Loss :0.00032971633481793106    Test Loss :0.0010325470939278603\n",
      "Epoch :0.45    Train Loss :0.0003253371105529368    Test Loss :0.0009785895235836506\n",
      "Epoch :0.4625    Train Loss :0.0014620628207921982    Test Loss :0.006163670681416988\n",
      "Epoch :0.475    Train Loss :0.0014095583464950323    Test Loss :0.0021282262168824673\n",
      "Epoch :0.4875    Train Loss :0.0007344048353843391    Test Loss :0.0011882022954523563\n",
      "Epoch :0.5    Train Loss :0.0005998452543281019    Test Loss :0.0013944364618510008\n",
      "Epoch :0.5125    Train Loss :0.00037821035948581994    Test Loss :0.0019527929835021496\n",
      "Epoch :0.525    Train Loss :0.00039057896356098354    Test Loss :0.001221386482939124\n",
      "Epoch :0.5375    Train Loss :0.00039003047277219594    Test Loss :0.0012598958564922214\n",
      "Epoch :0.55    Train Loss :0.0003367209865245968    Test Loss :0.0013203498674556613\n",
      "Epoch :0.5625    Train Loss :0.00034330791095271707    Test Loss :0.0011522781569510698\n",
      "Epoch :0.575    Train Loss :0.0003324847493786365    Test Loss :0.001311308005824685\n",
      "Epoch :0.5875    Train Loss :0.00033733443706296384    Test Loss :0.001040755189023912\n",
      "Epoch :0.6    Train Loss :0.0003250271256547421    Test Loss :0.001135562313720584\n",
      "Epoch :0.6125    Train Loss :0.00031914684223011136    Test Loss :0.0009836241370067\n",
      "Epoch :0.625    Train Loss :0.0003038856666535139    Test Loss :0.0010576331987977028\n",
      "Epoch :0.6375    Train Loss :0.00031941922497935593    Test Loss :0.0009136502631008625\n",
      "Epoch :0.65    Train Loss :0.00030350781162269413    Test Loss :0.0009039373253472149\n",
      "Epoch :0.6625    Train Loss :0.0002840635133907199    Test Loss :0.0008368546259589493\n",
      "Epoch :0.675    Train Loss :0.00029637699481099844    Test Loss :0.0007691521313972771\n",
      "Epoch :0.6875    Train Loss :0.000286929658614099    Test Loss :0.0008525997400283813\n",
      "Epoch :0.7    Train Loss :0.0002660286263562739    Test Loss :0.0007213333738036454\n",
      "Epoch :0.7125    Train Loss :0.0002899745013564825    Test Loss :0.0006867609918117523\n",
      "Epoch :0.725    Train Loss :0.0002589485375210643    Test Loss :0.0007233549258671701\n",
      "Epoch :0.7375    Train Loss :0.000267695082584396    Test Loss :0.0006695345509797335\n",
      "Epoch :0.75    Train Loss :0.0005631382227875292    Test Loss :0.00505218468606472\n",
      "Epoch :0.7625    Train Loss :0.0025977687910199165    Test Loss :0.010861638933420181\n",
      "Epoch :0.775    Train Loss :0.002775883302092552    Test Loss :0.0046396502293646336\n",
      "Epoch :0.7875    Train Loss :0.0013593821786344051    Test Loss :0.0016289360355585814\n",
      "Epoch :0.8    Train Loss :0.0006359877879731357    Test Loss :0.001758082420565188\n",
      "Epoch :0.8125    Train Loss :0.00047491045552305877    Test Loss :0.0016193515621125698\n",
      "Epoch :0.825    Train Loss :0.0004469233681447804    Test Loss :0.001652975333854556\n",
      "Epoch :0.8375    Train Loss :0.0004168556770309806    Test Loss :0.001571832923218608\n",
      "Epoch :0.85    Train Loss :0.00041419832268729806    Test Loss :0.0014672293327748775\n",
      "Epoch :0.8625    Train Loss :0.00040291904588229954    Test Loss :0.0013799777952954173\n",
      "Epoch :0.875    Train Loss :0.00039073568768799305    Test Loss :0.001387813943438232\n",
      "Epoch :0.8875    Train Loss :0.00039943959563970566    Test Loss :0.0013437701854854822\n",
      "Epoch :0.9    Train Loss :0.0004004112852271646    Test Loss :0.0013200531248003244\n",
      "Epoch :0.9125    Train Loss :0.00037861079908907413    Test Loss :0.0013106993865221739\n",
      "Epoch :0.925    Train Loss :0.00036558459396474063    Test Loss :0.0012749749002978206\n",
      "Epoch :0.9375    Train Loss :0.0003605109523050487    Test Loss :0.001245857565663755\n",
      "Epoch :0.95    Train Loss :0.0003591619897633791    Test Loss :0.0012942871544510126\n",
      "Epoch :0.9625    Train Loss :0.00036379514494910836    Test Loss :0.0011717037996277213\n",
      "Epoch :0.975    Train Loss :0.00035322524490766227    Test Loss :0.001265652826987207\n",
      "Epoch :0.9875    Train Loss :0.00032209043274633586    Test Loss :0.001171110081486404\n",
      "Epoch :1.0    Train Loss :0.0003210465656593442    Test Loss :0.001083278562873602\n",
      "RMSE: 10.514293152952934\n",
      "MAE: 8.094271006175418\n",
      "MAPE: 6.931409384510519%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 50, 'lr': 0.02, 'num_layers': 2}\n",
      "total:  9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.04341968894004822    Test Loss :0.25209179520606995\n",
      "Epoch :0.025    Train Loss :0.03175734356045723    Test Loss :0.19054029881954193\n",
      "Epoch :0.0375    Train Loss :0.049848996102809906    Test Loss :0.25920191407203674\n",
      "Epoch :0.05    Train Loss :0.018261395394802094    Test Loss :0.05649498850107193\n",
      "Epoch :0.0625    Train Loss :0.012824133969843388    Test Loss :0.03903675079345703\n",
      "Epoch :0.075    Train Loss :0.012070518918335438    Test Loss :0.058898355811834335\n",
      "Epoch :0.0875    Train Loss :0.008736029267311096    Test Loss :0.01866312138736248\n",
      "Epoch :0.1    Train Loss :0.005693277809768915    Test Loss :0.0318794883787632\n",
      "Epoch :0.1125    Train Loss :0.00606307340785861    Test Loss :0.02087082713842392\n",
      "Epoch :0.125    Train Loss :0.003012041561305523    Test Loss :0.015665333718061447\n",
      "Epoch :0.1375    Train Loss :0.002551635494455695    Test Loss :0.00736002204939723\n",
      "Epoch :0.15    Train Loss :0.0023018652573227882    Test Loss :0.010473063215613365\n",
      "Epoch :0.1625    Train Loss :0.0019409745000302792    Test Loss :0.005513758864253759\n",
      "Epoch :0.175    Train Loss :0.0017093709902837873    Test Loss :0.005123015958815813\n",
      "Epoch :0.1875    Train Loss :0.001489977352321148    Test Loss :0.005975647363811731\n",
      "Epoch :0.2    Train Loss :0.001446320442482829    Test Loss :0.005122696049511433\n",
      "Epoch :0.2125    Train Loss :0.0013755016261711717    Test Loss :0.004374593496322632\n",
      "Epoch :0.225    Train Loss :0.0012455327669158578    Test Loss :0.003997916355729103\n",
      "Epoch :0.2375    Train Loss :0.0012096813879907131    Test Loss :0.0034270700998604298\n",
      "Epoch :0.25    Train Loss :0.0011486205039545894    Test Loss :0.0034772499930113554\n",
      "Epoch :0.2625    Train Loss :0.0010841282783076167    Test Loss :0.0033026435412466526\n",
      "Epoch :0.275    Train Loss :0.0010270935017615557    Test Loss :0.003049239283427596\n",
      "Epoch :0.2875    Train Loss :0.00097953830845654    Test Loss :0.0030152935069054365\n",
      "Epoch :0.3    Train Loss :0.0009569048415869474    Test Loss :0.0028161960653960705\n",
      "Epoch :0.3125    Train Loss :0.0009223785018548369    Test Loss :0.002599711762741208\n",
      "Epoch :0.325    Train Loss :0.0009219773346558213    Test Loss :0.00268646702170372\n",
      "Epoch :0.3375    Train Loss :0.0008981956052593887    Test Loss :0.00282310345210135\n",
      "Epoch :0.35    Train Loss :0.00087447912665084    Test Loss :0.002955681411549449\n",
      "Epoch :0.3625    Train Loss :0.0008268830715678632    Test Loss :0.003001985140144825\n",
      "Epoch :0.375    Train Loss :0.0008231469546444714    Test Loss :0.0027971253730356693\n",
      "Epoch :0.3875    Train Loss :0.0007911535794846714    Test Loss :0.0027769419830292463\n",
      "Epoch :0.4    Train Loss :0.000790316320490092    Test Loss :0.0026859124191105366\n",
      "Epoch :0.4125    Train Loss :0.0007734307437203825    Test Loss :0.002451952314004302\n",
      "Epoch :0.425    Train Loss :0.0007563758990727365    Test Loss :0.0025787572376430035\n",
      "Epoch :0.4375    Train Loss :0.000780270085670054    Test Loss :0.002644586842507124\n",
      "Epoch :0.45    Train Loss :0.0007244420703500509    Test Loss :0.002569802338257432\n",
      "Epoch :0.4625    Train Loss :0.0007054952438920736    Test Loss :0.0026316209696233273\n",
      "Epoch :0.475    Train Loss :0.0007135744672268629    Test Loss :0.0026088443119078875\n",
      "Epoch :0.4875    Train Loss :0.0007052841829136014    Test Loss :0.00261546578258276\n",
      "Epoch :0.5    Train Loss :0.000673655595164746    Test Loss :0.0024403452407568693\n",
      "Epoch :0.5125    Train Loss :0.0006597865140065551    Test Loss :0.0023802630603313446\n",
      "Epoch :0.525    Train Loss :0.000646894914098084    Test Loss :0.0022969404235482216\n",
      "Epoch :0.5375    Train Loss :0.0006348129245452583    Test Loss :0.0024941088631749153\n",
      "Epoch :0.55    Train Loss :0.001372467027977109    Test Loss :0.002582559362053871\n",
      "Epoch :0.5625    Train Loss :0.000999224022962153    Test Loss :0.002815862651914358\n",
      "Epoch :0.575    Train Loss :0.0007874019211158156    Test Loss :0.002730591921135783\n",
      "Epoch :0.5875    Train Loss :0.0006380380946211517    Test Loss :0.0024478354025632143\n",
      "Epoch :0.6    Train Loss :0.0006273679318837821    Test Loss :0.0025426202919334173\n",
      "Epoch :0.6125    Train Loss :0.0005983072915114462    Test Loss :0.002511572325602174\n",
      "Epoch :0.625    Train Loss :0.0005965824238955975    Test Loss :0.0023465806152671576\n",
      "Epoch :0.6375    Train Loss :0.0006424829480238259    Test Loss :0.0023458555806428194\n",
      "Epoch :0.65    Train Loss :0.0006125131621956825    Test Loss :0.002312011318281293\n",
      "Epoch :0.6625    Train Loss :0.0005665965145453811    Test Loss :0.002284938469529152\n",
      "Epoch :0.675    Train Loss :0.0005524384905584157    Test Loss :0.0023012831807136536\n",
      "Epoch :0.6875    Train Loss :0.0005230933893471956    Test Loss :0.0023015502374619246\n",
      "Epoch :0.7    Train Loss :0.0005249502719379961    Test Loss :0.002226936398074031\n",
      "Epoch :0.7125    Train Loss :0.0005334598827175796    Test Loss :0.002256415318697691\n",
      "Epoch :0.725    Train Loss :0.0005188336363062263    Test Loss :0.002283695386722684\n",
      "Epoch :0.7375    Train Loss :0.000817522406578064    Test Loss :0.0027301376685500145\n",
      "Epoch :0.75    Train Loss :0.0005768288974650204    Test Loss :0.0029614379163831472\n",
      "Epoch :0.7625    Train Loss :0.0006149943801574409    Test Loss :0.002324738074094057\n",
      "Epoch :0.775    Train Loss :0.0005613048560917377    Test Loss :0.0024147103540599346\n",
      "Epoch :0.7875    Train Loss :0.0005052817286923528    Test Loss :0.0022823044564574957\n",
      "Epoch :0.8    Train Loss :0.0004999821539968252    Test Loss :0.002163388766348362\n",
      "Epoch :0.8125    Train Loss :0.0005174863035790622    Test Loss :0.0021965866908431053\n",
      "Epoch :0.825    Train Loss :0.0004668420588131994    Test Loss :0.0021981666795909405\n",
      "Epoch :0.8375    Train Loss :0.00047520475345663726    Test Loss :0.002051964867860079\n",
      "Epoch :0.85    Train Loss :0.0004555305349640548    Test Loss :0.0020748237147927284\n",
      "Epoch :0.8625    Train Loss :0.00043507348163984716    Test Loss :0.002214323729276657\n",
      "Epoch :0.875    Train Loss :0.0008588766795583069    Test Loss :0.0024503148160874844\n",
      "Epoch :0.8875    Train Loss :0.0005869520828127861    Test Loss :0.0025070873089134693\n",
      "Epoch :0.9    Train Loss :0.0005108757177367806    Test Loss :0.0022429239470511675\n",
      "Epoch :0.9125    Train Loss :0.0005108024924993515    Test Loss :0.0023325535003095865\n",
      "Epoch :0.925    Train Loss :0.00043905110214836895    Test Loss :0.0021439676638692617\n",
      "Epoch :0.9375    Train Loss :0.00045633528498001397    Test Loss :0.0023005539551377296\n",
      "Epoch :0.95    Train Loss :0.0004283448215574026    Test Loss :0.002149758394807577\n",
      "Epoch :0.9625    Train Loss :0.00040148102561943233    Test Loss :0.0019716224633157253\n",
      "Epoch :0.975    Train Loss :0.00043972095591016114    Test Loss :0.00199042703025043\n",
      "Epoch :0.9875    Train Loss :0.00042185012716799974    Test Loss :0.0020468789152801037\n",
      "Epoch :1.0    Train Loss :0.00045882564154453576    Test Loss :0.002063027350232005\n",
      "RMSE: 8.780589614802592\n",
      "MAE: 7.1850133055306395\n",
      "MAPE: 6.524873112364856%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 50, 'lr': 0.02, 'num_layers': 5}\n",
      "total:  12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.04069523513317108    Test Loss :0.22057867050170898\n",
      "Epoch :0.025    Train Loss :0.016716577112674713    Test Loss :0.009562669321894646\n",
      "Epoch :0.0375    Train Loss :0.010503236204385757    Test Loss :0.026698337867856026\n",
      "Epoch :0.05    Train Loss :0.0043206606060266495    Test Loss :0.022291839122772217\n",
      "Epoch :0.0625    Train Loss :0.006144125014543533    Test Loss :0.004155096597969532\n",
      "Epoch :0.075    Train Loss :0.003246761392802    Test Loss :0.003768328810110688\n",
      "Epoch :0.0875    Train Loss :0.0012782851699739695    Test Loss :0.005771128926426172\n",
      "Epoch :0.1    Train Loss :0.0013007454108446836    Test Loss :0.004221568815410137\n",
      "Epoch :0.1125    Train Loss :0.0009946065256372094    Test Loss :0.0033544667530804873\n",
      "Epoch :0.125    Train Loss :0.0008368965936824679    Test Loss :0.002993994625285268\n",
      "Epoch :0.1375    Train Loss :0.0007588319131173193    Test Loss :0.003438571235165\n",
      "Epoch :0.15    Train Loss :0.0007367565995082259    Test Loss :0.002240358619019389\n",
      "Epoch :0.1625    Train Loss :0.0006481713498942554    Test Loss :0.002745112171396613\n",
      "Epoch :0.175    Train Loss :0.0006128260283730924    Test Loss :0.0021636912133544683\n",
      "Epoch :0.1875    Train Loss :0.0005821041413582861    Test Loss :0.0023911911994218826\n",
      "Epoch :0.2    Train Loss :0.0005663716001436114    Test Loss :0.002142256125807762\n",
      "Epoch :0.2125    Train Loss :0.0005514379590749741    Test Loss :0.0021105201449245214\n",
      "Epoch :0.225    Train Loss :0.0005194698460400105    Test Loss :0.001890760031528771\n",
      "Epoch :0.2375    Train Loss :0.000525532872416079    Test Loss :0.0018062622984871268\n",
      "Epoch :0.25    Train Loss :0.000504187133628875    Test Loss :0.0019797529093921185\n",
      "Epoch :0.2625    Train Loss :0.0004850583791267127    Test Loss :0.0016038884641602635\n",
      "Epoch :0.275    Train Loss :0.0004889972042292356    Test Loss :0.0018608833197504282\n",
      "Epoch :0.2875    Train Loss :0.00046137100434862077    Test Loss :0.0016513409791514277\n",
      "Epoch :0.3    Train Loss :0.0004789927916135639    Test Loss :0.0015869182534515858\n",
      "Epoch :0.3125    Train Loss :0.00044586171861737967    Test Loss :0.0016076014144346118\n",
      "Epoch :0.325    Train Loss :0.0004386937653180212    Test Loss :0.0015228157863020897\n",
      "Epoch :0.3375    Train Loss :0.00043893305701203644    Test Loss :0.0014812105800956488\n",
      "Epoch :0.35    Train Loss :0.000420835247496143    Test Loss :0.0015906228218227625\n",
      "Epoch :0.3625    Train Loss :0.0004360644961707294    Test Loss :0.0013773733517155051\n",
      "Epoch :0.375    Train Loss :0.00042539089918136597    Test Loss :0.0013726504985243082\n",
      "Epoch :0.3875    Train Loss :0.0004052239819429815    Test Loss :0.0013402373297140002\n",
      "Epoch :0.4    Train Loss :0.00039637298323214054    Test Loss :0.0012773098424077034\n",
      "Epoch :0.4125    Train Loss :0.0003914531844202429    Test Loss :0.0013519206549972296\n",
      "Epoch :0.425    Train Loss :0.0003844106395263225    Test Loss :0.001251526759006083\n",
      "Epoch :0.4375    Train Loss :0.00038732969551347196    Test Loss :0.0011833361349999905\n",
      "Epoch :0.45    Train Loss :0.0003799194237217307    Test Loss :0.0010866970987990499\n",
      "Epoch :0.4625    Train Loss :0.0003631245926953852    Test Loss :0.001148627488873899\n",
      "Epoch :0.475    Train Loss :0.00036052282666787505    Test Loss :0.0009952604304999113\n",
      "Epoch :0.4875    Train Loss :0.00036900059785693884    Test Loss :0.001085758558474481\n",
      "Epoch :0.5    Train Loss :0.0003639585920609534    Test Loss :0.0010538288624957204\n",
      "Epoch :0.5125    Train Loss :0.000347398774465546    Test Loss :0.0010629070457071066\n",
      "Epoch :0.525    Train Loss :0.00034580565989017487    Test Loss :0.0010431860573589802\n",
      "Epoch :0.5375    Train Loss :0.0003348135796841234    Test Loss :0.0010304170427843928\n",
      "Epoch :0.55    Train Loss :0.0003286456048954278    Test Loss :0.0009291947935707867\n",
      "Epoch :0.5625    Train Loss :0.000340430618962273    Test Loss :0.0009060219745151699\n",
      "Epoch :0.575    Train Loss :0.0003306383441668004    Test Loss :0.000996283139102161\n",
      "Epoch :0.5875    Train Loss :0.00032111766631715    Test Loss :0.0009173115249723196\n",
      "Epoch :0.6    Train Loss :0.00031656865030527115    Test Loss :0.000867030001245439\n",
      "Epoch :0.6125    Train Loss :0.00031782040605321527    Test Loss :0.000879982952028513\n",
      "Epoch :0.625    Train Loss :0.0003155943413730711    Test Loss :0.0008604342583566904\n",
      "Epoch :0.6375    Train Loss :0.0002965081366710365    Test Loss :0.0008330773562192917\n",
      "Epoch :0.65    Train Loss :0.00029186278698034585    Test Loss :0.0008134628878906369\n",
      "Epoch :0.6625    Train Loss :0.00029302830807864666    Test Loss :0.0008876678184606135\n",
      "Epoch :0.675    Train Loss :0.00028639385709539056    Test Loss :0.0008237968431785703\n",
      "Epoch :0.6875    Train Loss :0.00028179792570881546    Test Loss :0.0008628914365544915\n",
      "Epoch :0.7    Train Loss :0.0002810262958519161    Test Loss :0.0007374867564067245\n",
      "Epoch :0.7125    Train Loss :0.0002824715047609061    Test Loss :0.0007598879165016115\n",
      "Epoch :0.725    Train Loss :0.00027495561516843736    Test Loss :0.0008017155923880637\n",
      "Epoch :0.7375    Train Loss :0.00027243851218372583    Test Loss :0.0007225386216305196\n",
      "Epoch :0.75    Train Loss :0.00026143158902414143    Test Loss :0.0007342140888795257\n",
      "Epoch :0.7625    Train Loss :0.0002663272025529295    Test Loss :0.000715067028068006\n",
      "Epoch :0.775    Train Loss :0.00027973210671916604    Test Loss :0.0006405907915905118\n",
      "Epoch :0.7875    Train Loss :0.000251342193223536    Test Loss :0.0007284559542313218\n",
      "Epoch :0.8    Train Loss :0.0002620508603285998    Test Loss :0.000751816900447011\n",
      "Epoch :0.8125    Train Loss :0.0002459287934470922    Test Loss :0.0006641146028414369\n",
      "Epoch :0.825    Train Loss :0.0002487879537511617    Test Loss :0.0006605234229937196\n",
      "Epoch :0.8375    Train Loss :0.00025432207621634007    Test Loss :0.0006706203566864133\n",
      "Epoch :0.85    Train Loss :0.0005922605050727725    Test Loss :0.0019721731077879667\n",
      "Epoch :0.8625    Train Loss :0.00028044788632541895    Test Loss :0.0006210830179043114\n",
      "Epoch :0.875    Train Loss :0.00033345664269290864    Test Loss :0.000684498343616724\n",
      "Epoch :0.8875    Train Loss :0.0002740049094427377    Test Loss :0.0006624081288464367\n",
      "Epoch :0.9    Train Loss :0.00026491214521229267    Test Loss :0.0006341555272229016\n",
      "Epoch :0.9125    Train Loss :0.00027029053308069706    Test Loss :0.0006178912590257823\n",
      "Epoch :0.925    Train Loss :0.0002607536443974823    Test Loss :0.0006191189750097692\n",
      "Epoch :0.9375    Train Loss :0.00023150919878389686    Test Loss :0.0007080413051880896\n",
      "Epoch :0.95    Train Loss :0.00022473590797744691    Test Loss :0.0005786627298220992\n",
      "Epoch :0.9625    Train Loss :0.0002325447421753779    Test Loss :0.0005918586393818259\n",
      "Epoch :0.975    Train Loss :0.00022932722640689462    Test Loss :0.000645510561298579\n",
      "Epoch :0.9875    Train Loss :0.00022085885575506836    Test Loss :0.0005617329734377563\n",
      "Epoch :1.0    Train Loss :0.00021446174650918692    Test Loss :0.0005448526935651898\n",
      "RMSE: 9.639110990838871\n",
      "MAE: 7.768597178975662\n",
      "MAPE: 7.090154231340707%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 100, 'lr': 0.01, 'num_layers': 2}\n",
      "total:  16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.05441901832818985    Test Loss :0.3084776699542999\n",
      "Epoch :0.025    Train Loss :0.041565798223018646    Test Loss :0.17339177429676056\n",
      "Epoch :0.0375    Train Loss :0.041039809584617615    Test Loss :0.24610474705696106\n",
      "Epoch :0.05    Train Loss :0.03759598731994629    Test Loss :0.1699441373348236\n",
      "Epoch :0.0625    Train Loss :0.04145117849111557    Test Loss :0.03877615928649902\n",
      "Epoch :0.075    Train Loss :0.011689034290611744    Test Loss :0.03774172440171242\n",
      "Epoch :0.0875    Train Loss :0.010060198605060577    Test Loss :0.06354006379842758\n",
      "Epoch :0.1    Train Loss :0.005405026022344828    Test Loss :0.029613202437758446\n",
      "Epoch :0.1125    Train Loss :0.004367287736386061    Test Loss :0.006210717372596264\n",
      "Epoch :0.125    Train Loss :0.001944115268997848    Test Loss :0.006794765125960112\n",
      "Epoch :0.1375    Train Loss :0.0024565975181758404    Test Loss :0.010940732434391975\n",
      "Epoch :0.15    Train Loss :0.0016211472684517503    Test Loss :0.005258781835436821\n",
      "Epoch :0.1625    Train Loss :0.0014344853116199374    Test Loss :0.00393736781552434\n",
      "Epoch :0.175    Train Loss :0.001188355265185237    Test Loss :0.0044566430151462555\n",
      "Epoch :0.1875    Train Loss :0.0011195639381185174    Test Loss :0.003671849612146616\n",
      "Epoch :0.2    Train Loss :0.0010600032983347774    Test Loss :0.0031226424034684896\n",
      "Epoch :0.2125    Train Loss :0.0009583598584868014    Test Loss :0.003272476838901639\n",
      "Epoch :0.225    Train Loss :0.0009313171613030136    Test Loss :0.003290631575509906\n",
      "Epoch :0.2375    Train Loss :0.0008497807430103421    Test Loss :0.002894276287406683\n",
      "Epoch :0.25    Train Loss :0.0008533183136023581    Test Loss :0.0027510840445756912\n",
      "Epoch :0.2625    Train Loss :0.0008242939948104322    Test Loss :0.0026752189733088017\n",
      "Epoch :0.275    Train Loss :0.0007955939508974552    Test Loss :0.0023811322171241045\n",
      "Epoch :0.2875    Train Loss :0.0007423828938044608    Test Loss :0.0022642347030341625\n",
      "Epoch :0.3    Train Loss :0.0007625168655067682    Test Loss :0.0022047124803066254\n",
      "Epoch :0.3125    Train Loss :0.0006972710834816098    Test Loss :0.002349737798795104\n",
      "Epoch :0.325    Train Loss :0.0006814530352130532    Test Loss :0.0019990019500255585\n",
      "Epoch :0.3375    Train Loss :0.00066494254861027    Test Loss :0.0019617299549281597\n",
      "Epoch :0.35    Train Loss :0.0006462484016083181    Test Loss :0.001776449615135789\n",
      "Epoch :0.3625    Train Loss :0.0006016831030137837    Test Loss :0.0020207962952554226\n",
      "Epoch :0.375    Train Loss :0.0005924947327002883    Test Loss :0.0017236887942999601\n",
      "Epoch :0.3875    Train Loss :0.0005685098585672677    Test Loss :0.0017045007552951574\n",
      "Epoch :0.4    Train Loss :0.000570598291233182    Test Loss :0.0016609644517302513\n",
      "Epoch :0.4125    Train Loss :0.0005564739694818854    Test Loss :0.001751433010213077\n",
      "Epoch :0.425    Train Loss :0.0009689121507108212    Test Loss :0.004914096090942621\n",
      "Epoch :0.4375    Train Loss :0.0006332763587124646    Test Loss :0.002604016335681081\n",
      "Epoch :0.45    Train Loss :0.0006186182145029306    Test Loss :0.0019850232638418674\n",
      "Epoch :0.4625    Train Loss :0.0005483093555085361    Test Loss :0.002338995458558202\n",
      "Epoch :0.475    Train Loss :0.0006056453566998243    Test Loss :0.0015181208727881312\n",
      "Epoch :0.4875    Train Loss :0.0005408553406596184    Test Loss :0.0016666012816131115\n",
      "Epoch :0.5    Train Loss :0.0005709607503376901    Test Loss :0.0017658318392932415\n",
      "Epoch :0.5125    Train Loss :0.0004980677622370422    Test Loss :0.0018267517443746328\n",
      "Epoch :0.525    Train Loss :0.0004830605466850102    Test Loss :0.0016347428318113089\n",
      "Epoch :0.5375    Train Loss :0.00048617064021527767    Test Loss :0.00150247139390558\n",
      "Epoch :0.55    Train Loss :0.00043594103772193193    Test Loss :0.001562035409733653\n",
      "Epoch :0.5625    Train Loss :0.0004350215313024819    Test Loss :0.00145399896427989\n",
      "Epoch :0.575    Train Loss :0.0014697564765810966    Test Loss :0.007621430326253176\n",
      "Epoch :0.5875    Train Loss :0.000868273142259568    Test Loss :0.0028736982494592667\n",
      "Epoch :0.6    Train Loss :0.0005439056549221277    Test Loss :0.0015220451168715954\n",
      "Epoch :0.6125    Train Loss :0.0006314572528935969    Test Loss :0.001542425132356584\n",
      "Epoch :0.625    Train Loss :0.0005195536068640649    Test Loss :0.001726354705169797\n",
      "Epoch :0.6375    Train Loss :0.0005149283679202199    Test Loss :0.001532054622657597\n",
      "Epoch :0.65    Train Loss :0.00046577429748140275    Test Loss :0.0015469350619241595\n",
      "Epoch :0.6625    Train Loss :0.0004673900257330388    Test Loss :0.0016053745057433844\n",
      "Epoch :0.675    Train Loss :0.00044717348646372557    Test Loss :0.0018050693906843662\n",
      "Epoch :0.6875    Train Loss :0.00043117537279613316    Test Loss :0.001649405574426055\n",
      "Epoch :0.7    Train Loss :0.00041456130566075444    Test Loss :0.0014920569956302643\n",
      "Epoch :0.7125    Train Loss :0.000412416469771415    Test Loss :0.0017086234875023365\n",
      "Epoch :0.725    Train Loss :0.00040254692430607975    Test Loss :0.001619194052182138\n",
      "Epoch :0.7375    Train Loss :0.00037254017661325634    Test Loss :0.0015980430180206895\n",
      "Epoch :0.75    Train Loss :0.00039750177529640496    Test Loss :0.0015555158024653792\n",
      "Epoch :0.7625    Train Loss :0.0009176763123832643    Test Loss :0.0019748425111174583\n",
      "Epoch :0.775    Train Loss :0.000457095360616222    Test Loss :0.0018377292435616255\n",
      "Epoch :0.7875    Train Loss :0.0005699442117474973    Test Loss :0.0017849482828751206\n",
      "Epoch :0.8    Train Loss :0.00041169498581439257    Test Loss :0.002097388496622443\n",
      "Epoch :0.8125    Train Loss :0.00037155713653191924    Test Loss :0.0017379828495904803\n",
      "Epoch :0.825    Train Loss :0.0003586076491046697    Test Loss :0.0016879042377695441\n",
      "Epoch :0.8375    Train Loss :0.0003861768345814198    Test Loss :0.0016026283847168088\n",
      "Epoch :0.85    Train Loss :0.0003605298406910151    Test Loss :0.0015567385125905275\n",
      "Epoch :0.8625    Train Loss :0.00033646434894762933    Test Loss :0.0014977836981415749\n",
      "Epoch :0.875    Train Loss :0.00034978968324139714    Test Loss :0.0016159856459125876\n",
      "Epoch :0.8875    Train Loss :0.00033241097116842866    Test Loss :0.0015377519885078073\n",
      "Epoch :0.9    Train Loss :0.00033604662166908383    Test Loss :0.0014596240362152457\n",
      "Epoch :0.9125    Train Loss :0.00034197402419522405    Test Loss :0.0014830541331321\n",
      "Epoch :0.925    Train Loss :0.0003553729329723865    Test Loss :0.0013850529212504625\n",
      "Epoch :0.9375    Train Loss :0.00033527068444527686    Test Loss :0.001522465143352747\n",
      "Epoch :0.95    Train Loss :0.0003556799201760441    Test Loss :0.0016339143039658666\n",
      "Epoch :0.9625    Train Loss :0.000325189670547843    Test Loss :0.0014490507310256362\n",
      "Epoch :0.975    Train Loss :0.0003911452367901802    Test Loss :0.001742898253723979\n",
      "Epoch :0.9875    Train Loss :0.00039748966810293496    Test Loss :0.001810409245081246\n",
      "Epoch :1.0    Train Loss :0.0003611856373026967    Test Loss :0.0014197444543242455\n",
      "RMSE: 54.31488602550357\n",
      "MAE: 52.49297827476807\n",
      "MAPE: 46.47341027678319%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 100, 'lr': 0.01, 'num_layers': 5}\n",
      "total:  19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.1131158396601677    Test Loss :0.31177544593811035\n",
      "Epoch :0.025    Train Loss :0.8957935571670532    Test Loss :0.2680964767932892\n",
      "Epoch :0.0375    Train Loss :0.040991708636283875    Test Loss :0.4782993793487549\n",
      "Epoch :0.05    Train Loss :0.03931413218379021    Test Loss :0.17281657457351685\n",
      "Epoch :0.0625    Train Loss :0.040942706167697906    Test Loss :0.10509294271469116\n",
      "Epoch :0.075    Train Loss :0.034515175968408585    Test Loss :0.21465396881103516\n",
      "Epoch :0.0875    Train Loss :0.022094601765275    Test Loss :0.08066607266664505\n",
      "Epoch :0.1    Train Loss :0.011974858120083809    Test Loss :0.034132152795791626\n",
      "Epoch :0.1125    Train Loss :0.004011654295027256    Test Loss :0.025525908917188644\n",
      "Epoch :0.125    Train Loss :0.004745400045067072    Test Loss :0.007986986078321934\n",
      "Epoch :0.1375    Train Loss :0.003618750488385558    Test Loss :0.01754387654364109\n",
      "Epoch :0.15    Train Loss :0.0023392499424517155    Test Loss :0.012352030724287033\n",
      "Epoch :0.1625    Train Loss :0.0018574263667687774    Test Loss :0.004756115842610598\n",
      "Epoch :0.175    Train Loss :0.0013560166116803885    Test Loss :0.005337650887668133\n",
      "Epoch :0.1875    Train Loss :0.0013872208073735237    Test Loss :0.00960664264857769\n",
      "Epoch :0.2    Train Loss :0.0012564811622723937    Test Loss :0.005045203026384115\n",
      "Epoch :0.2125    Train Loss :0.0012328778393566608    Test Loss :0.0041730874218046665\n",
      "Epoch :0.225    Train Loss :0.0010358785511925817    Test Loss :0.0045930263586342335\n",
      "Epoch :0.2375    Train Loss :0.0010778602445498109    Test Loss :0.004836386069655418\n",
      "Epoch :0.25    Train Loss :0.0010249741608276963    Test Loss :0.004226220306009054\n",
      "Epoch :0.2625    Train Loss :0.0009721844107843935    Test Loss :0.0039042204152792692\n",
      "Epoch :0.275    Train Loss :0.0009479698492214084    Test Loss :0.004345915280282497\n",
      "Epoch :0.2875    Train Loss :0.0009190336568281054    Test Loss :0.003849207190796733\n",
      "Epoch :0.3    Train Loss :0.0009103378397412598    Test Loss :0.0038720062002539635\n",
      "Epoch :0.3125    Train Loss :0.0008634795667603612    Test Loss :0.003783809719607234\n",
      "Epoch :0.325    Train Loss :0.0008631407981738448    Test Loss :0.0036227500531822443\n",
      "Epoch :0.3375    Train Loss :0.0008707841043360531    Test Loss :0.003418077016249299\n",
      "Epoch :0.35    Train Loss :0.0008173506357707083    Test Loss :0.0032397168688476086\n",
      "Epoch :0.3625    Train Loss :0.0008119696285575628    Test Loss :0.00326350424438715\n",
      "Epoch :0.375    Train Loss :0.0008049699827097356    Test Loss :0.0034575846511870623\n",
      "Epoch :0.3875    Train Loss :0.0007677432149648666    Test Loss :0.0030445994343608618\n",
      "Epoch :0.4    Train Loss :0.0007558175129815936    Test Loss :0.003246621461585164\n",
      "Epoch :0.4125    Train Loss :0.0007621791446581483    Test Loss :0.0027559844311326742\n",
      "Epoch :0.425    Train Loss :0.0007301789009943604    Test Loss :0.0027277737390249968\n",
      "Epoch :0.4375    Train Loss :0.0007268073386512697    Test Loss :0.002695430302992463\n",
      "Epoch :0.45    Train Loss :0.0006969263195060194    Test Loss :0.002759436145424843\n",
      "Epoch :0.4625    Train Loss :0.0006884749745950103    Test Loss :0.0027783687692135572\n",
      "Epoch :0.475    Train Loss :0.0006627095863223076    Test Loss :0.0026294609997421503\n",
      "Epoch :0.4875    Train Loss :0.0006726229912601411    Test Loss :0.0027050261851400137\n",
      "Epoch :0.5    Train Loss :0.000677851028740406    Test Loss :0.0025625997222959995\n",
      "Epoch :0.5125    Train Loss :0.0006664323154836893    Test Loss :0.0026703812181949615\n",
      "Epoch :0.525    Train Loss :0.0006486037163995206    Test Loss :0.0024556354619562626\n",
      "Epoch :0.5375    Train Loss :0.0006328829913400114    Test Loss :0.0024880473501980305\n",
      "Epoch :0.55    Train Loss :0.000628247216809541    Test Loss :0.002361007034778595\n",
      "Epoch :0.5625    Train Loss :0.0006184650119394064    Test Loss :0.0024123317562043667\n",
      "Epoch :0.575    Train Loss :0.0006028234492987394    Test Loss :0.0025620164815336466\n",
      "Epoch :0.5875    Train Loss :0.0006208226550370455    Test Loss :0.002337133977562189\n",
      "Epoch :0.6    Train Loss :0.0005912897177040577    Test Loss :0.0021787667647004128\n",
      "Epoch :0.6125    Train Loss :0.0006172575522214174    Test Loss :0.002255223225802183\n",
      "Epoch :0.625    Train Loss :0.0005822511739097536    Test Loss :0.002216191729530692\n",
      "Epoch :0.6375    Train Loss :0.0005903012934140861    Test Loss :0.001982453977689147\n",
      "Epoch :0.65    Train Loss :0.0006026308983564377    Test Loss :0.002019602572545409\n",
      "Epoch :0.6625    Train Loss :0.0005335654132068157    Test Loss :0.0023354804143309593\n",
      "Epoch :0.675    Train Loss :0.0005690938560292125    Test Loss :0.0019604796543717384\n",
      "Epoch :0.6875    Train Loss :0.0005463287816382945    Test Loss :0.0020954387728124857\n",
      "Epoch :0.7    Train Loss :0.0005559272249229252    Test Loss :0.0021116009447723627\n",
      "Epoch :0.7125    Train Loss :0.0005698351887986064    Test Loss :0.001976354280486703\n",
      "Epoch :0.725    Train Loss :0.0005754943704232574    Test Loss :0.002045904053375125\n",
      "Epoch :0.7375    Train Loss :0.0005232169642113149    Test Loss :0.002025554422289133\n",
      "Epoch :0.75    Train Loss :0.0005417121574282646    Test Loss :0.0020434826146811247\n",
      "Epoch :0.7625    Train Loss :0.0005270986002869904    Test Loss :0.0017721157055348158\n",
      "Epoch :0.775    Train Loss :0.0005121800932101905    Test Loss :0.0018410972552374005\n",
      "Epoch :0.7875    Train Loss :0.0005245880456641316    Test Loss :0.0017524927388876677\n",
      "Epoch :0.8    Train Loss :0.0005220129387453198    Test Loss :0.0017896583303809166\n",
      "Epoch :0.8125    Train Loss :0.0004891786957159638    Test Loss :0.0019256261875852942\n",
      "Epoch :0.825    Train Loss :0.0005009423475712538    Test Loss :0.0016048256075009704\n",
      "Epoch :0.8375    Train Loss :0.0005070714978501201    Test Loss :0.0018646790413185954\n",
      "Epoch :0.85    Train Loss :0.0005219665472395718    Test Loss :0.0018771080067381263\n",
      "Epoch :0.8625    Train Loss :0.0004659470869228244    Test Loss :0.001799250952899456\n",
      "Epoch :0.875    Train Loss :0.0004875620361417532    Test Loss :0.0015341062098741531\n",
      "Epoch :0.8875    Train Loss :0.0005164939793758094    Test Loss :0.001715609454549849\n",
      "Epoch :0.9    Train Loss :0.0004968962166458368    Test Loss :0.0015943045727908611\n",
      "Epoch :0.9125    Train Loss :0.00047916139010339975    Test Loss :0.0015527511714026332\n",
      "Epoch :0.925    Train Loss :0.00048387638526037335    Test Loss :0.0015457950066775084\n",
      "Epoch :0.9375    Train Loss :0.00046084471978247166    Test Loss :0.0016929007833823562\n",
      "Epoch :0.95    Train Loss :0.0004851429257541895    Test Loss :0.0014648542273789644\n",
      "Epoch :0.9625    Train Loss :0.0004535834304988384    Test Loss :0.0016124725807458162\n",
      "Epoch :0.975    Train Loss :0.0004685295280069113    Test Loss :0.001499971724115312\n",
      "Epoch :0.9875    Train Loss :0.0004989054286852479    Test Loss :0.0015241412911564112\n",
      "Epoch :1.0    Train Loss :0.00044618648826144636    Test Loss :0.0015487616183236241\n",
      "RMSE: 10.783045461924155\n",
      "MAE: 8.922363504632234\n",
      "MAPE: 7.634134206395772%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 100, 'lr': 0.02, 'num_layers': 2}\n",
      "total:  22.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.07833925634622574    Test Loss :3.180297374725342\n",
      "Epoch :0.025    Train Loss :0.13037298619747162    Test Loss :0.1052226796746254\n",
      "Epoch :0.0375    Train Loss :0.0442747101187706    Test Loss :0.20231598615646362\n",
      "Epoch :0.05    Train Loss :0.0405411422252655    Test Loss :0.2488919347524643\n",
      "Epoch :0.0625    Train Loss :0.04049225524067879    Test Loss :0.18989697098731995\n",
      "Epoch :0.075    Train Loss :0.04054643586277962    Test Loss :0.24529334902763367\n",
      "Epoch :0.0875    Train Loss :0.04059682413935661    Test Loss :0.1969740390777588\n",
      "Epoch :0.1    Train Loss :0.04060044512152672    Test Loss :0.23745745420455933\n",
      "Epoch :0.1125    Train Loss :0.04059993103146553    Test Loss :0.20321184396743774\n",
      "Epoch :0.125    Train Loss :0.040603265166282654    Test Loss :0.2307220697402954\n",
      "Epoch :0.1375    Train Loss :0.04057038947939873    Test Loss :0.2099047303199768\n",
      "Epoch :0.15    Train Loss :0.040528204292058945    Test Loss :0.22302758693695068\n",
      "Epoch :0.1625    Train Loss :0.040483079850673676    Test Loss :0.2174348682165146\n",
      "Epoch :0.175    Train Loss :0.04045158252120018    Test Loss :0.21650828421115875\n",
      "Epoch :0.1875    Train Loss :0.04045988991856575    Test Loss :0.22141709923744202\n",
      "Epoch :0.2    Train Loss :0.040458280593156815    Test Loss :0.2157628834247589\n",
      "Epoch :0.2125    Train Loss :0.04045968875288963    Test Loss :0.219254270195961\n",
      "Epoch :0.225    Train Loss :0.040456537157297134    Test Loss :0.21890340745449066\n",
      "Epoch :0.2375    Train Loss :0.04045388475060463    Test Loss :0.21707293391227722\n",
      "Epoch :0.25    Train Loss :0.04045601561665535    Test Loss :0.2190210372209549\n",
      "Epoch :0.2625    Train Loss :0.040453407913446426    Test Loss :0.2184719741344452\n",
      "Epoch :0.275    Train Loss :0.040455058217048645    Test Loss :0.21781937777996063\n",
      "Epoch :0.2875    Train Loss :0.04043302312493324    Test Loss :0.21843940019607544\n",
      "Epoch :0.3    Train Loss :0.0404517762362957    Test Loss :0.21833151578903198\n",
      "Epoch :0.3125    Train Loss :0.040427472442388535    Test Loss :0.21799471974372864\n",
      "Epoch :0.325    Train Loss :0.04042736440896988    Test Loss :0.2178667187690735\n",
      "Epoch :0.3375    Train Loss :0.04034459963440895    Test Loss :0.21701976656913757\n",
      "Epoch :0.35    Train Loss :0.040234342217445374    Test Loss :0.21708637475967407\n",
      "Epoch :0.3625    Train Loss :0.04007170349359512    Test Loss :0.21641404926776886\n",
      "Epoch :0.375    Train Loss :0.039783306419849396    Test Loss :0.21149305999279022\n",
      "Epoch :0.3875    Train Loss :0.03888550028204918    Test Loss :0.20493781566619873\n",
      "Epoch :0.4    Train Loss :0.038907602429389954    Test Loss :0.20825538039207458\n",
      "Epoch :0.4125    Train Loss :0.04086538404226303    Test Loss :0.213829904794693\n",
      "Epoch :0.425    Train Loss :0.04059465974569321    Test Loss :0.20510834455490112\n",
      "Epoch :0.4375    Train Loss :0.040521763265132904    Test Loss :0.21463900804519653\n",
      "Epoch :0.45    Train Loss :0.040547605603933334    Test Loss :0.22515732049942017\n",
      "Epoch :0.4625    Train Loss :0.040470581501722336    Test Loss :0.2226097583770752\n",
      "Epoch :0.475    Train Loss :0.04048903286457062    Test Loss :0.21576540172100067\n",
      "Epoch :0.4875    Train Loss :0.04046814516186714    Test Loss :0.2149643898010254\n",
      "Epoch :0.5    Train Loss :0.04046141356229782    Test Loss :0.2183242291212082\n",
      "Epoch :0.5125    Train Loss :0.04047246277332306    Test Loss :0.22039376199245453\n",
      "Epoch :0.525    Train Loss :0.040460892021656036    Test Loss :0.21949653327465057\n",
      "Epoch :0.5375    Train Loss :0.040452904999256134    Test Loss :0.2181226760149002\n",
      "Epoch :0.55    Train Loss :0.04047772288322449    Test Loss :0.21725870668888092\n",
      "Epoch :0.5625    Train Loss :0.040454380214214325    Test Loss :0.2159818559885025\n",
      "Epoch :0.575    Train Loss :0.040461465716362    Test Loss :0.21726727485656738\n",
      "Epoch :0.5875    Train Loss :0.04045691713690758    Test Loss :0.21867695450782776\n",
      "Epoch :0.6    Train Loss :0.040467508137226105    Test Loss :0.2192791998386383\n",
      "Epoch :0.6125    Train Loss :0.04046204686164856    Test Loss :0.2189827412366867\n",
      "Epoch :0.625    Train Loss :0.04045142978429794    Test Loss :0.21938344836235046\n",
      "Epoch :0.6375    Train Loss :0.04047068580985069    Test Loss :0.23408904671669006\n",
      "Epoch :0.65    Train Loss :0.04054403305053711    Test Loss :0.22346585988998413\n",
      "Epoch :0.6625    Train Loss :0.0405784472823143    Test Loss :0.21451124548912048\n",
      "Epoch :0.675    Train Loss :0.040496066212654114    Test Loss :0.21143800020217896\n",
      "Epoch :0.6875    Train Loss :0.04045563191175461    Test Loss :0.2130010575056076\n",
      "Epoch :0.7    Train Loss :0.040463708341121674    Test Loss :0.2162618190050125\n",
      "Epoch :0.7125    Train Loss :0.040478192269802094    Test Loss :0.2190643846988678\n",
      "Epoch :0.725    Train Loss :0.04046882688999176    Test Loss :0.22041738033294678\n",
      "Epoch :0.7375    Train Loss :0.04046463593840599    Test Loss :0.22041144967079163\n",
      "Epoch :0.75    Train Loss :0.04046327993273735    Test Loss :0.21971628069877625\n",
      "Epoch :0.7625    Train Loss :0.04046328738331795    Test Loss :0.2188827395439148\n",
      "Epoch :0.775    Train Loss :0.04046053811907768    Test Loss :0.21829496324062347\n",
      "Epoch :0.7875    Train Loss :0.04045891389250755    Test Loss :0.21797867119312286\n",
      "Epoch :0.8    Train Loss :0.040463510900735855    Test Loss :0.21788930892944336\n",
      "Epoch :0.8125    Train Loss :0.040457189083099365    Test Loss :0.21795618534088135\n",
      "Epoch :0.825    Train Loss :0.04045797884464264    Test Loss :0.217950239777565\n",
      "Epoch :0.8375    Train Loss :0.04045602306723595    Test Loss :0.21820607781410217\n",
      "Epoch :0.85    Train Loss :0.04046320915222168    Test Loss :0.21823719143867493\n",
      "Epoch :0.8625    Train Loss :0.04044859856367111    Test Loss :0.21842224895954132\n",
      "Epoch :0.875    Train Loss :0.040462955832481384    Test Loss :0.21812985837459564\n",
      "Epoch :0.8875    Train Loss :0.04046271741390228    Test Loss :0.21800604462623596\n",
      "Epoch :0.9    Train Loss :0.04046494886279106    Test Loss :0.21806029975414276\n",
      "Epoch :0.9125    Train Loss :0.04046439006924629    Test Loss :0.21822388470172882\n",
      "Epoch :0.925    Train Loss :0.040460504591464996    Test Loss :0.21823303401470184\n",
      "Epoch :0.9375    Train Loss :0.040458764880895615    Test Loss :0.21830393373966217\n",
      "Epoch :0.95    Train Loss :0.0404479093849659    Test Loss :0.21853017807006836\n",
      "Epoch :0.9625    Train Loss :0.04042265936732292    Test Loss :0.22263503074645996\n",
      "Epoch :0.975    Train Loss :0.040545426309108734    Test Loss :0.24919775128364563\n",
      "Epoch :0.9875    Train Loss :0.04052714258432388    Test Loss :0.2410777360200882\n",
      "Epoch :1.0    Train Loss :0.04051889479160309    Test Loss :0.2353547364473343\n",
      "RMSE: 46.91498401189351\n",
      "MAE: 46.253784926311596\n",
      "MAPE: 40.54489974219607%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 100, 'lr': 0.02, 'num_layers': 5}\n",
      "total:  25.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.03980329632759094    Test Loss :0.2954349219799042\n",
      "Epoch :0.016666666666666666    Train Loss :0.03584594652056694    Test Loss :0.1802894026041031\n",
      "Epoch :0.025    Train Loss :0.026132168248295784    Test Loss :0.04660642892122269\n",
      "Epoch :0.03333333333333333    Train Loss :0.009474233724176884    Test Loss :0.045817017555236816\n",
      "Epoch :0.041666666666666664    Train Loss :0.0025352926459163427    Test Loss :0.010563629679381847\n",
      "Epoch :0.05    Train Loss :0.004886448848992586    Test Loss :0.020166411995887756\n",
      "Epoch :0.058333333333333334    Train Loss :0.0016908919205889106    Test Loss :0.0056445603258907795\n",
      "Epoch :0.06666666666666667    Train Loss :0.0017902656691148877    Test Loss :0.005179674830287695\n",
      "Epoch :0.075    Train Loss :0.0016282823635265231    Test Loss :0.01336975209414959\n",
      "Epoch :0.08333333333333333    Train Loss :0.0011221494060009718    Test Loss :0.004572511650621891\n",
      "Epoch :0.09166666666666666    Train Loss :0.001172041636891663    Test Loss :0.0050375075079500675\n",
      "Epoch :0.1    Train Loss :0.001039815484546125    Test Loss :0.005761567037552595\n",
      "Epoch :0.10833333333333334    Train Loss :0.0008596954867243767    Test Loss :0.002972582820802927\n",
      "Epoch :0.11666666666666667    Train Loss :0.0008263032068498433    Test Loss :0.003605213016271591\n",
      "Epoch :0.125    Train Loss :0.0007954475004225969    Test Loss :0.0037457726430147886\n",
      "Epoch :0.13333333333333333    Train Loss :0.0007022565696388483    Test Loss :0.002797585679218173\n",
      "Epoch :0.14166666666666666    Train Loss :0.0007047288818284869    Test Loss :0.003116330597549677\n",
      "Epoch :0.15    Train Loss :0.0006683061365038157    Test Loss :0.003094030311331153\n",
      "Epoch :0.15833333333333333    Train Loss :0.0006693779723718762    Test Loss :0.0024216442834585905\n",
      "Epoch :0.16666666666666666    Train Loss :0.0006452223751693964    Test Loss :0.0025274804793298244\n",
      "Epoch :0.175    Train Loss :0.0006173430010676384    Test Loss :0.0022072906140238047\n",
      "Epoch :0.18333333333333332    Train Loss :0.000615488039329648    Test Loss :0.0023439438082277775\n",
      "Epoch :0.19166666666666668    Train Loss :0.0005791087751276791    Test Loss :0.0021916476543992758\n",
      "Epoch :0.2    Train Loss :0.0005555514362640679    Test Loss :0.0019893997814506292\n",
      "Epoch :0.20833333333333334    Train Loss :0.0005351026193238795    Test Loss :0.0019881632179021835\n",
      "Epoch :0.21666666666666667    Train Loss :0.0004997101495973766    Test Loss :0.0019456228474155068\n",
      "Epoch :0.225    Train Loss :0.0005264692008495331    Test Loss :0.001916330074891448\n",
      "Epoch :0.23333333333333334    Train Loss :0.0004896532627753913    Test Loss :0.0018427615286782384\n",
      "Epoch :0.24166666666666667    Train Loss :0.0004961040103808045    Test Loss :0.0016613197512924671\n",
      "Epoch :0.25    Train Loss :0.0005047591403126717    Test Loss :0.0015761854592710733\n",
      "Epoch :0.25833333333333336    Train Loss :0.00047715334221720695    Test Loss :0.001640251255594194\n",
      "Epoch :0.26666666666666666    Train Loss :0.0004888738039880991    Test Loss :0.001512595801614225\n",
      "Epoch :0.275    Train Loss :0.00046611600555479527    Test Loss :0.0016063188668340445\n",
      "Epoch :0.2833333333333333    Train Loss :0.00043877650750800967    Test Loss :0.0015837425598874688\n",
      "Epoch :0.2916666666666667    Train Loss :0.00046361240674741566    Test Loss :0.0014332724967971444\n",
      "Epoch :0.3    Train Loss :0.0004310590447857976    Test Loss :0.0014928365126252174\n",
      "Epoch :0.30833333333333335    Train Loss :0.000430418731411919    Test Loss :0.0014551635831594467\n",
      "Epoch :0.31666666666666665    Train Loss :0.0004524342075455934    Test Loss :0.0014127230970188975\n",
      "Epoch :0.325    Train Loss :0.000416975817643106    Test Loss :0.0012492695823311806\n",
      "Epoch :0.3333333333333333    Train Loss :0.0004198679234832525    Test Loss :0.0013936763862147927\n",
      "Epoch :0.3416666666666667    Train Loss :0.000417965289670974    Test Loss :0.0012560011819005013\n",
      "Epoch :0.35    Train Loss :0.00039448871393688023    Test Loss :0.001162319560535252\n",
      "Epoch :0.35833333333333334    Train Loss :0.0003913874679710716    Test Loss :0.0011242983164265752\n",
      "Epoch :0.36666666666666664    Train Loss :0.00038671024958603084    Test Loss :0.0012721656821668148\n",
      "Epoch :0.375    Train Loss :0.0003714782069437206    Test Loss :0.0010890624253079295\n",
      "Epoch :0.38333333333333336    Train Loss :0.00036492105573415756    Test Loss :0.0010281162103638053\n",
      "Epoch :0.39166666666666666    Train Loss :0.00035482540261000395    Test Loss :0.0011686914367601275\n",
      "Epoch :0.4    Train Loss :0.00035456419573165476    Test Loss :0.00114931829739362\n",
      "Epoch :0.4083333333333333    Train Loss :0.00038082199171185493    Test Loss :0.0010145279811695218\n",
      "Epoch :0.4166666666666667    Train Loss :0.0003697550855576992    Test Loss :0.0009945376077666879\n",
      "Epoch :0.425    Train Loss :0.0003464417241048068    Test Loss :0.0010979194194078445\n",
      "Epoch :0.43333333333333335    Train Loss :0.0003553238930180669    Test Loss :0.001006551319733262\n",
      "Epoch :0.44166666666666665    Train Loss :0.0003473341348581016    Test Loss :0.0008476965595036745\n",
      "Epoch :0.45    Train Loss :0.0003354526706971228    Test Loss :0.0010134944459423423\n",
      "Epoch :0.4583333333333333    Train Loss :0.0003310111351311207    Test Loss :0.0009749613236635923\n",
      "Epoch :0.4666666666666667    Train Loss :0.00030961737502366304    Test Loss :0.0008755230810493231\n",
      "Epoch :0.475    Train Loss :0.0003121795307379216    Test Loss :0.0008339278865605593\n",
      "Epoch :0.48333333333333334    Train Loss :0.00031391516677103937    Test Loss :0.0008648067596368492\n",
      "Epoch :0.49166666666666664    Train Loss :0.0003121734189335257    Test Loss :0.0008252886473201215\n",
      "Epoch :0.5    Train Loss :0.00039788722642697394    Test Loss :0.0013673270586878061\n",
      "Epoch :0.5083333333333333    Train Loss :0.0002983420854434371    Test Loss :0.0010159946978092194\n",
      "Epoch :0.5166666666666667    Train Loss :0.0004790782113559544    Test Loss :0.0009096575668081641\n",
      "Epoch :0.525    Train Loss :0.00040467505459673703    Test Loss :0.0010188213782384992\n",
      "Epoch :0.5333333333333333    Train Loss :0.00035482991370372474    Test Loss :0.0007998422952368855\n",
      "Epoch :0.5416666666666666    Train Loss :0.00035345397191122174    Test Loss :0.0008774315356276929\n",
      "Epoch :0.55    Train Loss :0.0003330202598590404    Test Loss :0.0007299359422177076\n",
      "Epoch :0.5583333333333333    Train Loss :0.0003198792110197246    Test Loss :0.0009140265756286681\n",
      "Epoch :0.5666666666666667    Train Loss :0.0002929705078713596    Test Loss :0.000715451140422374\n",
      "Epoch :0.575    Train Loss :0.00028176180785521865    Test Loss :0.0007904229569248855\n",
      "Epoch :0.5833333333333334    Train Loss :0.00027495017275214195    Test Loss :0.0007051072316244245\n",
      "Epoch :0.5916666666666667    Train Loss :0.00027562238392420113    Test Loss :0.0006773211644031107\n",
      "Epoch :0.6    Train Loss :0.00027616883744485676    Test Loss :0.0006347648450173438\n",
      "Epoch :0.6083333333333333    Train Loss :0.0002685035578906536    Test Loss :0.0006515640998259187\n",
      "Epoch :0.6166666666666667    Train Loss :0.0002798439818434417    Test Loss :0.0006077280850149691\n",
      "Epoch :0.625    Train Loss :0.0003132623969577253    Test Loss :0.0007688433397561312\n",
      "Epoch :0.6333333333333333    Train Loss :0.0002661013859324157    Test Loss :0.0005707209347747266\n",
      "Epoch :0.6416666666666667    Train Loss :0.0003408637421671301    Test Loss :0.0009514613775536418\n",
      "Epoch :0.65    Train Loss :0.0002611773961689323    Test Loss :0.0005931464838795364\n",
      "Epoch :0.6583333333333333    Train Loss :0.0003346411103848368    Test Loss :0.0007998448563739657\n",
      "Epoch :0.6666666666666666    Train Loss :0.0002548877673689276    Test Loss :0.0005780700012110174\n",
      "Epoch :0.675    Train Loss :0.00029137605451978743    Test Loss :0.0007792206597514451\n",
      "Epoch :0.6833333333333333    Train Loss :0.00024827956804074347    Test Loss :0.0005819127545692027\n",
      "Epoch :0.6916666666666667    Train Loss :0.00027252279687672853    Test Loss :0.0006602175999432802\n",
      "Epoch :0.7    Train Loss :0.00029773713322356343    Test Loss :0.0007771175587549806\n",
      "Epoch :0.7083333333333334    Train Loss :0.0003127171949017793    Test Loss :0.0006636859616264701\n",
      "Epoch :0.7166666666666667    Train Loss :0.00023473742476198822    Test Loss :0.000546255789231509\n",
      "Epoch :0.725    Train Loss :0.00027076381957158446    Test Loss :0.0006813443615101278\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7333333333333333    Train Loss :0.00022747358889319003    Test Loss :0.0005745443049818277\n",
      "Epoch :0.7416666666666667    Train Loss :0.00030015219817869365    Test Loss :0.0008000400266610086\n",
      "Epoch :0.75    Train Loss :0.00036229475517757237    Test Loss :0.0008634250261820853\n",
      "Epoch :0.7583333333333333    Train Loss :0.00034460326423868537    Test Loss :0.0009168384131044149\n",
      "Epoch :0.7666666666666667    Train Loss :0.0003044312179554254    Test Loss :0.0006859115092083812\n",
      "Epoch :0.775    Train Loss :0.0002484121359884739    Test Loss :0.0005748920375481248\n",
      "Epoch :0.7833333333333333    Train Loss :0.0002356487384531647    Test Loss :0.0005616085836663842\n",
      "Epoch :0.7916666666666666    Train Loss :0.00022836358402855694    Test Loss :0.0005590848159044981\n",
      "Epoch :0.8    Train Loss :0.00021986744832247496    Test Loss :0.0005487375310622156\n",
      "Epoch :0.8083333333333333    Train Loss :0.00022678020468447357    Test Loss :0.0005162358283996582\n",
      "Epoch :0.8166666666666667    Train Loss :0.0002793402236420661    Test Loss :0.000768330879509449\n",
      "Epoch :0.825    Train Loss :0.00022218866797629744    Test Loss :0.00053527916315943\n",
      "Epoch :0.8333333333333334    Train Loss :0.00022498384350910783    Test Loss :0.0005222968757152557\n",
      "Epoch :0.8416666666666667    Train Loss :0.00023388900444842875    Test Loss :0.0005767814582213759\n",
      "Epoch :0.85    Train Loss :0.00032071201712824404    Test Loss :0.0009915151167660952\n",
      "Epoch :0.8583333333333333    Train Loss :0.0003386235621292144    Test Loss :0.00048196909483522177\n",
      "Epoch :0.8666666666666667    Train Loss :0.0002577536797616631    Test Loss :0.0005099124973639846\n",
      "Epoch :0.875    Train Loss :0.00022183236433193088    Test Loss :0.0005373742314986885\n",
      "Epoch :0.8833333333333333    Train Loss :0.00021568025113083422    Test Loss :0.0005520392442122102\n",
      "Epoch :0.8916666666666667    Train Loss :0.00021679238125216216    Test Loss :0.0005785374669358134\n",
      "Epoch :0.9    Train Loss :0.00021582277258858085    Test Loss :0.0005764278466813266\n",
      "Epoch :0.9083333333333333    Train Loss :0.00021346111316233873    Test Loss :0.0005642203032039106\n",
      "Epoch :0.9166666666666666    Train Loss :0.00022968182747717947    Test Loss :0.0005113168153911829\n",
      "Epoch :0.925    Train Loss :0.00022942265786696225    Test Loss :0.0005170259391888976\n",
      "Epoch :0.9333333333333333    Train Loss :0.00021493231179192662    Test Loss :0.0004683970473706722\n",
      "Epoch :0.9416666666666667    Train Loss :0.0002109167689923197    Test Loss :0.0004498301132116467\n",
      "Epoch :0.95    Train Loss :0.00025878159794956446    Test Loss :0.0007848228560760617\n",
      "Epoch :0.9583333333333334    Train Loss :0.0005049832980148494    Test Loss :0.0010791480308398604\n",
      "Epoch :0.9666666666666667    Train Loss :0.00031086502713151276    Test Loss :0.000568834540899843\n",
      "Epoch :0.975    Train Loss :0.00026768987299874425    Test Loss :0.0004817571316380054\n",
      "Epoch :0.9833333333333333    Train Loss :0.0002176444249926135    Test Loss :0.0005772364092990756\n",
      "Epoch :0.9916666666666667    Train Loss :0.00024385584401898086    Test Loss :0.0005885289283469319\n",
      "Epoch :1.0    Train Loss :0.00020887698337901384    Test Loss :0.0004908642149530351\n",
      "RMSE: 16.29095474231589\n",
      "MAE: 13.09656497042361\n",
      "MAPE: 11.137640713210018%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 600, 'hid_size': 50, 'lr': 0.01, 'num_layers': 2}\n",
      "total:  28.000000000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.05850108340382576    Test Loss :0.3821878135204315\n",
      "Epoch :0.016666666666666666    Train Loss :0.04165877029299736    Test Loss :0.19635461270809174\n",
      "Epoch :0.025    Train Loss :0.041622694581747055    Test Loss :0.21138650178909302\n",
      "Epoch :0.03333333333333333    Train Loss :0.041813142597675323    Test Loss :0.2520217001438141\n",
      "Epoch :0.041666666666666664    Train Loss :0.04025563970208168    Test Loss :0.21738408505916595\n",
      "Epoch :0.05    Train Loss :0.0377727709710598    Test Loss :0.1884729266166687\n",
      "Epoch :0.058333333333333334    Train Loss :0.017423201352357864    Test Loss :0.10596510022878647\n",
      "Epoch :0.06666666666666667    Train Loss :0.008125724270939827    Test Loss :0.008520069532096386\n",
      "Epoch :0.075    Train Loss :0.004932762589305639    Test Loss :0.009265710599720478\n",
      "Epoch :0.08333333333333333    Train Loss :0.0048841251991689205    Test Loss :0.014940041117370129\n",
      "Epoch :0.09166666666666666    Train Loss :0.002862149616703391    Test Loss :0.004778618458658457\n",
      "Epoch :0.1    Train Loss :0.002323816530406475    Test Loss :0.005472754593938589\n",
      "Epoch :0.10833333333333334    Train Loss :0.001656958949752152    Test Loss :0.009093515574932098\n",
      "Epoch :0.11666666666666667    Train Loss :0.0017593083903193474    Test Loss :0.007010222412645817\n",
      "Epoch :0.125    Train Loss :0.0015145910438150167    Test Loss :0.004651615396142006\n",
      "Epoch :0.13333333333333333    Train Loss :0.0014442711835727096    Test Loss :0.004296992905437946\n",
      "Epoch :0.14166666666666666    Train Loss :0.0013028575340285897    Test Loss :0.004559756256639957\n",
      "Epoch :0.15    Train Loss :0.0012369763571769    Test Loss :0.0039358120411634445\n",
      "Epoch :0.15833333333333333    Train Loss :0.0011756544699892402    Test Loss :0.0036163947079330683\n",
      "Epoch :0.16666666666666666    Train Loss :0.001107164309360087    Test Loss :0.003549454268068075\n",
      "Epoch :0.175    Train Loss :0.001100186607800424    Test Loss :0.0032077825162559748\n",
      "Epoch :0.18333333333333332    Train Loss :0.001003198092803359    Test Loss :0.003165039699524641\n",
      "Epoch :0.19166666666666668    Train Loss :0.0009854957461357117    Test Loss :0.003084259806200862\n",
      "Epoch :0.2    Train Loss :0.0009968482190743089    Test Loss :0.00338739063590765\n",
      "Epoch :0.20833333333333334    Train Loss :0.0009519963641650975    Test Loss :0.002878580242395401\n",
      "Epoch :0.21666666666666667    Train Loss :0.0009098846348933876    Test Loss :0.002826159819960594\n",
      "Epoch :0.225    Train Loss :0.0008749820990487933    Test Loss :0.0030382974073290825\n",
      "Epoch :0.23333333333333334    Train Loss :0.0008746426901780069    Test Loss :0.0027320049703121185\n",
      "Epoch :0.24166666666666667    Train Loss :0.0008147412445396185    Test Loss :0.0022991152945905924\n",
      "Epoch :0.25    Train Loss :0.000825477996841073    Test Loss :0.0026567403692752123\n",
      "Epoch :0.25833333333333336    Train Loss :0.0007888274849392474    Test Loss :0.002165040699765086\n",
      "Epoch :0.26666666666666666    Train Loss :0.0007505449466407299    Test Loss :0.0024574962444603443\n",
      "Epoch :0.275    Train Loss :0.0008864263654686511    Test Loss :0.004746895749121904\n",
      "Epoch :0.2833333333333333    Train Loss :0.0009014675160869956    Test Loss :0.003518245881423354\n",
      "Epoch :0.2916666666666667    Train Loss :0.0007235353114083409    Test Loss :0.002786475932225585\n",
      "Epoch :0.3    Train Loss :0.0006874831742607057    Test Loss :0.003040280658751726\n",
      "Epoch :0.30833333333333335    Train Loss :0.000684161379467696    Test Loss :0.002145162085071206\n",
      "Epoch :0.31666666666666665    Train Loss :0.000648808665573597    Test Loss :0.0024248550180345774\n",
      "Epoch :0.325    Train Loss :0.0006226841360330582    Test Loss :0.0023085675202310085\n",
      "Epoch :0.3333333333333333    Train Loss :0.0006296797655522823    Test Loss :0.002145499922335148\n",
      "Epoch :0.3416666666666667    Train Loss :0.0006068446091376245    Test Loss :0.0024011568166315556\n",
      "Epoch :0.35    Train Loss :0.0005970092606730759    Test Loss :0.00244358042255044\n",
      "Epoch :0.35833333333333334    Train Loss :0.0005672057159245014    Test Loss :0.002427399856969714\n",
      "Epoch :0.36666666666666664    Train Loss :0.0005458610830828547    Test Loss :0.002498847898095846\n",
      "Epoch :0.375    Train Loss :0.0005826078704558313    Test Loss :0.0029170471243560314\n",
      "Epoch :0.38333333333333336    Train Loss :0.0005567961488850415    Test Loss :0.002480668481439352\n",
      "Epoch :0.39166666666666666    Train Loss :0.000515952066052705    Test Loss :0.002194936154410243\n",
      "Epoch :0.4    Train Loss :0.0005302734789438546    Test Loss :0.0021611417178064585\n",
      "Epoch :0.4083333333333333    Train Loss :0.0005013998015783727    Test Loss :0.0022494241129606962\n",
      "Epoch :0.4166666666666667    Train Loss :0.0005687210359610617    Test Loss :0.0021184501238167286\n",
      "Epoch :0.425    Train Loss :0.0007339391158893704    Test Loss :0.0027145061176270247\n",
      "Epoch :0.43333333333333335    Train Loss :0.0006185845704749227    Test Loss :0.002690190216526389\n",
      "Epoch :0.44166666666666665    Train Loss :0.0005557082477025688    Test Loss :0.002655396703630686\n",
      "Epoch :0.45    Train Loss :0.0005483864224515855    Test Loss :0.0019265367882326245\n",
      "Epoch :0.4583333333333333    Train Loss :0.0004956058692187071    Test Loss :0.0018697389168664813\n",
      "Epoch :0.4666666666666667    Train Loss :0.0004672321956604719    Test Loss :0.00213852827437222\n",
      "Epoch :0.475    Train Loss :0.00045242870692163706    Test Loss :0.0019904032815247774\n",
      "Epoch :0.48333333333333334    Train Loss :0.0004523038223851472    Test Loss :0.0020851818844676018\n",
      "Epoch :0.49166666666666664    Train Loss :0.0004516853950917721    Test Loss :0.001890041632577777\n",
      "Epoch :0.5    Train Loss :0.0004330044030211866    Test Loss :0.0019833568949252367\n",
      "Epoch :0.5083333333333333    Train Loss :0.00043300367542542517    Test Loss :0.001914466149173677\n",
      "Epoch :0.5166666666666667    Train Loss :0.0004280555876903236    Test Loss :0.0018740786472335458\n",
      "Epoch :0.525    Train Loss :0.0004766752535942942    Test Loss :0.0020335845183581114\n",
      "Epoch :0.5333333333333333    Train Loss :0.0005189623334445059    Test Loss :0.0019514921586960554\n",
      "Epoch :0.5416666666666666    Train Loss :0.0004591982578858733    Test Loss :0.002145199105143547\n",
      "Epoch :0.55    Train Loss :0.0004385325883049518    Test Loss :0.002246059477329254\n",
      "Epoch :0.5583333333333333    Train Loss :0.0004258681437931955    Test Loss :0.001985227921977639\n",
      "Epoch :0.5666666666666667    Train Loss :0.00042124095489270985    Test Loss :0.0018031003419309855\n",
      "Epoch :0.575    Train Loss :0.000402452249545604    Test Loss :0.001831914414651692\n",
      "Epoch :0.5833333333333334    Train Loss :0.000418430776335299    Test Loss :0.002042802283540368\n",
      "Epoch :0.5916666666666667    Train Loss :0.00038709453656338155    Test Loss :0.0017670911038294435\n",
      "Epoch :0.6    Train Loss :0.0004032782162539661    Test Loss :0.0017314079450443387\n",
      "Epoch :0.6083333333333333    Train Loss :0.00039289938285946846    Test Loss :0.0016405734932050109\n",
      "Epoch :0.6166666666666667    Train Loss :0.000398296513594687    Test Loss :0.0018339470261707902\n",
      "Epoch :0.625    Train Loss :0.00038139245589263737    Test Loss :0.001791847520507872\n",
      "Epoch :0.6333333333333333    Train Loss :0.00038873168523423374    Test Loss :0.00166415108833462\n",
      "Epoch :0.6416666666666667    Train Loss :0.00039278840995393693    Test Loss :0.0018693073652684689\n",
      "Epoch :0.65    Train Loss :0.00037311395863071084    Test Loss :0.0018812798662111163\n",
      "Epoch :0.6583333333333333    Train Loss :0.0005979728302918375    Test Loss :0.0020936946384608746\n",
      "Epoch :0.6666666666666666    Train Loss :0.00046312855556607246    Test Loss :0.002180259209126234\n",
      "Epoch :0.675    Train Loss :0.0004527288838289678    Test Loss :0.0020651277154684067\n",
      "Epoch :0.6833333333333333    Train Loss :0.0004010361444670707    Test Loss :0.0019873592536896467\n",
      "Epoch :0.6916666666666667    Train Loss :0.0003729313029907644    Test Loss :0.0018486017361283302\n",
      "Epoch :0.7    Train Loss :0.0003912817919626832    Test Loss :0.0018837982788681984\n",
      "Epoch :0.7083333333333334    Train Loss :0.0003903299511875957    Test Loss :0.0019393053371459246\n",
      "Epoch :0.7166666666666667    Train Loss :0.0004640856641344726    Test Loss :0.0023680629674345255\n",
      "Epoch :0.725    Train Loss :0.000407640531193465    Test Loss :0.001957709202542901\n",
      "Epoch :0.7333333333333333    Train Loss :0.0003652766754385084    Test Loss :0.0016974241007119417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7416666666666667    Train Loss :0.00039195208228193223    Test Loss :0.0017890294548124075\n",
      "Epoch :0.75    Train Loss :0.000372263602912426    Test Loss :0.001940265647135675\n",
      "Epoch :0.7583333333333333    Train Loss :0.0003766269364859909    Test Loss :0.001874010544270277\n",
      "Epoch :0.7666666666666667    Train Loss :0.0003642254159785807    Test Loss :0.0015988865634426475\n",
      "Epoch :0.775    Train Loss :0.0003614841552916914    Test Loss :0.0015094017144292593\n",
      "Epoch :0.7833333333333333    Train Loss :0.0004084896936547011    Test Loss :0.0021411229390650988\n",
      "Epoch :0.7916666666666666    Train Loss :0.0003506955981720239    Test Loss :0.0017577960388734937\n",
      "Epoch :0.8    Train Loss :0.00034673811751417816    Test Loss :0.0018537843134254217\n",
      "Epoch :0.8083333333333333    Train Loss :0.00036410699249245226    Test Loss :0.0017830373253673315\n",
      "Epoch :0.8166666666666667    Train Loss :0.0003453102253843099    Test Loss :0.0017105224542319775\n",
      "Epoch :0.825    Train Loss :0.0003735767968464643    Test Loss :0.0018330734455958009\n",
      "Epoch :0.8333333333333334    Train Loss :0.0003580221673473716    Test Loss :0.0017525487346574664\n",
      "Epoch :0.8416666666666667    Train Loss :0.00043526836088858545    Test Loss :0.001644515316002071\n",
      "Epoch :0.85    Train Loss :0.00036968369386158884    Test Loss :0.0016422608168795705\n",
      "Epoch :0.8583333333333333    Train Loss :0.00033465612796135247    Test Loss :0.001775443204678595\n",
      "Epoch :0.8666666666666667    Train Loss :0.0003425978356972337    Test Loss :0.001598465139977634\n",
      "Epoch :0.875    Train Loss :0.00035820077755488455    Test Loss :0.0020742532797157764\n",
      "Epoch :0.8833333333333333    Train Loss :0.0004775298584718257    Test Loss :0.0020139487460255623\n",
      "Epoch :0.8916666666666667    Train Loss :0.00038776491419412196    Test Loss :0.0017462498508393764\n",
      "Epoch :0.9    Train Loss :0.0003412267833482474    Test Loss :0.0018696050392463803\n",
      "Epoch :0.9083333333333333    Train Loss :0.0003385778109077364    Test Loss :0.001831813482567668\n",
      "Epoch :0.9166666666666666    Train Loss :0.00036521401489153504    Test Loss :0.001552837435156107\n",
      "Epoch :0.925    Train Loss :0.00031673553166911006    Test Loss :0.0016928722616285086\n",
      "Epoch :0.9333333333333333    Train Loss :0.0003367119061294943    Test Loss :0.0016705923480913043\n",
      "Epoch :0.9416666666666667    Train Loss :0.0003125298535451293    Test Loss :0.0014636246487498283\n",
      "Epoch :0.95    Train Loss :0.0003145754453726113    Test Loss :0.0015073136892169714\n",
      "Epoch :0.9583333333333334    Train Loss :0.0003168871917296201    Test Loss :0.0016302248695865273\n",
      "Epoch :0.9666666666666667    Train Loss :0.00033115080441348255    Test Loss :0.0015849304618313909\n",
      "Epoch :0.975    Train Loss :0.00033226251252926886    Test Loss :0.0016605631681159139\n",
      "Epoch :0.9833333333333333    Train Loss :0.0003313916677143425    Test Loss :0.0016199591336771846\n",
      "Epoch :0.9916666666666667    Train Loss :0.0003217580378986895    Test Loss :0.0016062591457739472\n",
      "Epoch :1.0    Train Loss :0.0003437754639890045    Test Loss :0.0019125877879559994\n",
      "RMSE: 45.3016767720135\n",
      "MAE: 44.17927419286889\n",
      "MAPE: 38.8217018641637%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 600, 'hid_size': 50, 'lr': 0.01, 'num_layers': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  31.0\n",
      "Epoch :0.008333333333333333    Train Loss :0.051470622420310974    Test Loss :0.2781341075897217\n",
      "Epoch :0.016666666666666666    Train Loss :0.03904242068529129    Test Loss :0.1377013623714447\n",
      "Epoch :0.025    Train Loss :0.0114654041826725    Test Loss :0.010984825901687145\n",
      "Epoch :0.03333333333333333    Train Loss :0.0027199143078178167    Test Loss :0.017558574676513672\n",
      "Epoch :0.041666666666666664    Train Loss :0.0022291545756161213    Test Loss :0.01474975049495697\n",
      "Epoch :0.05    Train Loss :0.0015499088913202286    Test Loss :0.015131507068872452\n",
      "Epoch :0.058333333333333334    Train Loss :0.001293806592002511    Test Loss :0.0051908292807638645\n",
      "Epoch :0.06666666666666667    Train Loss :0.0010854514548555017    Test Loss :0.0033824476413428783\n",
      "Epoch :0.075    Train Loss :0.0009887408232316375    Test Loss :0.003110388992354274\n",
      "Epoch :0.08333333333333333    Train Loss :0.0008145967149175704    Test Loss :0.0035803888458758593\n",
      "Epoch :0.09166666666666666    Train Loss :0.0007238436373881996    Test Loss :0.0029310062527656555\n",
      "Epoch :0.1    Train Loss :0.0006198748596943915    Test Loss :0.0033273014705628157\n",
      "Epoch :0.10833333333333334    Train Loss :0.0005839100922457874    Test Loss :0.0019054636359214783\n",
      "Epoch :0.11666666666666667    Train Loss :0.0005568836932070553    Test Loss :0.0021866161841899157\n",
      "Epoch :0.125    Train Loss :0.0005296237650327384    Test Loss :0.0018623365322127938\n",
      "Epoch :0.13333333333333333    Train Loss :0.0005086025921627879    Test Loss :0.0016468847170472145\n",
      "Epoch :0.14166666666666666    Train Loss :0.0004925561952404678    Test Loss :0.001520983874797821\n",
      "Epoch :0.15    Train Loss :0.00046184772509150207    Test Loss :0.0018824223661795259\n",
      "Epoch :0.15833333333333333    Train Loss :0.0004727925988845527    Test Loss :0.00149498600512743\n",
      "Epoch :0.16666666666666666    Train Loss :0.00045256083831191063    Test Loss :0.0014522033743560314\n",
      "Epoch :0.175    Train Loss :0.0004365627537481487    Test Loss :0.0015014874516054988\n",
      "Epoch :0.18333333333333332    Train Loss :0.0004290109209250659    Test Loss :0.0015146741643548012\n",
      "Epoch :0.19166666666666668    Train Loss :0.0004279326240066439    Test Loss :0.0013824061024934053\n",
      "Epoch :0.2    Train Loss :0.0004062144726049155    Test Loss :0.0012935163686051965\n",
      "Epoch :0.20833333333333334    Train Loss :0.00039317598566412926    Test Loss :0.001363774761557579\n",
      "Epoch :0.21666666666666667    Train Loss :0.00036445376463234425    Test Loss :0.0012865921016782522\n",
      "Epoch :0.225    Train Loss :0.0003651980368886143    Test Loss :0.001182430423796177\n",
      "Epoch :0.23333333333333334    Train Loss :0.0003604617959354073    Test Loss :0.0011442583054304123\n",
      "Epoch :0.24166666666666667    Train Loss :0.0003743213601410389    Test Loss :0.0015579682076349854\n",
      "Epoch :0.25    Train Loss :0.0007137703942134976    Test Loss :0.001469023060053587\n",
      "Epoch :0.25833333333333336    Train Loss :0.0004180511168669909    Test Loss :0.0011325906962156296\n",
      "Epoch :0.26666666666666666    Train Loss :0.000434417393989861    Test Loss :0.0010069707641378045\n",
      "Epoch :0.275    Train Loss :0.00040129508124664426    Test Loss :0.001044248347170651\n",
      "Epoch :0.2833333333333333    Train Loss :0.000319022306939587    Test Loss :0.0012331698089838028\n",
      "Epoch :0.2916666666666667    Train Loss :0.00032590661430731416    Test Loss :0.0012127311201766133\n",
      "Epoch :0.3    Train Loss :0.0003333337081130594    Test Loss :0.0011591555085033178\n",
      "Epoch :0.30833333333333335    Train Loss :0.00030672186403535306    Test Loss :0.001033782958984375\n",
      "Epoch :0.31666666666666665    Train Loss :0.00033618995803408325    Test Loss :0.0008260331815108657\n",
      "Epoch :0.325    Train Loss :0.00037596336915157735    Test Loss :0.001510638976469636\n",
      "Epoch :0.3333333333333333    Train Loss :0.002349265618249774    Test Loss :0.0009279900696128607\n",
      "Epoch :0.3416666666666667    Train Loss :0.001283161574974656    Test Loss :0.002146918559446931\n",
      "Epoch :0.35    Train Loss :0.000464095501229167    Test Loss :0.003881598124280572\n",
      "Epoch :0.35833333333333334    Train Loss :0.0006717117503285408    Test Loss :0.0016315585235133767\n",
      "Epoch :0.36666666666666664    Train Loss :0.00038823249633423984    Test Loss :0.0013332482194527984\n",
      "Epoch :0.375    Train Loss :0.0004129779990762472    Test Loss :0.0010397888254374266\n",
      "Epoch :0.38333333333333336    Train Loss :0.00039128115167841315    Test Loss :0.0010763754835352302\n",
      "Epoch :0.39166666666666666    Train Loss :0.00036706027458421886    Test Loss :0.0011655123671516776\n",
      "Epoch :0.4    Train Loss :0.0003573749854695052    Test Loss :0.0010202046250924468\n",
      "Epoch :0.4083333333333333    Train Loss :0.0003255592891946435    Test Loss :0.0009133997373282909\n",
      "Epoch :0.4166666666666667    Train Loss :0.00033206157968379557    Test Loss :0.0009659886709414423\n",
      "Epoch :0.425    Train Loss :0.000308214221149683    Test Loss :0.0010552520398050547\n",
      "Epoch :0.43333333333333335    Train Loss :0.000314566888846457    Test Loss :0.0008780703064985573\n",
      "Epoch :0.44166666666666665    Train Loss :0.0003027048660442233    Test Loss :0.0008396319462917745\n",
      "Epoch :0.45    Train Loss :0.0003038439026568085    Test Loss :0.0007904870435595512\n",
      "Epoch :0.4583333333333333    Train Loss :0.0002914567303378135    Test Loss :0.00081196881365031\n",
      "Epoch :0.4666666666666667    Train Loss :0.0002782772353384644    Test Loss :0.000851078424602747\n",
      "Epoch :0.475    Train Loss :0.00026285304920747876    Test Loss :0.000802330207079649\n",
      "Epoch :0.48333333333333334    Train Loss :0.0002605224144645035    Test Loss :0.0007875210721977055\n",
      "Epoch :0.49166666666666664    Train Loss :0.0002617230638861656    Test Loss :0.0007977634086273611\n",
      "Epoch :0.5    Train Loss :0.00026445541880093515    Test Loss :0.0007052578730508685\n",
      "Epoch :0.5083333333333333    Train Loss :0.00025551236467435956    Test Loss :0.0006773217464797199\n",
      "Epoch :0.5166666666666667    Train Loss :0.00025101983919739723    Test Loss :0.0006470824591815472\n",
      "Epoch :0.525    Train Loss :0.00024644259246997535    Test Loss :0.0006765376892872155\n",
      "Epoch :0.5333333333333333    Train Loss :0.00024852805654518306    Test Loss :0.0006848557968623936\n",
      "Epoch :0.5416666666666666    Train Loss :0.00026700343005359173    Test Loss :0.0007978726061992347\n",
      "Epoch :0.55    Train Loss :0.00032610786729492247    Test Loss :0.0012019076384603977\n",
      "Epoch :0.5583333333333333    Train Loss :0.0002917428209912032    Test Loss :0.0012247581034898758\n",
      "Epoch :0.5666666666666667    Train Loss :0.0002647516375873238    Test Loss :0.0009466406190767884\n",
      "Epoch :0.575    Train Loss :0.00023299297026824206    Test Loss :0.0008387219277210534\n",
      "Epoch :0.5833333333333334    Train Loss :0.00023687815701123327    Test Loss :0.0008015259518288076\n",
      "Epoch :0.5916666666666667    Train Loss :0.00023450303706340492    Test Loss :0.000763966585509479\n",
      "Epoch :0.6    Train Loss :0.000262671266682446    Test Loss :0.000772862636949867\n",
      "Epoch :0.6083333333333333    Train Loss :0.000799183442723006    Test Loss :0.003050760133191943\n",
      "Epoch :0.6166666666666667    Train Loss :0.00047139445086941123    Test Loss :0.00148538569919765\n",
      "Epoch :0.625    Train Loss :0.0002736167225521058    Test Loss :0.0009165946394205093\n",
      "Epoch :0.6333333333333333    Train Loss :0.0002461019321344793    Test Loss :0.0008047992014326155\n",
      "Epoch :0.6416666666666667    Train Loss :0.00028622313402593136    Test Loss :0.0008560263086110353\n",
      "Epoch :0.65    Train Loss :0.0002442794793751091    Test Loss :0.0008480704273097217\n",
      "Epoch :0.6583333333333333    Train Loss :0.00023902958491817117    Test Loss :0.0008998009143397212\n",
      "Epoch :0.6666666666666666    Train Loss :0.00022753345547243953    Test Loss :0.0007088325801305473\n",
      "Epoch :0.675    Train Loss :0.00023214622342493385    Test Loss :0.0007509948336519301\n",
      "Epoch :0.6833333333333333    Train Loss :0.0002214487612945959    Test Loss :0.0007980017107911408\n",
      "Epoch :0.6916666666666667    Train Loss :0.00023104749561753124    Test Loss :0.0008626635535620153\n",
      "Epoch :0.7    Train Loss :0.00021453491353895515    Test Loss :0.0009491448290646076\n",
      "Epoch :0.7083333333333334    Train Loss :0.0002723315265029669    Test Loss :0.001007736660540104\n",
      "Epoch :0.7166666666666667    Train Loss :0.00021935152471996844    Test Loss :0.0009934838162735105\n",
      "Epoch :0.725    Train Loss :0.000239071567193605    Test Loss :0.0010897367028519511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7333333333333333    Train Loss :0.00021240995556581765    Test Loss :0.0008377456106245518\n",
      "Epoch :0.7416666666666667    Train Loss :0.0002109423658112064    Test Loss :0.0009034493123181164\n",
      "Epoch :0.75    Train Loss :0.0002596072154119611    Test Loss :0.0009685057448223233\n",
      "Epoch :0.7583333333333333    Train Loss :0.0014437983045354486    Test Loss :0.0027287111151963472\n",
      "Epoch :0.7666666666666667    Train Loss :0.00035240536089986563    Test Loss :0.001431307289749384\n",
      "Epoch :0.775    Train Loss :0.00030079076532274485    Test Loss :0.0018072971142828465\n",
      "Epoch :0.7833333333333333    Train Loss :0.0003328107704874128    Test Loss :0.0010308908531442285\n",
      "Epoch :0.7916666666666666    Train Loss :0.00033373330370523036    Test Loss :0.0010759198339655995\n",
      "Epoch :0.8    Train Loss :0.00028163345996290445    Test Loss :0.0008776739123277366\n",
      "Epoch :0.8083333333333333    Train Loss :0.00026999818510375917    Test Loss :0.0009134124265983701\n",
      "Epoch :0.8166666666666667    Train Loss :0.0002450351894367486    Test Loss :0.0009501453023403883\n",
      "Epoch :0.825    Train Loss :0.00023616215912625194    Test Loss :0.0007848353707231581\n",
      "Epoch :0.8333333333333334    Train Loss :0.00023966454318724573    Test Loss :0.0008732019341550767\n",
      "Epoch :0.8416666666666667    Train Loss :0.000219512585317716    Test Loss :0.0008991094655357301\n",
      "Epoch :0.85    Train Loss :0.00023474566114600748    Test Loss :0.0008768648258410394\n",
      "Epoch :0.8583333333333333    Train Loss :0.00022975172032602131    Test Loss :0.0009737206273712218\n",
      "Epoch :0.8666666666666667    Train Loss :0.00022179137158673257    Test Loss :0.0010028062388300896\n",
      "Epoch :0.875    Train Loss :0.00021243543596938252    Test Loss :0.0009786341106519103\n",
      "Epoch :0.8833333333333333    Train Loss :0.0002149995998479426    Test Loss :0.0010279557900503278\n",
      "Epoch :0.8916666666666667    Train Loss :0.0002095415984513238    Test Loss :0.0009348958847112954\n",
      "Epoch :0.9    Train Loss :0.0002074056537821889    Test Loss :0.0009354394860565662\n",
      "Epoch :0.9083333333333333    Train Loss :0.00021260211360640824    Test Loss :0.0008442659745924175\n",
      "Epoch :0.9166666666666666    Train Loss :0.00020557217067107558    Test Loss :0.0009314245544373989\n",
      "Epoch :0.925    Train Loss :0.00020176863472443074    Test Loss :0.0008099186234176159\n",
      "Epoch :0.9333333333333333    Train Loss :0.00020077821682207286    Test Loss :0.0008396551129408181\n",
      "Epoch :0.9416666666666667    Train Loss :0.00033513319795019925    Test Loss :0.0032814685255289078\n",
      "Epoch :0.95    Train Loss :0.00025689564063213766    Test Loss :0.0018728282302618027\n",
      "Epoch :0.9583333333333334    Train Loss :0.0003355932130943984    Test Loss :0.000955042487476021\n",
      "Epoch :0.9666666666666667    Train Loss :0.0003004629397764802    Test Loss :0.0009031295194290578\n",
      "Epoch :0.975    Train Loss :0.00022609725419897586    Test Loss :0.0011072764173150063\n",
      "Epoch :0.9833333333333333    Train Loss :0.00022519896447192878    Test Loss :0.0009072588291019201\n",
      "Epoch :0.9916666666666667    Train Loss :0.00024042818404268473    Test Loss :0.000853927806019783\n",
      "Epoch :1.0    Train Loss :0.0002265859511680901    Test Loss :0.000926598091609776\n",
      "RMSE: 59.99733694319571\n",
      "MAE: 57.559627727843534\n",
      "MAPE: 51.05724018349404%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 600, 'hid_size': 50, 'lr': 0.02, 'num_layers': 2}\n",
      "total:  34.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.0553659126162529    Test Loss :0.27309727668762207\n",
      "Epoch :0.016666666666666666    Train Loss :0.04145276919007301    Test Loss :0.21445496380329132\n",
      "Epoch :0.025    Train Loss :0.04078422486782074    Test Loss :0.226797953248024\n",
      "Epoch :0.03333333333333333    Train Loss :0.04069077596068382    Test Loss :0.21381203830242157\n",
      "Epoch :0.041666666666666664    Train Loss :0.040482260286808014    Test Loss :0.2219158113002777\n",
      "Epoch :0.05    Train Loss :0.04047305881977081    Test Loss :0.21697020530700684\n",
      "Epoch :0.058333333333333334    Train Loss :0.040510986000299454    Test Loss :0.21906328201293945\n",
      "Epoch :0.06666666666666667    Train Loss :0.040443699806928635    Test Loss :0.2188846319913864\n",
      "Epoch :0.075    Train Loss :0.040443990379571915    Test Loss :0.21742552518844604\n",
      "Epoch :0.08333333333333333    Train Loss :0.040462736040353775    Test Loss :0.21930788457393646\n",
      "Epoch :0.09166666666666666    Train Loss :0.04047830402851105    Test Loss :0.21775245666503906\n",
      "Epoch :0.1    Train Loss :0.04046346992254257    Test Loss :0.21872082352638245\n",
      "Epoch :0.10833333333333334    Train Loss :0.04048954322934151    Test Loss :0.21812789142131805\n",
      "Epoch :0.11666666666666667    Train Loss :0.04046471789479256    Test Loss :0.21845653653144836\n",
      "Epoch :0.125    Train Loss :0.040469322353601456    Test Loss :0.21842162311077118\n",
      "Epoch :0.13333333333333333    Train Loss :0.04045625776052475    Test Loss :0.2183639109134674\n",
      "Epoch :0.14166666666666666    Train Loss :0.04045610502362251    Test Loss :0.21854190528392792\n",
      "Epoch :0.15    Train Loss :0.04046554118394852    Test Loss :0.21841950714588165\n",
      "Epoch :0.15833333333333333    Train Loss :0.04047299548983574    Test Loss :0.21856778860092163\n",
      "Epoch :0.16666666666666666    Train Loss :0.04045255109667778    Test Loss :0.21807025372982025\n",
      "Epoch :0.175    Train Loss :0.04045899584889412    Test Loss :0.21826493740081787\n",
      "Epoch :0.18333333333333332    Train Loss :0.04045224189758301    Test Loss :0.2183896005153656\n",
      "Epoch :0.19166666666666668    Train Loss :0.040464021265506744    Test Loss :0.21884945034980774\n",
      "Epoch :0.2    Train Loss :0.04048828035593033    Test Loss :0.21817465126514435\n",
      "Epoch :0.20833333333333334    Train Loss :0.040485482662916183    Test Loss :0.21859587728977203\n",
      "Epoch :0.21666666666666667    Train Loss :0.04046826809644699    Test Loss :0.21822941303253174\n",
      "Epoch :0.225    Train Loss :0.04046902433037758    Test Loss :0.2187100499868393\n",
      "Epoch :0.23333333333333334    Train Loss :0.040481533855199814    Test Loss :0.21821898221969604\n",
      "Epoch :0.24166666666666667    Train Loss :0.04044974967837334    Test Loss :0.21831144392490387\n",
      "Epoch :0.25    Train Loss :0.04046027362346649    Test Loss :0.21853244304656982\n",
      "Epoch :0.25833333333333336    Train Loss :0.040442947298288345    Test Loss :0.21835725009441376\n",
      "Epoch :0.26666666666666666    Train Loss :0.04045870527625084    Test Loss :0.21876265108585358\n",
      "Epoch :0.275    Train Loss :0.04046492651104927    Test Loss :0.21788431704044342\n",
      "Epoch :0.2833333333333333    Train Loss :0.040470998734235764    Test Loss :0.21807505190372467\n",
      "Epoch :0.2916666666666667    Train Loss :0.04045304283499718    Test Loss :0.21860194206237793\n",
      "Epoch :0.3    Train Loss :0.0404377356171608    Test Loss :0.21822336316108704\n",
      "Epoch :0.30833333333333335    Train Loss :0.04046384245157242    Test Loss :0.21794581413269043\n",
      "Epoch :0.31666666666666665    Train Loss :0.040461212396621704    Test Loss :0.21877872943878174\n",
      "Epoch :0.325    Train Loss :0.04046018421649933    Test Loss :0.21867913007736206\n",
      "Epoch :0.3333333333333333    Train Loss :0.04046105220913887    Test Loss :0.21795105934143066\n",
      "Epoch :0.3416666666666667    Train Loss :0.04047049954533577    Test Loss :0.2181859016418457\n",
      "Epoch :0.35    Train Loss :0.040464747697114944    Test Loss :0.21851707994937897\n",
      "Epoch :0.35833333333333334    Train Loss :0.04045123979449272    Test Loss :0.21830259263515472\n",
      "Epoch :0.36666666666666664    Train Loss :0.04048138111829758    Test Loss :0.2183450311422348\n",
      "Epoch :0.375    Train Loss :0.04046989604830742    Test Loss :0.2184188961982727\n",
      "Epoch :0.38333333333333336    Train Loss :0.04044859856367111    Test Loss :0.21864253282546997\n",
      "Epoch :0.39166666666666666    Train Loss :0.04045075178146362    Test Loss :0.21829062700271606\n",
      "Epoch :0.4    Train Loss :0.040445271879434586    Test Loss :0.2185477465391159\n",
      "Epoch :0.4083333333333333    Train Loss :0.04046196490526199    Test Loss :0.21845173835754395\n",
      "Epoch :0.4166666666666667    Train Loss :0.040463753044605255    Test Loss :0.21815122663974762\n",
      "Epoch :0.425    Train Loss :0.04048312455415726    Test Loss :0.2184252142906189\n",
      "Epoch :0.43333333333333335    Train Loss :0.04046381264925003    Test Loss :0.21865485608577728\n",
      "Epoch :0.44166666666666665    Train Loss :0.04046512767672539    Test Loss :0.21836483478546143\n",
      "Epoch :0.45    Train Loss :0.04044724255800247    Test Loss :0.21814408898353577\n",
      "Epoch :0.4583333333333333    Train Loss :0.04047055542469025    Test Loss :0.21844257414340973\n",
      "Epoch :0.4666666666666667    Train Loss :0.040457833558321    Test Loss :0.2183821201324463\n",
      "Epoch :0.475    Train Loss :0.04046367108821869    Test Loss :0.21850955486297607\n",
      "Epoch :0.48333333333333334    Train Loss :0.040471795946359634    Test Loss :0.2185753732919693\n",
      "Epoch :0.49166666666666664    Train Loss :0.0404604971408844    Test Loss :0.21852578222751617\n",
      "Epoch :0.5    Train Loss :0.04047248512506485    Test Loss :0.21824173629283905\n",
      "Epoch :0.5083333333333333    Train Loss :0.04044467210769653    Test Loss :0.2183058261871338\n",
      "Epoch :0.5166666666666667    Train Loss :0.04047558829188347    Test Loss :0.21850617229938507\n",
      "Epoch :0.525    Train Loss :0.04047233983874321    Test Loss :0.2185269445180893\n",
      "Epoch :0.5333333333333333    Train Loss :0.04046911746263504    Test Loss :0.2182614803314209\n",
      "Epoch :0.5416666666666666    Train Loss :0.04045885428786278    Test Loss :0.21835120022296906\n",
      "Epoch :0.55    Train Loss :0.040466271340847015    Test Loss :0.21852809190750122\n",
      "Epoch :0.5583333333333333    Train Loss :0.04045698791742325    Test Loss :0.2183123677968979\n",
      "Epoch :0.5666666666666667    Train Loss :0.040456026792526245    Test Loss :0.21818064153194427\n",
      "Epoch :0.575    Train Loss :0.04045961797237396    Test Loss :0.21835123002529144\n",
      "Epoch :0.5833333333333334    Train Loss :0.040468670427799225    Test Loss :0.21842585504055023\n",
      "Epoch :0.5916666666666667    Train Loss :0.040466029196977615    Test Loss :0.21842679381370544\n",
      "Epoch :0.6    Train Loss :0.04046355187892914    Test Loss :0.21838392317295074\n",
      "Epoch :0.6083333333333333    Train Loss :0.040457844734191895    Test Loss :0.21808233857154846\n",
      "Epoch :0.6166666666666667    Train Loss :0.04046110063791275    Test Loss :0.21845711767673492\n",
      "Epoch :0.625    Train Loss :0.040462881326675415    Test Loss :0.21838931739330292\n",
      "Epoch :0.6333333333333333    Train Loss :0.04046311601996422    Test Loss :0.21833369135856628\n",
      "Epoch :0.6416666666666667    Train Loss :0.04046101123094559    Test Loss :0.2183995395898819\n",
      "Epoch :0.65    Train Loss :0.04046360403299332    Test Loss :0.21848991513252258\n",
      "Epoch :0.6583333333333333    Train Loss :0.040472108870744705    Test Loss :0.21839983761310577\n",
      "Epoch :0.6666666666666666    Train Loss :0.040461037307977676    Test Loss :0.21825571358203888\n",
      "Epoch :0.675    Train Loss :0.04046310484409332    Test Loss :0.2183213084936142\n",
      "Epoch :0.6833333333333333    Train Loss :0.04045771807432175    Test Loss :0.21833154559135437\n",
      "Epoch :0.6916666666666667    Train Loss :0.04046062007546425    Test Loss :0.21839267015457153\n",
      "Epoch :0.7    Train Loss :0.04046459496021271    Test Loss :0.21858927607536316\n",
      "Epoch :0.7083333333333334    Train Loss :0.04045989364385605    Test Loss :0.2183532863855362\n",
      "Epoch :0.7166666666666667    Train Loss :0.04046028479933739    Test Loss :0.2182074338197708\n",
      "Epoch :0.725    Train Loss :0.04045988246798515    Test Loss :0.21829235553741455\n",
      "Epoch :0.7333333333333333    Train Loss :0.04046253859996796    Test Loss :0.21831990778446198\n",
      "Epoch :0.7416666666666667    Train Loss :0.04044995456933975    Test Loss :0.21837225556373596\n",
      "Epoch :0.75    Train Loss :0.04048139601945877    Test Loss :0.2184552252292633\n",
      "Epoch :0.7583333333333333    Train Loss :0.04047061502933502    Test Loss :0.21840521693229675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7666666666666667    Train Loss :0.040454428642988205    Test Loss :0.21828600764274597\n",
      "Epoch :0.775    Train Loss :0.0404617078602314    Test Loss :0.21845954656600952\n",
      "Epoch :0.7833333333333333    Train Loss :0.040461860597133636    Test Loss :0.21829482913017273\n",
      "Epoch :0.7916666666666666    Train Loss :0.04045926779508591    Test Loss :0.2184678167104721\n",
      "Epoch :0.8    Train Loss :0.04045507311820984    Test Loss :0.21846958994865417\n",
      "Epoch :0.8083333333333333    Train Loss :0.04045864939689636    Test Loss :0.21836362779140472\n",
      "Epoch :0.8166666666666667    Train Loss :0.040453244000673294    Test Loss :0.2183191329240799\n",
      "Epoch :0.825    Train Loss :0.04045309126377106    Test Loss :0.21805153787136078\n",
      "Epoch :0.8333333333333334    Train Loss :0.04045265540480614    Test Loss :0.21844494342803955\n",
      "Epoch :0.8416666666666667    Train Loss :0.040330756455659866    Test Loss :0.21749573945999146\n",
      "Epoch :0.85    Train Loss :0.040539443492889404    Test Loss :0.20626696944236755\n",
      "Epoch :0.8583333333333333    Train Loss :0.04049268364906311    Test Loss :0.22678253054618835\n",
      "Epoch :0.8666666666666667    Train Loss :0.0405147410929203    Test Loss :0.22397558391094208\n",
      "Epoch :0.875    Train Loss :0.040457792580127716    Test Loss :0.2203225940465927\n",
      "Epoch :0.8833333333333333    Train Loss :0.04048972204327583    Test Loss :0.21589979529380798\n",
      "Epoch :0.8916666666666667    Train Loss :0.04047375172376633    Test Loss :0.21760764718055725\n",
      "Epoch :0.9    Train Loss :0.0404721163213253    Test Loss :0.22049011290073395\n",
      "Epoch :0.9083333333333333    Train Loss :0.0404469333589077    Test Loss :0.21750549972057343\n",
      "Epoch :0.9166666666666666    Train Loss :0.0403686948120594    Test Loss :0.217262864112854\n",
      "Epoch :0.925    Train Loss :0.04000789672136307    Test Loss :0.21650731563568115\n",
      "Epoch :0.9333333333333333    Train Loss :0.0427645705640316    Test Loss :0.21358926594257355\n",
      "Epoch :0.9416666666666667    Train Loss :0.03831222280859947    Test Loss :0.19858025014400482\n",
      "Epoch :0.95    Train Loss :0.024155447259545326    Test Loss :0.08789276331663132\n",
      "Epoch :0.9583333333333334    Train Loss :0.009800156578421593    Test Loss :0.012197058647871017\n",
      "Epoch :0.9666666666666667    Train Loss :0.00885323528200388    Test Loss :0.036554474383592606\n",
      "Epoch :0.975    Train Loss :0.005823194980621338    Test Loss :0.0119281355291605\n",
      "Epoch :0.9833333333333333    Train Loss :0.0046866657212376595    Test Loss :0.006205912679433823\n",
      "Epoch :0.9916666666666667    Train Loss :0.003371278755366802    Test Loss :0.018618211150169373\n",
      "Epoch :1.0    Train Loss :0.002460888586938381    Test Loss :0.006789825856685638\n",
      "RMSE: 8.839745380770468\n",
      "MAE: 7.0589215347778955\n",
      "MAPE: 6.0934694292513925%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 600, 'hid_size': 50, 'lr': 0.02, 'num_layers': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  38.0\n",
      "Epoch :0.008333333333333333    Train Loss :0.03322101756930351    Test Loss :0.085905522108078\n",
      "Epoch :0.016666666666666666    Train Loss :0.03404286876320839    Test Loss :0.05466895550489426\n",
      "Epoch :0.025    Train Loss :0.008285685442388058    Test Loss :0.004981459118425846\n",
      "Epoch :0.03333333333333333    Train Loss :0.010554241016507149    Test Loss :0.074474036693573\n",
      "Epoch :0.041666666666666664    Train Loss :0.00286459201015532    Test Loss :0.027266234159469604\n",
      "Epoch :0.05    Train Loss :0.004170132335275412    Test Loss :0.013738480396568775\n",
      "Epoch :0.058333333333333334    Train Loss :0.003729521529749036    Test Loss :0.005946167279034853\n",
      "Epoch :0.06666666666666667    Train Loss :0.0018073609098792076    Test Loss :0.005143795628100634\n",
      "Epoch :0.075    Train Loss :0.001523941638879478    Test Loss :0.003023087978363037\n",
      "Epoch :0.08333333333333333    Train Loss :0.0011451896280050278    Test Loss :0.005584319122135639\n",
      "Epoch :0.09166666666666666    Train Loss :0.0008079482358880341    Test Loss :0.0036922339349985123\n",
      "Epoch :0.1    Train Loss :0.0008568865596316755    Test Loss :0.002477225847542286\n",
      "Epoch :0.10833333333333334    Train Loss :0.000728007813449949    Test Loss :0.0045988126657903194\n",
      "Epoch :0.11666666666666667    Train Loss :0.0007162833353504539    Test Loss :0.0025874271523207426\n",
      "Epoch :0.125    Train Loss :0.0007075457833707333    Test Loss :0.0025739860720932484\n",
      "Epoch :0.13333333333333333    Train Loss :0.0006468261708505452    Test Loss :0.0028580771759152412\n",
      "Epoch :0.14166666666666666    Train Loss :0.0006167456740513444    Test Loss :0.0023364981170743704\n",
      "Epoch :0.15    Train Loss :0.0006086301291361451    Test Loss :0.0024997559376060963\n",
      "Epoch :0.15833333333333333    Train Loss :0.0005518195102922618    Test Loss :0.0022159258369356394\n",
      "Epoch :0.16666666666666666    Train Loss :0.0005526349414139986    Test Loss :0.002025380963459611\n",
      "Epoch :0.175    Train Loss :0.0005211139214225113    Test Loss :0.0021597181912511587\n",
      "Epoch :0.18333333333333332    Train Loss :0.0005270890542306006    Test Loss :0.0017848138231784105\n",
      "Epoch :0.19166666666666668    Train Loss :0.0004919748171232641    Test Loss :0.001817760057747364\n",
      "Epoch :0.2    Train Loss :0.0005089744809083641    Test Loss :0.001964907394722104\n",
      "Epoch :0.20833333333333334    Train Loss :0.0005030668107792735    Test Loss :0.0018419030820950866\n",
      "Epoch :0.21666666666666667    Train Loss :0.00048619956942275167    Test Loss :0.0017200999427586794\n",
      "Epoch :0.225    Train Loss :0.0004796149441972375    Test Loss :0.0016747665358707309\n",
      "Epoch :0.23333333333333334    Train Loss :0.00047354126581922174    Test Loss :0.0017614720854908228\n",
      "Epoch :0.24166666666666667    Train Loss :0.00045265170047059655    Test Loss :0.0016664635622873902\n",
      "Epoch :0.25    Train Loss :0.0004791608080267906    Test Loss :0.0014556620735675097\n",
      "Epoch :0.25833333333333336    Train Loss :0.00044027817784808576    Test Loss :0.001564597710967064\n",
      "Epoch :0.26666666666666666    Train Loss :0.0004458665498532355    Test Loss :0.0015744330594316125\n",
      "Epoch :0.275    Train Loss :0.00043159807682968676    Test Loss :0.001519981655292213\n",
      "Epoch :0.2833333333333333    Train Loss :0.00042727330583147705    Test Loss :0.001433342113159597\n",
      "Epoch :0.2916666666666667    Train Loss :0.0004254631348885596    Test Loss :0.001383696566335857\n",
      "Epoch :0.3    Train Loss :0.00043770685442723334    Test Loss :0.0013962651137262583\n",
      "Epoch :0.30833333333333335    Train Loss :0.0004071847361046821    Test Loss :0.001285846927203238\n",
      "Epoch :0.31666666666666665    Train Loss :0.0004137201467528939    Test Loss :0.0013136324705556035\n",
      "Epoch :0.325    Train Loss :0.000404451071517542    Test Loss :0.001256899326108396\n",
      "Epoch :0.3333333333333333    Train Loss :0.00039627807564102113    Test Loss :0.001266936189495027\n",
      "Epoch :0.3416666666666667    Train Loss :0.0003896788402926177    Test Loss :0.00112060084939003\n",
      "Epoch :0.35    Train Loss :0.0003968655946664512    Test Loss :0.0011989219347015023\n",
      "Epoch :0.35833333333333334    Train Loss :0.00037990641430951655    Test Loss :0.0011388111161068082\n",
      "Epoch :0.36666666666666664    Train Loss :0.0003754547506105155    Test Loss :0.0011534745572134852\n",
      "Epoch :0.375    Train Loss :0.00037992087891325355    Test Loss :0.0010032783029600978\n",
      "Epoch :0.38333333333333336    Train Loss :0.00037392546073533595    Test Loss :0.0010042570065706968\n",
      "Epoch :0.39166666666666666    Train Loss :0.0003681026864796877    Test Loss :0.0010562569368630648\n",
      "Epoch :0.4    Train Loss :0.0003697639622259885    Test Loss :0.0010266266763210297\n",
      "Epoch :0.4083333333333333    Train Loss :0.00035577648668549955    Test Loss :0.0010449961991980672\n",
      "Epoch :0.4166666666666667    Train Loss :0.00035471824230626225    Test Loss :0.0009980177273973823\n",
      "Epoch :0.425    Train Loss :0.000357951270416379    Test Loss :0.0009706165292300284\n",
      "Epoch :0.43333333333333335    Train Loss :0.0003433206584304571    Test Loss :0.000994227477349341\n",
      "Epoch :0.44166666666666665    Train Loss :0.00037156554753892124    Test Loss :0.0010097721824422479\n",
      "Epoch :0.45    Train Loss :0.00035394547739997506    Test Loss :0.0009173802100121975\n",
      "Epoch :0.4583333333333333    Train Loss :0.0003510293026920408    Test Loss :0.0009843383450061083\n",
      "Epoch :0.4666666666666667    Train Loss :0.00033671752316877246    Test Loss :0.0009811820928007364\n",
      "Epoch :0.475    Train Loss :0.0003282256075181067    Test Loss :0.0009279350051656365\n",
      "Epoch :0.48333333333333334    Train Loss :0.0003319409443065524    Test Loss :0.0008689275709912181\n",
      "Epoch :0.49166666666666664    Train Loss :0.00031260799732990563    Test Loss :0.000888311886228621\n",
      "Epoch :0.5    Train Loss :0.0003172228462062776    Test Loss :0.0008382356609217823\n",
      "Epoch :0.5083333333333333    Train Loss :0.0003138777974527329    Test Loss :0.0008869721204973757\n",
      "Epoch :0.5166666666666667    Train Loss :0.00030909787165001035    Test Loss :0.0008103607688099146\n",
      "Epoch :0.525    Train Loss :0.00030635209986940026    Test Loss :0.0008899373933672905\n",
      "Epoch :0.5333333333333333    Train Loss :0.00030712044099345803    Test Loss :0.0007955878390930593\n",
      "Epoch :0.5416666666666666    Train Loss :0.00029857628396712244    Test Loss :0.0007842862396501005\n",
      "Epoch :0.55    Train Loss :0.00029056816129013896    Test Loss :0.0008700438775122166\n",
      "Epoch :0.5583333333333333    Train Loss :0.0002944634179584682    Test Loss :0.0007822405896149576\n",
      "Epoch :0.5666666666666667    Train Loss :0.00029025733238086104    Test Loss :0.000797378655988723\n",
      "Epoch :0.575    Train Loss :0.00028213553014211357    Test Loss :0.0007564855040982366\n",
      "Epoch :0.5833333333333334    Train Loss :0.0002851502795238048    Test Loss :0.0008279024623334408\n",
      "Epoch :0.5916666666666667    Train Loss :0.0002740647178143263    Test Loss :0.0007014768780209124\n",
      "Epoch :0.6    Train Loss :0.000286631693597883    Test Loss :0.0007355752750299871\n",
      "Epoch :0.6083333333333333    Train Loss :0.0002726509119383991    Test Loss :0.0006659166538156569\n",
      "Epoch :0.6166666666666667    Train Loss :0.0002658532466739416    Test Loss :0.0007273817318491638\n",
      "Epoch :0.625    Train Loss :0.00026727019576355815    Test Loss :0.0006866795592941344\n",
      "Epoch :0.6333333333333333    Train Loss :0.0002679985191207379    Test Loss :0.00066565559245646\n",
      "Epoch :0.6416666666666667    Train Loss :0.00025306627503596246    Test Loss :0.0005842390237376094\n",
      "Epoch :0.65    Train Loss :0.0002609285875223577    Test Loss :0.0006529208621941507\n",
      "Epoch :0.6583333333333333    Train Loss :0.0002483232819940895    Test Loss :0.0006197338225319982\n",
      "Epoch :0.6666666666666666    Train Loss :0.0002506792370695621    Test Loss :0.000630772381555289\n",
      "Epoch :0.675    Train Loss :0.00025850124075077474    Test Loss :0.0006546788499690592\n",
      "Epoch :0.6833333333333333    Train Loss :0.00026373652508482337    Test Loss :0.0006497862050309777\n",
      "Epoch :0.6916666666666667    Train Loss :0.0007539215730503201    Test Loss :0.00234568165615201\n",
      "Epoch :0.7    Train Loss :0.00034644242259673774    Test Loss :0.0006090852548368275\n",
      "Epoch :0.7083333333333334    Train Loss :0.0003240015939809382    Test Loss :0.0007730586221441627\n",
      "Epoch :0.7166666666666667    Train Loss :0.0002933591604232788    Test Loss :0.000634629453998059\n",
      "Epoch :0.725    Train Loss :0.00026196695398539305    Test Loss :0.0007322212914004922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7333333333333333    Train Loss :0.00024148603552021086    Test Loss :0.0006377653335221112\n",
      "Epoch :0.7416666666666667    Train Loss :0.0002528556215111166    Test Loss :0.0007215581135824323\n",
      "Epoch :0.75    Train Loss :0.00024515268160030246    Test Loss :0.0006150946719571948\n",
      "Epoch :0.7583333333333333    Train Loss :0.0002449402818456292    Test Loss :0.0006274942425079644\n",
      "Epoch :0.7666666666666667    Train Loss :0.00023512264306191355    Test Loss :0.0006474117399193347\n",
      "Epoch :0.775    Train Loss :0.0002320941857760772    Test Loss :0.0006474507972598076\n",
      "Epoch :0.7833333333333333    Train Loss :0.00022948677360545844    Test Loss :0.000586486014071852\n",
      "Epoch :0.7916666666666666    Train Loss :0.00022986704425420612    Test Loss :0.0005522386054508388\n",
      "Epoch :0.8    Train Loss :0.00022981056827120483    Test Loss :0.0005452043260447681\n",
      "Epoch :0.8083333333333333    Train Loss :0.00022307244944386184    Test Loss :0.0005577080883085728\n",
      "Epoch :0.8166666666666667    Train Loss :0.00021242476941552013    Test Loss :0.0005236222641542554\n",
      "Epoch :0.825    Train Loss :0.00021231608116067946    Test Loss :0.0005042958655394614\n",
      "Epoch :0.8333333333333334    Train Loss :0.0002136369002982974    Test Loss :0.0005414606421254575\n",
      "Epoch :0.8416666666666667    Train Loss :0.0008760044584050775    Test Loss :0.00581714604049921\n",
      "Epoch :0.85    Train Loss :0.0007996330386959016    Test Loss :0.0016242587007582188\n",
      "Epoch :0.8583333333333333    Train Loss :0.0005456010112538934    Test Loss :0.0006844063173048198\n",
      "Epoch :0.8666666666666667    Train Loss :0.0002810614532791078    Test Loss :0.000721700256690383\n",
      "Epoch :0.875    Train Loss :0.00031517253955826163    Test Loss :0.0008166286861523986\n",
      "Epoch :0.8833333333333333    Train Loss :0.00024803035194054246    Test Loss :0.0009039909346029162\n",
      "Epoch :0.8916666666666667    Train Loss :0.0002684635983314365    Test Loss :0.0007077791378833354\n",
      "Epoch :0.9    Train Loss :0.0002355444012209773    Test Loss :0.0006608708063140512\n",
      "Epoch :0.9083333333333333    Train Loss :0.00024834086070768535    Test Loss :0.0006936231511645019\n",
      "Epoch :0.9166666666666666    Train Loss :0.0002479689137544483    Test Loss :0.0006953667616471648\n",
      "Epoch :0.925    Train Loss :0.0002331382711417973    Test Loss :0.0006392996874637902\n",
      "Epoch :0.9333333333333333    Train Loss :0.00023223660537041724    Test Loss :0.0006339644314721227\n",
      "Epoch :0.9416666666666667    Train Loss :0.00021991670655552298    Test Loss :0.0006093881092965603\n",
      "Epoch :0.95    Train Loss :0.00021876998653169721    Test Loss :0.0006312936311587691\n",
      "Epoch :0.9583333333333334    Train Loss :0.00021488816128112376    Test Loss :0.000619400932919234\n",
      "Epoch :0.9666666666666667    Train Loss :0.00021680195641238242    Test Loss :0.0005961298011243343\n",
      "Epoch :0.975    Train Loss :0.00020787214452866465    Test Loss :0.0005532033974304795\n",
      "Epoch :0.9833333333333333    Train Loss :0.00021400340483523905    Test Loss :0.0006353646167553961\n",
      "Epoch :0.9916666666666667    Train Loss :0.00020865238911937922    Test Loss :0.0004690588975790888\n",
      "Epoch :1.0    Train Loss :0.00020768049580510706    Test Loss :0.0005345421377569437\n",
      "RMSE: 38.75131280885119\n",
      "MAE: 36.063042617304156\n",
      "MAPE: 31.681290381960203%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 600, 'hid_size': 100, 'lr': 0.01, 'num_layers': 2}\n",
      "total:  41.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.041699301451444626    Test Loss :0.277098685503006\n",
      "Epoch :0.016666666666666666    Train Loss :0.04124212637543678    Test Loss :0.2150920182466507\n",
      "Epoch :0.025    Train Loss :0.026303505524992943    Test Loss :0.02074299193918705\n",
      "Epoch :0.03333333333333333    Train Loss :0.027759026736021042    Test Loss :0.13091592490673065\n",
      "Epoch :0.041666666666666664    Train Loss :0.026196377351880074    Test Loss :0.12337710708379745\n",
      "Epoch :0.05    Train Loss :0.010230832733213902    Test Loss :0.04113030806183815\n",
      "Epoch :0.058333333333333334    Train Loss :0.006599977612495422    Test Loss :0.043699342757463455\n",
      "Epoch :0.06666666666666667    Train Loss :0.005488239228725433    Test Loss :0.006284452509135008\n",
      "Epoch :0.075    Train Loss :0.0027800677344202995    Test Loss :0.006983011029660702\n",
      "Epoch :0.08333333333333333    Train Loss :0.0020766044035553932    Test Loss :0.010591644793748856\n",
      "Epoch :0.09166666666666666    Train Loss :0.001896533416584134    Test Loss :0.004181183408945799\n",
      "Epoch :0.1    Train Loss :0.0013423669151961803    Test Loss :0.0056637004017829895\n",
      "Epoch :0.10833333333333334    Train Loss :0.0012108165537938476    Test Loss :0.005085678305476904\n",
      "Epoch :0.11666666666666667    Train Loss :0.0010287471814081073    Test Loss :0.004105024039745331\n",
      "Epoch :0.125    Train Loss :0.0009602210484445095    Test Loss :0.0044907331466674805\n",
      "Epoch :0.13333333333333333    Train Loss :0.0008662644540891051    Test Loss :0.0035752614494413137\n",
      "Epoch :0.14166666666666666    Train Loss :0.000832870602607727    Test Loss :0.0026024621911346912\n",
      "Epoch :0.15    Train Loss :0.0007962397066876292    Test Loss :0.0024657726753503084\n",
      "Epoch :0.15833333333333333    Train Loss :0.000761822855565697    Test Loss :0.0024120125453919172\n",
      "Epoch :0.16666666666666666    Train Loss :0.0007281754515133798    Test Loss :0.0026327541563659906\n",
      "Epoch :0.175    Train Loss :0.0007186457514762878    Test Loss :0.002447678241878748\n",
      "Epoch :0.18333333333333332    Train Loss :0.0006755136419087648    Test Loss :0.0022177561186254025\n",
      "Epoch :0.19166666666666668    Train Loss :0.0006363354623317719    Test Loss :0.0022315611131489277\n",
      "Epoch :0.2    Train Loss :0.0006130843539722264    Test Loss :0.00230275746434927\n",
      "Epoch :0.20833333333333334    Train Loss :0.0006399896228685975    Test Loss :0.0023358629550784826\n",
      "Epoch :0.21666666666666667    Train Loss :0.0005993415252305567    Test Loss :0.00201683328486979\n",
      "Epoch :0.225    Train Loss :0.0005940304254181683    Test Loss :0.002092560986056924\n",
      "Epoch :0.23333333333333334    Train Loss :0.0007337383576668799    Test Loss :0.0028779120184481144\n",
      "Epoch :0.24166666666666667    Train Loss :0.0005732859135605395    Test Loss :0.0023374869488179684\n",
      "Epoch :0.25    Train Loss :0.0006138855242170393    Test Loss :0.0020002194214612246\n",
      "Epoch :0.25833333333333336    Train Loss :0.0005892948247492313    Test Loss :0.0019632428884506226\n",
      "Epoch :0.26666666666666666    Train Loss :0.000548212556168437    Test Loss :0.0020867513958364725\n",
      "Epoch :0.275    Train Loss :0.0005558369448408484    Test Loss :0.0021870522759854794\n",
      "Epoch :0.2833333333333333    Train Loss :0.0022171770688146353    Test Loss :0.0033487523905932903\n",
      "Epoch :0.2916666666666667    Train Loss :0.0008204019977711141    Test Loss :0.003063156735152006\n",
      "Epoch :0.3    Train Loss :0.000688059430103749    Test Loss :0.0018799833487719297\n",
      "Epoch :0.30833333333333335    Train Loss :0.0006868985365144908    Test Loss :0.0018432042561471462\n",
      "Epoch :0.31666666666666665    Train Loss :0.0006343791028484702    Test Loss :0.0021058679558336735\n",
      "Epoch :0.325    Train Loss :0.0005666580982506275    Test Loss :0.002383071929216385\n",
      "Epoch :0.3333333333333333    Train Loss :0.000536660198122263    Test Loss :0.0021763984113931656\n",
      "Epoch :0.3416666666666667    Train Loss :0.0005219875602051616    Test Loss :0.0018603408243507147\n",
      "Epoch :0.35    Train Loss :0.0005117375403642654    Test Loss :0.0020057226065546274\n",
      "Epoch :0.35833333333333334    Train Loss :0.0004845553485210985    Test Loss :0.0018307844875380397\n",
      "Epoch :0.36666666666666664    Train Loss :0.0004933103336952627    Test Loss :0.001932873041369021\n",
      "Epoch :0.375    Train Loss :0.0004865805385634303    Test Loss :0.0020240056328475475\n",
      "Epoch :0.38333333333333336    Train Loss :0.00047516333870589733    Test Loss :0.0019719956908375025\n",
      "Epoch :0.39166666666666666    Train Loss :0.0004513486346695572    Test Loss :0.0020947514567524195\n",
      "Epoch :0.4    Train Loss :0.0004553604230750352    Test Loss :0.001893723150715232\n",
      "Epoch :0.4083333333333333    Train Loss :0.0004585112619679421    Test Loss :0.002148483181372285\n",
      "Epoch :0.4166666666666667    Train Loss :0.0004967152490280569    Test Loss :0.002272457117214799\n",
      "Epoch :0.425    Train Loss :0.00042695211595855653    Test Loss :0.0019938999321311712\n",
      "Epoch :0.43333333333333335    Train Loss :0.0004683438455685973    Test Loss :0.0019117756746709347\n",
      "Epoch :0.44166666666666665    Train Loss :0.0005988383200019598    Test Loss :0.0025680840481072664\n",
      "Epoch :0.45    Train Loss :0.0004730166692752391    Test Loss :0.002236902015283704\n",
      "Epoch :0.4583333333333333    Train Loss :0.0004243000876158476    Test Loss :0.0018761977553367615\n",
      "Epoch :0.4666666666666667    Train Loss :0.0005928047467023134    Test Loss :0.0022325871977955103\n",
      "Epoch :0.475    Train Loss :0.00041763376793824136    Test Loss :0.001942679169587791\n",
      "Epoch :0.48333333333333334    Train Loss :0.00045067330938763916    Test Loss :0.001983353868126869\n",
      "Epoch :0.49166666666666664    Train Loss :0.00039449523319490254    Test Loss :0.0021523083560168743\n",
      "Epoch :0.5    Train Loss :0.0006272198515944183    Test Loss :0.003978010732680559\n",
      "Epoch :0.5083333333333333    Train Loss :0.0010535253677517176    Test Loss :0.0027789664454758167\n",
      "Epoch :0.5166666666666667    Train Loss :0.0005826987326145172    Test Loss :0.0020184579771012068\n",
      "Epoch :0.525    Train Loss :0.0006270241574384272    Test Loss :0.0016069220146164298\n",
      "Epoch :0.5333333333333333    Train Loss :0.0005822323146276176    Test Loss :0.0026600940618664026\n",
      "Epoch :0.5416666666666666    Train Loss :0.0005037347436882555    Test Loss :0.0022922183852642775\n",
      "Epoch :0.55    Train Loss :0.00044977772631682456    Test Loss :0.0018982011824846268\n",
      "Epoch :0.5583333333333333    Train Loss :0.00045522916479967535    Test Loss :0.0018195732263848186\n",
      "Epoch :0.5666666666666667    Train Loss :0.0003991218109149486    Test Loss :0.0019867524970322847\n",
      "Epoch :0.575    Train Loss :0.00040219188667833805    Test Loss :0.0020441939122974873\n",
      "Epoch :0.5833333333333334    Train Loss :0.0003960895992349833    Test Loss :0.0018201731145381927\n",
      "Epoch :0.5916666666666667    Train Loss :0.0003921692259609699    Test Loss :0.0018634552834555507\n",
      "Epoch :0.6    Train Loss :0.0003734089550562203    Test Loss :0.0019134964095428586\n",
      "Epoch :0.6083333333333333    Train Loss :0.0003772596246562898    Test Loss :0.0017963851569220424\n",
      "Epoch :0.6166666666666667    Train Loss :0.00047654996160417795    Test Loss :0.0019107384141534567\n",
      "Epoch :0.625    Train Loss :0.0007305779145099223    Test Loss :0.0028468817472457886\n",
      "Epoch :0.6333333333333333    Train Loss :0.00051517115207389    Test Loss :0.0024666518438607454\n",
      "Epoch :0.6416666666666667    Train Loss :0.00044425216037780046    Test Loss :0.0018348725279793143\n",
      "Epoch :0.65    Train Loss :0.00039016662049107254    Test Loss :0.0018355193315073848\n",
      "Epoch :0.6583333333333333    Train Loss :0.00035661758738569915    Test Loss :0.0018947581993415952\n",
      "Epoch :0.6666666666666666    Train Loss :0.0003445954353082925    Test Loss :0.0018052164232358336\n",
      "Epoch :0.675    Train Loss :0.0005654491251334548    Test Loss :0.004037113860249519\n",
      "Epoch :0.6833333333333333    Train Loss :0.000465331511804834    Test Loss :0.0024761022068560123\n",
      "Epoch :0.6916666666666667    Train Loss :0.0005985356983728707    Test Loss :0.002737643662840128\n",
      "Epoch :0.7    Train Loss :0.0004963043029420078    Test Loss :0.0019432608969509602\n",
      "Epoch :0.7083333333333334    Train Loss :0.00043142499634996057    Test Loss :0.0021617140155285597\n",
      "Epoch :0.7166666666666667    Train Loss :0.0004744238394778222    Test Loss :0.0018909628270193934\n",
      "Epoch :0.725    Train Loss :0.00035401221248321235    Test Loss :0.0019066034583374858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7333333333333333    Train Loss :0.0003878501302096993    Test Loss :0.002043979475274682\n",
      "Epoch :0.7416666666666667    Train Loss :0.0003771440824493766    Test Loss :0.0019729984924197197\n",
      "Epoch :0.75    Train Loss :0.0003288010193500668    Test Loss :0.001726121292449534\n",
      "Epoch :0.7583333333333333    Train Loss :0.00031464846688322723    Test Loss :0.0018155908910557628\n",
      "Epoch :0.7666666666666667    Train Loss :0.00035569132887758315    Test Loss :0.0016913545550778508\n",
      "Epoch :0.775    Train Loss :0.0004114230687264353    Test Loss :0.002034674398601055\n",
      "Epoch :0.7833333333333333    Train Loss :0.0005357409245334566    Test Loss :0.0021595556754618883\n",
      "Epoch :0.7916666666666666    Train Loss :0.00036619571619667113    Test Loss :0.0017988053150475025\n",
      "Epoch :0.8    Train Loss :0.00034895280259661376    Test Loss :0.0017366278916597366\n",
      "Epoch :0.8083333333333333    Train Loss :0.00039636011933907866    Test Loss :0.001911704195663333\n",
      "Epoch :0.8166666666666667    Train Loss :0.000381728372303769    Test Loss :0.0017689354717731476\n",
      "Epoch :0.825    Train Loss :0.0003185084497090429    Test Loss :0.0016852562548592687\n",
      "Epoch :0.8333333333333334    Train Loss :0.00033827332663349807    Test Loss :0.0017125388840213418\n",
      "Epoch :0.8416666666666667    Train Loss :0.0003057614085264504    Test Loss :0.0016486290842294693\n",
      "Epoch :0.85    Train Loss :0.00031750762718729675    Test Loss :0.001715411781333387\n",
      "Epoch :0.8583333333333333    Train Loss :0.0003027106577064842    Test Loss :0.001597894006408751\n",
      "Epoch :0.8666666666666667    Train Loss :0.0004998674849048257    Test Loss :0.0029386356472969055\n",
      "Epoch :0.875    Train Loss :0.0007021334022283554    Test Loss :0.0022401236928999424\n",
      "Epoch :0.8833333333333333    Train Loss :0.00047875355812720954    Test Loss :0.00220930902287364\n",
      "Epoch :0.8916666666666667    Train Loss :0.0005642003961838782    Test Loss :0.002009310759603977\n",
      "Epoch :0.9    Train Loss :0.00048558192793279886    Test Loss :0.002188457641750574\n",
      "Epoch :0.9083333333333333    Train Loss :0.0003592080029193312    Test Loss :0.0019019945757463574\n",
      "Epoch :0.9166666666666666    Train Loss :0.0003729753661900759    Test Loss :0.0015824015717953444\n",
      "Epoch :0.925    Train Loss :0.0003650178841780871    Test Loss :0.0017624407773837447\n",
      "Epoch :0.9333333333333333    Train Loss :0.00030020379927009344    Test Loss :0.0017203128663823009\n",
      "Epoch :0.9416666666666667    Train Loss :0.0003127479285467416    Test Loss :0.0017761136405169964\n",
      "Epoch :0.95    Train Loss :0.0003153233847115189    Test Loss :0.0016766157932579517\n",
      "Epoch :0.9583333333333334    Train Loss :0.00029761288897134364    Test Loss :0.0015224621165543795\n",
      "Epoch :0.9666666666666667    Train Loss :0.00029102404369041324    Test Loss :0.0014830105938017368\n",
      "Epoch :0.975    Train Loss :0.0002865897258743644    Test Loss :0.001663535600528121\n",
      "Epoch :0.9833333333333333    Train Loss :0.00031811234657652676    Test Loss :0.0016330385114997625\n",
      "Epoch :0.9916666666666667    Train Loss :0.00033365614945068955    Test Loss :0.001631847582757473\n",
      "Epoch :1.0    Train Loss :0.0002850611344911158    Test Loss :0.001628002617508173\n",
      "RMSE: 8.636320655938844\n",
      "MAE: 7.049596975020804\n",
      "MAPE: 6.395139071159912%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 600, 'hid_size': 100, 'lr': 0.01, 'num_layers': 5}\n",
      "total: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 44.0\n",
      "Epoch :0.008333333333333333    Train Loss :0.13178391754627228    Test Loss :0.5047999024391174\n",
      "Epoch :0.016666666666666666    Train Loss :0.0411512665450573    Test Loss :0.13137558102607727\n",
      "Epoch :0.025    Train Loss :0.07011660933494568    Test Loss :0.1946842521429062\n",
      "Epoch :0.03333333333333333    Train Loss :0.07939823716878891    Test Loss :0.43247392773628235\n",
      "Epoch :0.041666666666666664    Train Loss :0.04651235416531563    Test Loss :0.2567085921764374\n",
      "Epoch :0.05    Train Loss :0.034774329513311386    Test Loss :0.11488626897335052\n",
      "Epoch :0.058333333333333334    Train Loss :0.02606109343469143    Test Loss :0.12565499544143677\n",
      "Epoch :0.06666666666666667    Train Loss :0.01806720159947872    Test Loss :0.05351120978593826\n",
      "Epoch :0.075    Train Loss :0.005221398081630468    Test Loss :0.010257025249302387\n",
      "Epoch :0.08333333333333333    Train Loss :0.00351884588599205    Test Loss :0.009824453853070736\n",
      "Epoch :0.09166666666666666    Train Loss :0.003553125075995922    Test Loss :0.005847747437655926\n",
      "Epoch :0.1    Train Loss :0.0019260658882558346    Test Loss :0.0031742542050778866\n",
      "Epoch :0.10833333333333334    Train Loss :0.0020077021326869726    Test Loss :0.0063177854754030704\n",
      "Epoch :0.11666666666666667    Train Loss :0.001777869532816112    Test Loss :0.006721826735883951\n",
      "Epoch :0.125    Train Loss :0.0014032041653990746    Test Loss :0.004122959449887276\n",
      "Epoch :0.13333333333333333    Train Loss :0.0011855337070301175    Test Loss :0.003240694757550955\n",
      "Epoch :0.14166666666666666    Train Loss :0.0011142221046611667    Test Loss :0.0025447187945246696\n",
      "Epoch :0.15    Train Loss :0.0010923712980002165    Test Loss :0.002677955897524953\n",
      "Epoch :0.15833333333333333    Train Loss :0.0010582479881122708    Test Loss :0.0032999396789819\n",
      "Epoch :0.16666666666666666    Train Loss :0.0010345984483137727    Test Loss :0.003031207248568535\n",
      "Epoch :0.175    Train Loss :0.0009455681429244578    Test Loss :0.0028681608382612467\n",
      "Epoch :0.18333333333333332    Train Loss :0.0009349415777251124    Test Loss :0.002886612666770816\n",
      "Epoch :0.19166666666666668    Train Loss :0.0009113525738939643    Test Loss :0.002601045649498701\n",
      "Epoch :0.2    Train Loss :0.0008973933872766793    Test Loss :0.0024280373472720385\n",
      "Epoch :0.20833333333333334    Train Loss :0.0008616472478024662    Test Loss :0.0025160114746540785\n",
      "Epoch :0.21666666666666667    Train Loss :0.0008523380965925753    Test Loss :0.0024471839424222708\n",
      "Epoch :0.225    Train Loss :0.0008415674092248082    Test Loss :0.0024664406664669514\n",
      "Epoch :0.23333333333333334    Train Loss :0.0008438015938736498    Test Loss :0.002523350063711405\n",
      "Epoch :0.24166666666666667    Train Loss :0.0007931344443932176    Test Loss :0.0022983814124017954\n",
      "Epoch :0.25    Train Loss :0.0007797907455824316    Test Loss :0.0022911084815859795\n",
      "Epoch :0.25833333333333336    Train Loss :0.0008129562484100461    Test Loss :0.002182074822485447\n",
      "Epoch :0.26666666666666666    Train Loss :0.0008370567229576409    Test Loss :0.0020755885634571314\n",
      "Epoch :0.275    Train Loss :0.0007659292314201593    Test Loss :0.0021337575744837523\n",
      "Epoch :0.2833333333333333    Train Loss :0.0007485668174922466    Test Loss :0.00236105197109282\n",
      "Epoch :0.2916666666666667    Train Loss :0.00075019511859864    Test Loss :0.00218606973066926\n",
      "Epoch :0.3    Train Loss :0.0007401045877486467    Test Loss :0.001963602378964424\n",
      "Epoch :0.30833333333333335    Train Loss :0.0007192965131253004    Test Loss :0.0020417615305632353\n",
      "Epoch :0.31666666666666665    Train Loss :0.000730336585547775    Test Loss :0.002001010114327073\n",
      "Epoch :0.325    Train Loss :0.0007267732871696353    Test Loss :0.0018361428519710898\n",
      "Epoch :0.3333333333333333    Train Loss :0.0007270484929904342    Test Loss :0.002048940397799015\n",
      "Epoch :0.3416666666666667    Train Loss :0.0007040068157948554    Test Loss :0.001989588839933276\n",
      "Epoch :0.35    Train Loss :0.0007147631258703768    Test Loss :0.0017693151021376252\n",
      "Epoch :0.35833333333333334    Train Loss :0.0006853412487544119    Test Loss :0.002018566243350506\n",
      "Epoch :0.36666666666666664    Train Loss :0.0006718867225572467    Test Loss :0.002016273560002446\n",
      "Epoch :0.375    Train Loss :0.0006609128322452307    Test Loss :0.0017371930880472064\n",
      "Epoch :0.38333333333333336    Train Loss :0.0006870989454910159    Test Loss :0.00183701585046947\n",
      "Epoch :0.39166666666666666    Train Loss :0.0006957339937798679    Test Loss :0.0018704637186601758\n",
      "Epoch :0.4    Train Loss :0.0006621098727919161    Test Loss :0.001806871616281569\n",
      "Epoch :0.4083333333333333    Train Loss :0.000668174063321203    Test Loss :0.001877293223515153\n",
      "Epoch :0.4166666666666667    Train Loss :0.0006905738264322281    Test Loss :0.001797154196538031\n",
      "Epoch :0.425    Train Loss :0.0006323052220977843    Test Loss :0.0017250534147024155\n",
      "Epoch :0.43333333333333335    Train Loss :0.0006483700126409531    Test Loss :0.0017150900093838573\n",
      "Epoch :0.44166666666666665    Train Loss :0.0006580995977856219    Test Loss :0.0016893218271434307\n",
      "Epoch :0.45    Train Loss :0.0006334303761832416    Test Loss :0.001809729146771133\n",
      "Epoch :0.4583333333333333    Train Loss :0.0006506487261503935    Test Loss :0.0016770076472312212\n",
      "Epoch :0.4666666666666667    Train Loss :0.0006252283346839249    Test Loss :0.0016855945577844977\n",
      "Epoch :0.475    Train Loss :0.0006163870566524565    Test Loss :0.00154986628331244\n",
      "Epoch :0.48333333333333334    Train Loss :0.0006284556002356112    Test Loss :0.0015209598932415247\n",
      "Epoch :0.49166666666666664    Train Loss :0.0006214659078978002    Test Loss :0.0017325044609606266\n",
      "Epoch :0.5    Train Loss :0.0006360802217386663    Test Loss :0.0016364648472517729\n",
      "Epoch :0.5083333333333333    Train Loss :0.000617532292380929    Test Loss :0.0017572707729414105\n",
      "Epoch :0.5166666666666667    Train Loss :0.0006130448309704661    Test Loss :0.001621350646018982\n",
      "Epoch :0.525    Train Loss :0.0006158781470730901    Test Loss :0.0015433707740157843\n",
      "Epoch :0.5333333333333333    Train Loss :0.0006179075571708381    Test Loss :0.0015463079325854778\n",
      "Epoch :0.5416666666666666    Train Loss :0.0006300554960034788    Test Loss :0.0017284598434343934\n",
      "Epoch :0.55    Train Loss :0.0006173509755171835    Test Loss :0.0015916046686470509\n",
      "Epoch :0.5583333333333333    Train Loss :0.0005945331649854779    Test Loss :0.0015416789101436734\n",
      "Epoch :0.5666666666666667    Train Loss :0.0006033604149706662    Test Loss :0.0015003465814515948\n",
      "Epoch :0.575    Train Loss :0.0006000433349981904    Test Loss :0.0015588649548590183\n",
      "Epoch :0.5833333333333334    Train Loss :0.0005917474627494812    Test Loss :0.0014248023508116603\n",
      "Epoch :0.5916666666666667    Train Loss :0.0005709142424166203    Test Loss :0.0015162475174292922\n",
      "Epoch :0.6    Train Loss :0.0005777873448096216    Test Loss :0.0015377920353785157\n",
      "Epoch :0.6083333333333333    Train Loss :0.0005924114375375211    Test Loss :0.001606401870958507\n",
      "Epoch :0.6166666666666667    Train Loss :0.0005928256432525814    Test Loss :0.0014386046677827835\n",
      "Epoch :0.625    Train Loss :0.0005776381585747004    Test Loss :0.0014643690083175898\n",
      "Epoch :0.6333333333333333    Train Loss :0.0005571911460720003    Test Loss :0.0014070481993258\n",
      "Epoch :0.6416666666666667    Train Loss :0.0005710694240406156    Test Loss :0.0014695810386911035\n",
      "Epoch :0.65    Train Loss :0.0005649641971103847    Test Loss :0.0015221800422295928\n",
      "Epoch :0.6583333333333333    Train Loss :0.0005706895026378334    Test Loss :0.0013362363679334521\n",
      "Epoch :0.6666666666666666    Train Loss :0.0005856369971297681    Test Loss :0.001429673284292221\n",
      "Epoch :0.675    Train Loss :0.0005970572237856686    Test Loss :0.0014424948021769524\n",
      "Epoch :0.6833333333333333    Train Loss :0.0005787567351944745    Test Loss :0.001405412913300097\n",
      "Epoch :0.6916666666666667    Train Loss :0.000561649794690311    Test Loss :0.0014640724984928966\n",
      "Epoch :0.7    Train Loss :0.0005561052821576595    Test Loss :0.0013834602432325482\n",
      "Epoch :0.7083333333333334    Train Loss :0.0005560379358939826    Test Loss :0.001406306866556406\n",
      "Epoch :0.7166666666666667    Train Loss :0.0005690452526323497    Test Loss :0.0014300302136689425\n",
      "Epoch :0.725    Train Loss :0.0005520705599337816    Test Loss :0.0015661538345739245\n",
      "Epoch :0.7333333333333333    Train Loss :0.00054663181072101    Test Loss :0.0013629213208332658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7416666666666667    Train Loss :0.0005348734557628632    Test Loss :0.0013930335408076644\n",
      "Epoch :0.75    Train Loss :0.0005402229726314545    Test Loss :0.001323253964073956\n",
      "Epoch :0.7583333333333333    Train Loss :0.0005206139176152647    Test Loss :0.0015208640834316611\n",
      "Epoch :0.7666666666666667    Train Loss :0.0005271725822240114    Test Loss :0.001307755010202527\n",
      "Epoch :0.775    Train Loss :0.0005343431839719415    Test Loss :0.0014490761095657945\n",
      "Epoch :0.7833333333333333    Train Loss :0.0005454081692732871    Test Loss :0.0013233722420409322\n",
      "Epoch :0.7916666666666666    Train Loss :0.0005315066664479673    Test Loss :0.0013672378845512867\n",
      "Epoch :0.8    Train Loss :0.0005388090503402054    Test Loss :0.0013851475669071078\n",
      "Epoch :0.8083333333333333    Train Loss :0.0005450238240882754    Test Loss :0.0012235434260219336\n",
      "Epoch :0.8166666666666667    Train Loss :0.0005419702501967549    Test Loss :0.0013089528074488044\n",
      "Epoch :0.825    Train Loss :0.0005292033310979605    Test Loss :0.0013351363595575094\n",
      "Epoch :0.8333333333333334    Train Loss :0.0005224553169682622    Test Loss :0.0013173340121284127\n",
      "Epoch :0.8416666666666667    Train Loss :0.00052382837748155    Test Loss :0.0011988404439762235\n",
      "Epoch :0.85    Train Loss :0.000522311485838145    Test Loss :0.0013014294672757387\n",
      "Epoch :0.8583333333333333    Train Loss :0.0005187050555832684    Test Loss :0.0011442751856520772\n",
      "Epoch :0.8666666666666667    Train Loss :0.0005204377230256796    Test Loss :0.001263005891814828\n",
      "Epoch :0.875    Train Loss :0.000535341736394912    Test Loss :0.0013279331615194678\n",
      "Epoch :0.8833333333333333    Train Loss :0.0005259959143586457    Test Loss :0.0013384104240685701\n",
      "Epoch :0.8916666666666667    Train Loss :0.000520099070854485    Test Loss :0.0011946456506848335\n",
      "Epoch :0.9    Train Loss :0.0005083574797026813    Test Loss :0.0012506133643910289\n",
      "Epoch :0.9083333333333333    Train Loss :0.000529336160980165    Test Loss :0.0012388674076646566\n",
      "Epoch :0.9166666666666666    Train Loss :0.0004950869479216635    Test Loss :0.0012322445400059223\n",
      "Epoch :0.925    Train Loss :0.0004967505228705704    Test Loss :0.001198209123685956\n",
      "Epoch :0.9333333333333333    Train Loss :0.00048519938718527555    Test Loss :0.0011178322602063417\n",
      "Epoch :0.9416666666666667    Train Loss :0.0005099845002405345    Test Loss :0.0010598321678116918\n",
      "Epoch :0.95    Train Loss :0.0005004102713428438    Test Loss :0.0010391385294497013\n",
      "Epoch :0.9583333333333334    Train Loss :0.0004941229708492756    Test Loss :0.0012474566465243697\n",
      "Epoch :0.9666666666666667    Train Loss :0.0004956425982527435    Test Loss :0.0011270842514932156\n",
      "Epoch :0.975    Train Loss :0.0005070362822152674    Test Loss :0.001212258473969996\n",
      "Epoch :0.9833333333333333    Train Loss :0.00048511975910514593    Test Loss :0.0011773959267884493\n",
      "Epoch :0.9916666666666667    Train Loss :0.0004929627757519484    Test Loss :0.0011775294551625848\n",
      "Epoch :1.0    Train Loss :0.0005038953386247158    Test Loss :0.0011978590628132224\n",
      "RMSE: 8.64592384906597\n",
      "MAE: 6.868960760916683\n",
      "MAPE: 5.96488327130988%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 600, 'hid_size': 100, 'lr': 0.02, 'num_layers': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  47.0\n",
      "Epoch :0.008333333333333333    Train Loss :0.07068443298339844    Test Loss :0.35256049036979675\n",
      "Epoch :0.016666666666666666    Train Loss :0.04225414618849754    Test Loss :0.17701546847820282\n",
      "Epoch :0.025    Train Loss :0.04066307097673416    Test Loss :0.16859403252601624\n",
      "Epoch :0.03333333333333333    Train Loss :0.03720682114362717    Test Loss :0.16177967190742493\n",
      "Epoch :0.041666666666666664    Train Loss :0.04032188653945923    Test Loss :0.22363640367984772\n",
      "Epoch :0.05    Train Loss :0.04019802436232567    Test Loss :0.26424440741539\n",
      "Epoch :0.058333333333333334    Train Loss :0.03986304625868797    Test Loss :0.15736684203147888\n",
      "Epoch :0.06666666666666667    Train Loss :0.028533723205327988    Test Loss :0.11598388850688934\n",
      "Epoch :0.075    Train Loss :0.03095152974128723    Test Loss :0.03281962126493454\n",
      "Epoch :0.08333333333333333    Train Loss :0.00792598631232977    Test Loss :0.059401992708444595\n",
      "Epoch :0.09166666666666666    Train Loss :0.006944803521037102    Test Loss :0.00787661038339138\n",
      "Epoch :0.1    Train Loss :0.005319405812770128    Test Loss :0.010237546637654305\n",
      "Epoch :0.10833333333333334    Train Loss :0.0037672400940209627    Test Loss :0.02230825088918209\n",
      "Epoch :0.11666666666666667    Train Loss :0.0023397805634886026    Test Loss :0.010374612174928188\n",
      "Epoch :0.125    Train Loss :0.0018360523972660303    Test Loss :0.007192022632807493\n",
      "Epoch :0.13333333333333333    Train Loss :0.0014795918250456452    Test Loss :0.0052870637737214565\n",
      "Epoch :0.14166666666666666    Train Loss :0.001385094947181642    Test Loss :0.005763176362961531\n",
      "Epoch :0.15    Train Loss :0.0012295317137613893    Test Loss :0.004429567605257034\n",
      "Epoch :0.15833333333333333    Train Loss :0.0011684357887133956    Test Loss :0.004027131944894791\n",
      "Epoch :0.16666666666666666    Train Loss :0.0010898637119680643    Test Loss :0.004046059213578701\n",
      "Epoch :0.175    Train Loss :0.0010005688527598977    Test Loss :0.003778947051614523\n",
      "Epoch :0.18333333333333332    Train Loss :0.0009632671135477722    Test Loss :0.0026934114284813404\n",
      "Epoch :0.19166666666666668    Train Loss :0.0008985311724245548    Test Loss :0.0029603480361402035\n",
      "Epoch :0.2    Train Loss :0.0008951548370532691    Test Loss :0.0028621938545256853\n",
      "Epoch :0.20833333333333334    Train Loss :0.0008269376121461391    Test Loss :0.0026454392354935408\n",
      "Epoch :0.21666666666666667    Train Loss :0.0008686678484082222    Test Loss :0.002797061111778021\n",
      "Epoch :0.225    Train Loss :0.0008144982857629657    Test Loss :0.0026951846666634083\n",
      "Epoch :0.23333333333333334    Train Loss :0.0007702134898863733    Test Loss :0.0026199608109891415\n",
      "Epoch :0.24166666666666667    Train Loss :0.000775903754401952    Test Loss :0.0025513179134577513\n",
      "Epoch :0.25    Train Loss :0.0007629235624335706    Test Loss :0.0024485012982040644\n",
      "Epoch :0.25833333333333336    Train Loss :0.0007455648155882955    Test Loss :0.002433378715068102\n",
      "Epoch :0.26666666666666666    Train Loss :0.0007077858899720013    Test Loss :0.0024123096372932196\n",
      "Epoch :0.275    Train Loss :0.0007010995177552104    Test Loss :0.0023799703922122717\n",
      "Epoch :0.2833333333333333    Train Loss :0.0006931879324838519    Test Loss :0.0021779206581413746\n",
      "Epoch :0.2916666666666667    Train Loss :0.0006753816269338131    Test Loss :0.0024186319205909967\n",
      "Epoch :0.3    Train Loss :0.00065952492877841    Test Loss :0.0023414636962115765\n",
      "Epoch :0.30833333333333335    Train Loss :0.0006424018647521734    Test Loss :0.0022392491810023785\n",
      "Epoch :0.31666666666666665    Train Loss :0.0006451157969422638    Test Loss :0.0024127757642418146\n",
      "Epoch :0.325    Train Loss :0.0006370003684423864    Test Loss :0.0021989180240780115\n",
      "Epoch :0.3333333333333333    Train Loss :0.0006117575685493648    Test Loss :0.0022331734653562307\n",
      "Epoch :0.3416666666666667    Train Loss :0.0006420751451514661    Test Loss :0.0022256558295339346\n",
      "Epoch :0.35    Train Loss :0.0006115908618085086    Test Loss :0.002111381385475397\n",
      "Epoch :0.35833333333333334    Train Loss :0.0005973296356387436    Test Loss :0.002186035504564643\n",
      "Epoch :0.36666666666666664    Train Loss :0.000595572404563427    Test Loss :0.0021341426763683558\n",
      "Epoch :0.375    Train Loss :0.0005734079168178141    Test Loss :0.0022426717914640903\n",
      "Epoch :0.38333333333333336    Train Loss :0.0005969341145828366    Test Loss :0.0020456183701753616\n",
      "Epoch :0.39166666666666666    Train Loss :0.000581078405957669    Test Loss :0.0021735273767262697\n",
      "Epoch :0.4    Train Loss :0.0005472266348078847    Test Loss :0.002055410761386156\n",
      "Epoch :0.4083333333333333    Train Loss :0.0005549637717194855    Test Loss :0.001987469382584095\n",
      "Epoch :0.4166666666666667    Train Loss :0.0005511862691491842    Test Loss :0.0021762014366686344\n",
      "Epoch :0.425    Train Loss :0.0005462546832859516    Test Loss :0.0020947973243892193\n",
      "Epoch :0.43333333333333335    Train Loss :0.0005297182942740619    Test Loss :0.002273856196552515\n",
      "Epoch :0.44166666666666665    Train Loss :0.0005243818159215152    Test Loss :0.002139474032446742\n",
      "Epoch :0.45    Train Loss :0.0005371264996938407    Test Loss :0.0020883018150925636\n",
      "Epoch :0.4583333333333333    Train Loss :0.0005364957614801824    Test Loss :0.0019812078680843115\n",
      "Epoch :0.4666666666666667    Train Loss :0.0005307367537170649    Test Loss :0.0021193670108914375\n",
      "Epoch :0.475    Train Loss :0.0004851300036534667    Test Loss :0.0020192377269268036\n",
      "Epoch :0.48333333333333334    Train Loss :0.000534704711753875    Test Loss :0.0020654741674661636\n",
      "Epoch :0.49166666666666664    Train Loss :0.0005069178296253085    Test Loss :0.0020708884112536907\n",
      "Epoch :0.5    Train Loss :0.0004942707601003349    Test Loss :0.0020834875758737326\n",
      "Epoch :0.5083333333333333    Train Loss :0.0004982183454558253    Test Loss :0.0020049908198416233\n",
      "Epoch :0.5166666666666667    Train Loss :0.00047746970085427165    Test Loss :0.0020509378518909216\n",
      "Epoch :0.525    Train Loss :0.0004968683351762593    Test Loss :0.0019803857430815697\n",
      "Epoch :0.5333333333333333    Train Loss :0.0005073146312497556    Test Loss :0.002404110971838236\n",
      "Epoch :0.5416666666666666    Train Loss :0.0005267257220111787    Test Loss :0.0021867991890758276\n",
      "Epoch :0.55    Train Loss :0.0004876156453974545    Test Loss :0.0022043976932764053\n",
      "Epoch :0.5583333333333333    Train Loss :0.00046992237912490964    Test Loss :0.0020183182787150145\n",
      "Epoch :0.5666666666666667    Train Loss :0.0004810422542504966    Test Loss :0.0019813040271401405\n",
      "Epoch :0.575    Train Loss :0.0005102457362227142    Test Loss :0.0021752226166427135\n",
      "Epoch :0.5833333333333334    Train Loss :0.0005474878125824034    Test Loss :0.0022906712256371975\n",
      "Epoch :0.5916666666666667    Train Loss :0.0005138433771207929    Test Loss :0.002298288978636265\n",
      "Epoch :0.6    Train Loss :0.0004924245877191424    Test Loss :0.002165823709219694\n",
      "Epoch :0.6083333333333333    Train Loss :0.0005232661496847868    Test Loss :0.002153112320229411\n",
      "Epoch :0.6166666666666667    Train Loss :0.0004912668955512345    Test Loss :0.002224859781563282\n",
      "Epoch :0.625    Train Loss :0.0004602512053679675    Test Loss :0.0019266951130703092\n",
      "Epoch :0.6333333333333333    Train Loss :0.00045971479266881943    Test Loss :0.0020635430701076984\n",
      "Epoch :0.6416666666666667    Train Loss :0.0004594298079609871    Test Loss :0.0023195540998131037\n",
      "Epoch :0.65    Train Loss :0.00040565483504906297    Test Loss :0.0020983763970434666\n",
      "Epoch :0.6583333333333333    Train Loss :0.0004714200331363827    Test Loss :0.001955657731741667\n",
      "Epoch :0.6666666666666666    Train Loss :0.0004207334714010358    Test Loss :0.0019420454045757651\n",
      "Epoch :0.675    Train Loss :0.0004043945809826255    Test Loss :0.001962528331205249\n",
      "Epoch :0.6833333333333333    Train Loss :0.0006396652315743268    Test Loss :0.0032101627439260483\n",
      "Epoch :0.6916666666666667    Train Loss :0.00040810491191223264    Test Loss :0.0027605595532804728\n",
      "Epoch :0.7    Train Loss :0.0005030934698879719    Test Loss :0.0024514663964509964\n",
      "Epoch :0.7083333333333334    Train Loss :0.0004829954996239394    Test Loss :0.001976295141503215\n",
      "Epoch :0.7166666666666667    Train Loss :0.00039942035800777376    Test Loss :0.0018951244419440627\n",
      "Epoch :0.725    Train Loss :0.0004115248448215425    Test Loss :0.0017864900873973966\n",
      "Epoch :0.7333333333333333    Train Loss :0.0004023721267003566    Test Loss :0.0017266209470108151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7416666666666667    Train Loss :0.0003904961049556732    Test Loss :0.0018521469319239259\n",
      "Epoch :0.75    Train Loss :0.00039926328463479877    Test Loss :0.001915554516017437\n",
      "Epoch :0.7583333333333333    Train Loss :0.0004261855792719871    Test Loss :0.0019954226445406675\n",
      "Epoch :0.7666666666666667    Train Loss :0.00039663727511651814    Test Loss :0.002108419081196189\n",
      "Epoch :0.775    Train Loss :0.00037877875729463995    Test Loss :0.0018123723566532135\n",
      "Epoch :0.7833333333333333    Train Loss :0.00041992278420366347    Test Loss :0.0019051956478506327\n",
      "Epoch :0.7916666666666666    Train Loss :0.0004597623774316162    Test Loss :0.002027332317084074\n",
      "Epoch :0.8    Train Loss :0.000780215603299439    Test Loss :0.002645523054525256\n",
      "Epoch :0.8083333333333333    Train Loss :0.00044799892930313945    Test Loss :0.0020464430563151836\n",
      "Epoch :0.8166666666666667    Train Loss :0.0004905675305053592    Test Loss :0.0020447196438908577\n",
      "Epoch :0.825    Train Loss :0.0004153789777774364    Test Loss :0.0021331238094717264\n",
      "Epoch :0.8333333333333334    Train Loss :0.00044479459756985307    Test Loss :0.0020175373647361994\n",
      "Epoch :0.8416666666666667    Train Loss :0.0003838885750155896    Test Loss :0.0017561687855049968\n",
      "Epoch :0.85    Train Loss :0.00036862731212750077    Test Loss :0.0018800933612510562\n",
      "Epoch :0.8583333333333333    Train Loss :0.0003718332445714623    Test Loss :0.0019275869708508253\n",
      "Epoch :0.8666666666666667    Train Loss :0.00036439101677387953    Test Loss :0.0018096667481586337\n",
      "Epoch :0.875    Train Loss :0.0003668197023216635    Test Loss :0.0017853486351668835\n",
      "Epoch :0.8833333333333333    Train Loss :0.0004081038059666753    Test Loss :0.0021193495485931635\n",
      "Epoch :0.8916666666666667    Train Loss :0.00035766587825492024    Test Loss :0.0018149330280721188\n",
      "Epoch :0.9    Train Loss :0.0003784525324590504    Test Loss :0.001859358511865139\n",
      "Epoch :0.9083333333333333    Train Loss :0.00037222501123324037    Test Loss :0.0018799520330503583\n",
      "Epoch :0.9166666666666666    Train Loss :0.00043603399535641074    Test Loss :0.002269407967105508\n",
      "Epoch :0.925    Train Loss :0.0003524773055687547    Test Loss :0.0018643393414095044\n",
      "Epoch :0.9333333333333333    Train Loss :0.0003579100884962827    Test Loss :0.0017728750826790929\n",
      "Epoch :0.9416666666666667    Train Loss :0.0003493668045848608    Test Loss :0.0017529376782476902\n",
      "Epoch :0.95    Train Loss :0.0006967556546442211    Test Loss :0.005253833252936602\n",
      "Epoch :0.9583333333333334    Train Loss :0.0005157345440238714    Test Loss :0.002616019919514656\n",
      "Epoch :0.9666666666666667    Train Loss :0.0004615020880009979    Test Loss :0.002119565149769187\n",
      "Epoch :0.975    Train Loss :0.00041770259849727154    Test Loss :0.0027260975912213326\n",
      "Epoch :0.9833333333333333    Train Loss :0.0004218873509671539    Test Loss :0.0018860569689422846\n",
      "Epoch :0.9916666666666667    Train Loss :0.00042440483230166137    Test Loss :0.0016611003084108233\n",
      "Epoch :1.0    Train Loss :0.0003959515306632966    Test Loss :0.0018473499221727252\n",
      "RMSE: 8.294740226643256\n",
      "MAE: 6.743035532233694\n",
      "MAPE: 6.0822837761461095%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 600, 'hid_size': 100, 'lr': 0.02, 'num_layers': 5}\n",
      "total:  50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.04304426163434982    Test Loss :0.30804505944252014\n",
      "Epoch :0.025    Train Loss :0.03339626267552376    Test Loss :0.11215526610612869\n",
      "Epoch :0.0375    Train Loss :0.02460586279630661    Test Loss :0.1343744844198227\n",
      "Epoch :0.05    Train Loss :0.013360419310629368    Test Loss :0.04393405094742775\n",
      "Epoch :0.0625    Train Loss :0.005381848197430372    Test Loss :0.022607972845435143\n",
      "Epoch :0.075    Train Loss :0.005489117465913296    Test Loss :0.025993410497903824\n",
      "Epoch :0.0875    Train Loss :0.0026527117006480694    Test Loss :0.007076157722622156\n",
      "Epoch :0.1    Train Loss :0.002157624578103423    Test Loss :0.006719144061207771\n",
      "Epoch :0.1125    Train Loss :0.002203290816396475    Test Loss :0.012649977579712868\n",
      "Epoch :0.125    Train Loss :0.0016594035550951958    Test Loss :0.005400558467954397\n",
      "Epoch :0.1375    Train Loss :0.0014270349638536572    Test Loss :0.005244587082415819\n",
      "Epoch :0.15    Train Loss :0.0013773288810625672    Test Loss :0.005705005489289761\n",
      "Epoch :0.1625    Train Loss :0.0012921388261020184    Test Loss :0.0042730108834803104\n",
      "Epoch :0.175    Train Loss :0.0012697894126176834    Test Loss :0.00458076037466526\n",
      "Epoch :0.1875    Train Loss :0.0011348366970196366    Test Loss :0.004132078029215336\n",
      "Epoch :0.2    Train Loss :0.0011238467413932085    Test Loss :0.004475906491279602\n",
      "Epoch :0.2125    Train Loss :0.001115251798182726    Test Loss :0.004743415396660566\n",
      "Epoch :0.225    Train Loss :0.0010534289758652449    Test Loss :0.003993052989244461\n",
      "Epoch :0.2375    Train Loss :0.0009844561573117971    Test Loss :0.0038620256818830967\n",
      "Epoch :0.25    Train Loss :0.000955579336732626    Test Loss :0.003786592511460185\n",
      "Epoch :0.2625    Train Loss :0.0009245467954315245    Test Loss :0.0035642352886497974\n",
      "Epoch :0.275    Train Loss :0.0009063681936822832    Test Loss :0.003666928969323635\n",
      "Epoch :0.2875    Train Loss :0.0008782832883298397    Test Loss :0.0032557123340666294\n",
      "Epoch :0.3    Train Loss :0.0008642837055958807    Test Loss :0.0033780266530811787\n",
      "Epoch :0.3125    Train Loss :0.0008150554494932294    Test Loss :0.0032221623696386814\n",
      "Epoch :0.325    Train Loss :0.0008220939198508859    Test Loss :0.002967594424262643\n",
      "Epoch :0.3375    Train Loss :0.0007871757843531668    Test Loss :0.0027286403346806765\n",
      "Epoch :0.35    Train Loss :0.0007078115595504642    Test Loss :0.002601341810077429\n",
      "Epoch :0.3625    Train Loss :0.0007645522127859294    Test Loss :0.002672716975212097\n",
      "Epoch :0.375    Train Loss :0.0006996030570007861    Test Loss :0.002209410071372986\n",
      "Epoch :0.3875    Train Loss :0.000665106053929776    Test Loss :0.0021986644715070724\n",
      "Epoch :0.4    Train Loss :0.0006747370935045183    Test Loss :0.0020964257419109344\n",
      "Epoch :0.4125    Train Loss :0.0006910194060765207    Test Loss :0.001988236093893647\n",
      "Epoch :0.425    Train Loss :0.0006321370601654053    Test Loss :0.0017394294263795018\n",
      "Epoch :0.4375    Train Loss :0.0006139722536318004    Test Loss :0.001709013362415135\n",
      "Epoch :0.45    Train Loss :0.0006434820243157446    Test Loss :0.0017130373744294047\n",
      "Epoch :0.4625    Train Loss :0.0006053692777641118    Test Loss :0.0016837258590385318\n",
      "Epoch :0.475    Train Loss :0.000594317156355828    Test Loss :0.0016674441285431385\n",
      "Epoch :0.4875    Train Loss :0.0005452641053125262    Test Loss :0.0014447093708440661\n",
      "Epoch :0.5    Train Loss :0.0005280520999804139    Test Loss :0.001413284451700747\n",
      "Epoch :0.5125    Train Loss :0.0005830333684571087    Test Loss :0.0018143433844670653\n",
      "Epoch :0.525    Train Loss :0.0005390616715885699    Test Loss :0.001751631498336792\n",
      "Epoch :0.5375    Train Loss :0.000520937901455909    Test Loss :0.0015676526818424463\n",
      "Epoch :0.55    Train Loss :0.0005247968365438282    Test Loss :0.0016552126035094261\n",
      "Epoch :0.5625    Train Loss :0.0005242523620836437    Test Loss :0.0019178851507604122\n",
      "Epoch :0.575    Train Loss :0.00048516056267544627    Test Loss :0.0016949169803410769\n",
      "Epoch :0.5875    Train Loss :0.0004985606647096574    Test Loss :0.0016710528871044517\n",
      "Epoch :0.6    Train Loss :0.000515535706654191    Test Loss :0.001499200938269496\n",
      "Epoch :0.6125    Train Loss :0.0004530581063590944    Test Loss :0.001529205939732492\n",
      "Epoch :0.625    Train Loss :0.0004617822414729744    Test Loss :0.0013636202784255147\n",
      "Epoch :0.6375    Train Loss :0.000438473594840616    Test Loss :0.001370208221487701\n",
      "Epoch :0.65    Train Loss :0.00042893114732578397    Test Loss :0.0012497171992436051\n",
      "Epoch :0.6625    Train Loss :0.0005865843268111348    Test Loss :0.002106864470988512\n",
      "Epoch :0.675    Train Loss :0.00045467150630429387    Test Loss :0.002202774165198207\n",
      "Epoch :0.6875    Train Loss :0.000507725402712822    Test Loss :0.0015381425619125366\n",
      "Epoch :0.7    Train Loss :0.0004587933071888983    Test Loss :0.0012438640696927905\n",
      "Epoch :0.7125    Train Loss :0.0004677629913203418    Test Loss :0.0012018212582916021\n",
      "Epoch :0.725    Train Loss :0.0004153227782808244    Test Loss :0.0013042099308222532\n",
      "Epoch :0.7375    Train Loss :0.00041588759631849825    Test Loss :0.001375786610879004\n",
      "Epoch :0.75    Train Loss :0.0004117713833693415    Test Loss :0.001253669266588986\n",
      "Epoch :0.7625    Train Loss :0.00040269948658533394    Test Loss :0.0012845301534980536\n",
      "Epoch :0.775    Train Loss :0.00041878680349327624    Test Loss :0.0011182936141267419\n",
      "Epoch :0.7875    Train Loss :0.00040687090950086713    Test Loss :0.001158697996288538\n",
      "Epoch :0.8    Train Loss :0.00041279775905422866    Test Loss :0.0012233806774020195\n",
      "Epoch :0.8125    Train Loss :0.00037151004653424025    Test Loss :0.0010854029096662998\n",
      "Epoch :0.825    Train Loss :0.00040115497540682554    Test Loss :0.0010963366366922855\n",
      "Epoch :0.8375    Train Loss :0.0006326944567263126    Test Loss :0.0024736220948398113\n",
      "Epoch :0.85    Train Loss :0.00044307808275334537    Test Loss :0.0018399287946522236\n",
      "Epoch :0.8625    Train Loss :0.00043038930743932724    Test Loss :0.0013353802496567369\n",
      "Epoch :0.875    Train Loss :0.00038728665094822645    Test Loss :0.001046632183715701\n",
      "Epoch :0.8875    Train Loss :0.0004381172184366733    Test Loss :0.0012915824772790074\n",
      "Epoch :0.9    Train Loss :0.000421763863414526    Test Loss :0.0010777250863611698\n",
      "Epoch :0.9125    Train Loss :0.00037484377389773726    Test Loss :0.001271628076210618\n",
      "Epoch :0.925    Train Loss :0.0004019019252154976    Test Loss :0.0010114071192219853\n",
      "Epoch :0.9375    Train Loss :0.0004001101478934288    Test Loss :0.0012179045006632805\n",
      "Epoch :0.95    Train Loss :0.0003839308046735823    Test Loss :0.0010392179246991873\n",
      "Epoch :0.9625    Train Loss :0.00036208840901963413    Test Loss :0.000978844822384417\n",
      "Epoch :0.975    Train Loss :0.0003580328484531492    Test Loss :0.0009487715433351696\n",
      "Epoch :0.9875    Train Loss :0.0003566068480722606    Test Loss :0.0008895804639905691\n",
      "Epoch :1.0    Train Loss :0.0003495314158499241    Test Loss :0.0009724122355692089\n",
      "RMSE: 9.749575909706232\n",
      "MAE: 7.925999182501882\n",
      "MAPE: 7.270066132719939%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 400, 'hid_size': 50, 'lr': 0.01, 'num_layers': 2}\n",
      "total:  53.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.04089425131678581    Test Loss :0.21553738415241241\n",
      "Epoch :0.025    Train Loss :0.03830336406826973    Test Loss :0.17663566768169403\n",
      "Epoch :0.0375    Train Loss :0.016281060874462128    Test Loss :0.025725465267896652\n",
      "Epoch :0.05    Train Loss :0.009132938459515572    Test Loss :0.04774374142289162\n",
      "Epoch :0.0625    Train Loss :0.005755515303462744    Test Loss :0.013774704188108444\n",
      "Epoch :0.075    Train Loss :0.0038277767598628998    Test Loss :0.012781571596860886\n",
      "Epoch :0.0875    Train Loss :0.0037491568364202976    Test Loss :0.020646261051297188\n",
      "Epoch :0.1    Train Loss :0.0030342168174684048    Test Loss :0.011766421608626842\n",
      "Epoch :0.1125    Train Loss :0.0027556277345865965    Test Loss :0.006967690773308277\n",
      "Epoch :0.125    Train Loss :0.002361407969146967    Test Loss :0.007731851655989885\n",
      "Epoch :0.1375    Train Loss :0.00207564071752131    Test Loss :0.007697897031903267\n",
      "Epoch :0.15    Train Loss :0.0020268894731998444    Test Loss :0.008198904804885387\n",
      "Epoch :0.1625    Train Loss :0.0018982639303430915    Test Loss :0.007510271854698658\n",
      "Epoch :0.175    Train Loss :0.0017818708438426256    Test Loss :0.005834360606968403\n",
      "Epoch :0.1875    Train Loss :0.001614729524590075    Test Loss :0.005617835093289614\n",
      "Epoch :0.2    Train Loss :0.002745747799053788    Test Loss :0.00663177901878953\n",
      "Epoch :0.2125    Train Loss :0.0014541572891175747    Test Loss :0.00540172541514039\n",
      "Epoch :0.225    Train Loss :0.0014458531513810158    Test Loss :0.007404593285173178\n",
      "Epoch :0.2375    Train Loss :0.0015181301860138774    Test Loss :0.004234449006617069\n",
      "Epoch :0.25    Train Loss :0.0013368572108447552    Test Loss :0.00488580297678709\n",
      "Epoch :0.2625    Train Loss :0.0012795062502846122    Test Loss :0.004392916336655617\n",
      "Epoch :0.275    Train Loss :0.0011585440952330828    Test Loss :0.004537736065685749\n",
      "Epoch :0.2875    Train Loss :0.0012602139031514525    Test Loss :0.003668693359941244\n",
      "Epoch :0.3    Train Loss :0.0010913955047726631    Test Loss :0.0036763364914804697\n",
      "Epoch :0.3125    Train Loss :0.001099677407182753    Test Loss :0.003642421215772629\n",
      "Epoch :0.325    Train Loss :0.001078512636013329    Test Loss :0.00407735351473093\n",
      "Epoch :0.3375    Train Loss :0.0009951237589120865    Test Loss :0.0034236989449709654\n",
      "Epoch :0.35    Train Loss :0.0010399979073554277    Test Loss :0.003350467188283801\n",
      "Epoch :0.3625    Train Loss :0.001277211937122047    Test Loss :0.0044787279330194\n",
      "Epoch :0.375    Train Loss :0.0010821991600096226    Test Loss :0.004113954957574606\n",
      "Epoch :0.3875    Train Loss :0.0010619011009112    Test Loss :0.004135051276534796\n",
      "Epoch :0.4    Train Loss :0.0010885620722547174    Test Loss :0.004184095188975334\n",
      "Epoch :0.4125    Train Loss :0.0009424949530512094    Test Loss :0.003316946793347597\n",
      "Epoch :0.425    Train Loss :0.0008445063722319901    Test Loss :0.0028934143483638763\n",
      "Epoch :0.4375    Train Loss :0.0008451464236713946    Test Loss :0.0031814698595553637\n",
      "Epoch :0.45    Train Loss :0.0009200683562085032    Test Loss :0.003326042089611292\n",
      "Epoch :0.4625    Train Loss :0.0007830986869521439    Test Loss :0.0031163173262029886\n",
      "Epoch :0.475    Train Loss :0.000789336278103292    Test Loss :0.0030308689456433058\n",
      "Epoch :0.4875    Train Loss :0.0007393417181447148    Test Loss :0.002759717172011733\n",
      "Epoch :0.5    Train Loss :0.0008447707514278591    Test Loss :0.0037554858718067408\n",
      "Epoch :0.5125    Train Loss :0.001073983614332974    Test Loss :0.006097760517150164\n",
      "Epoch :0.525    Train Loss :0.001083567040041089    Test Loss :0.003533612471073866\n",
      "Epoch :0.5375    Train Loss :0.0010188637534156442    Test Loss :0.0027521499432623386\n",
      "Epoch :0.55    Train Loss :0.0008588020573370159    Test Loss :0.0028931964188814163\n",
      "Epoch :0.5625    Train Loss :0.0008128296467475593    Test Loss :0.0035211278591305017\n",
      "Epoch :0.575    Train Loss :0.0007582493708468974    Test Loss :0.0028659559320658445\n",
      "Epoch :0.5875    Train Loss :0.0007448517135344446    Test Loss :0.002954538445919752\n",
      "Epoch :0.6    Train Loss :0.0006836820975877345    Test Loss :0.0030992107931524515\n",
      "Epoch :0.6125    Train Loss :0.0006696890341117978    Test Loss :0.0027516153641045094\n",
      "Epoch :0.625    Train Loss :0.0006743133999407291    Test Loss :0.0028141357470303774\n",
      "Epoch :0.6375    Train Loss :0.0006733540794812143    Test Loss :0.0028848345391452312\n",
      "Epoch :0.65    Train Loss :0.0006780254188925028    Test Loss :0.002725177211686969\n",
      "Epoch :0.6625    Train Loss :0.0006691542221233249    Test Loss :0.0027276878245174885\n",
      "Epoch :0.675    Train Loss :0.0006641489453613758    Test Loss :0.0027875436935573816\n",
      "Epoch :0.6875    Train Loss :0.0006577131571248174    Test Loss :0.0025895079597830772\n",
      "Epoch :0.7    Train Loss :0.0006492820102721453    Test Loss :0.0027602300979197025\n",
      "Epoch :0.7125    Train Loss :0.0006470271619036794    Test Loss :0.0027491014916449785\n",
      "Epoch :0.725    Train Loss :0.0006108036614023149    Test Loss :0.002812074963003397\n",
      "Epoch :0.7375    Train Loss :0.0006342084961943328    Test Loss :0.0027356480713933706\n",
      "Epoch :0.75    Train Loss :0.0006678110221400857    Test Loss :0.0026965998113155365\n",
      "Epoch :0.7625    Train Loss :0.0006143971113488078    Test Loss :0.002574525075033307\n",
      "Epoch :0.775    Train Loss :0.0006094607524573803    Test Loss :0.002647264627739787\n",
      "Epoch :0.7875    Train Loss :0.0009445651085115969    Test Loss :0.003836017334833741\n",
      "Epoch :0.8    Train Loss :0.0007166112773120403    Test Loss :0.002921321429312229\n",
      "Epoch :0.8125    Train Loss :0.0006752364570274949    Test Loss :0.0024856848176568747\n",
      "Epoch :0.825    Train Loss :0.0005967458127997816    Test Loss :0.0029322258196771145\n",
      "Epoch :0.8375    Train Loss :0.0006286152638494968    Test Loss :0.0027028105687350035\n",
      "Epoch :0.85    Train Loss :0.0008344780071638525    Test Loss :0.0029793954454362392\n",
      "Epoch :0.8625    Train Loss :0.000764671538490802    Test Loss :0.0036587254144251347\n",
      "Epoch :0.875    Train Loss :0.0006723382975906134    Test Loss :0.002783216070383787\n",
      "Epoch :0.8875    Train Loss :0.0006263156537897885    Test Loss :0.00279727834276855\n",
      "Epoch :0.9    Train Loss :0.0006238067289814353    Test Loss :0.0026817945763468742\n",
      "Epoch :0.9125    Train Loss :0.0006254642503336072    Test Loss :0.0025312318466603756\n",
      "Epoch :0.925    Train Loss :0.0005593420355580747    Test Loss :0.0026381853967905045\n",
      "Epoch :0.9375    Train Loss :0.0005638980073854327    Test Loss :0.0026787705719470978\n",
      "Epoch :0.95    Train Loss :0.000570272037293762    Test Loss :0.0027179133612662554\n",
      "Epoch :0.9625    Train Loss :0.0005598716088570654    Test Loss :0.002767805475741625\n",
      "Epoch :0.975    Train Loss :0.0005347427795641124    Test Loss :0.002553520491346717\n",
      "Epoch :0.9875    Train Loss :0.0005339250201359391    Test Loss :0.0023565557785332203\n",
      "Epoch :1.0    Train Loss :0.0005526029854081571    Test Loss :0.002580083906650543\n",
      "RMSE: 36.27079934032405\n",
      "MAE: 26.013670879822623\n",
      "MAPE: 22.76574411982086%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 400, 'hid_size': 50, 'lr': 0.01, 'num_layers': 5}\n",
      "total:  56.00000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.04088304191827774    Test Loss :0.1659567952156067\n",
      "Epoch :0.025    Train Loss :0.03286959230899811    Test Loss :0.2012917697429657\n",
      "Epoch :0.0375    Train Loss :0.005560572259128094    Test Loss :0.044014785438776016\n",
      "Epoch :0.05    Train Loss :0.006094480864703655    Test Loss :0.03063286282122135\n",
      "Epoch :0.0625    Train Loss :0.0024230077397078276    Test Loss :0.013163429684937\n",
      "Epoch :0.075    Train Loss :0.0037551415152847767    Test Loss :0.016421565786004066\n",
      "Epoch :0.0875    Train Loss :0.002258873078972101    Test Loss :0.011138501577079296\n",
      "Epoch :0.1    Train Loss :0.001623945077881217    Test Loss :0.005384099204093218\n",
      "Epoch :0.1125    Train Loss :0.0017095553921535611    Test Loss :0.010989117436110973\n",
      "Epoch :0.125    Train Loss :0.0014388752169907093    Test Loss :0.004705647937953472\n",
      "Epoch :0.1375    Train Loss :0.001288196537643671    Test Loss :0.004711475223302841\n",
      "Epoch :0.15    Train Loss :0.0012200191849842668    Test Loss :0.005298594478517771\n",
      "Epoch :0.1625    Train Loss :0.0010634437203407288    Test Loss :0.005094311200082302\n",
      "Epoch :0.175    Train Loss :0.001055347383953631    Test Loss :0.004252972546964884\n",
      "Epoch :0.1875    Train Loss :0.0009576230659149587    Test Loss :0.004055618308484554\n",
      "Epoch :0.2    Train Loss :0.0009490619995631278    Test Loss :0.004025704693049192\n",
      "Epoch :0.2125    Train Loss :0.0008697838638909161    Test Loss :0.004063417203724384\n",
      "Epoch :0.225    Train Loss :0.0008600869914516807    Test Loss :0.003491844516247511\n",
      "Epoch :0.2375    Train Loss :0.0008252828265540302    Test Loss :0.003480480285361409\n",
      "Epoch :0.25    Train Loss :0.0007476095925085247    Test Loss :0.0032899680081754923\n",
      "Epoch :0.2625    Train Loss :0.0007452898425981402    Test Loss :0.0032987180165946484\n",
      "Epoch :0.275    Train Loss :0.0007603829726576805    Test Loss :0.0034337190445512533\n",
      "Epoch :0.2875    Train Loss :0.0006830320926383138    Test Loss :0.0031090073753148317\n",
      "Epoch :0.3    Train Loss :0.0006804533768445253    Test Loss :0.0025483157951384783\n",
      "Epoch :0.3125    Train Loss :0.0007054047891870141    Test Loss :0.0025593643076717854\n",
      "Epoch :0.325    Train Loss :0.0006447933847084641    Test Loss :0.0027971267700195312\n",
      "Epoch :0.3375    Train Loss :0.0006256776978261769    Test Loss :0.002338257385417819\n",
      "Epoch :0.35    Train Loss :0.000613310607150197    Test Loss :0.002194269560277462\n",
      "Epoch :0.3625    Train Loss :0.0006128150853328407    Test Loss :0.0021584394853562117\n",
      "Epoch :0.375    Train Loss :0.000574544770643115    Test Loss :0.002056263154372573\n",
      "Epoch :0.3875    Train Loss :0.0005860733799636364    Test Loss :0.002025754889473319\n",
      "Epoch :0.4    Train Loss :0.000568968360312283    Test Loss :0.0021566227078437805\n",
      "Epoch :0.4125    Train Loss :0.00062881491612643    Test Loss :0.0018373976927250624\n",
      "Epoch :0.425    Train Loss :0.000593597418628633    Test Loss :0.001895604538731277\n",
      "Epoch :0.4375    Train Loss :0.000559035106562078    Test Loss :0.001787014422006905\n",
      "Epoch :0.45    Train Loss :0.000535573170054704    Test Loss :0.0018037903355434537\n",
      "Epoch :0.4625    Train Loss :0.0005370156140998006    Test Loss :0.0018091783858835697\n",
      "Epoch :0.475    Train Loss :0.003039803821593523    Test Loss :0.0031321297865360975\n",
      "Epoch :0.4875    Train Loss :0.0006505996570922434    Test Loss :0.004651085007935762\n",
      "Epoch :0.5    Train Loss :0.001092789345420897    Test Loss :0.0024483483284711838\n",
      "Epoch :0.5125    Train Loss :0.0006683512474410236    Test Loss :0.002331233350560069\n",
      "Epoch :0.525    Train Loss :0.0006343013374134898    Test Loss :0.002631966955959797\n",
      "Epoch :0.5375    Train Loss :0.0006749739986844361    Test Loss :0.002215326065197587\n",
      "Epoch :0.55    Train Loss :0.0006257276982069016    Test Loss :0.0022113171871751547\n",
      "Epoch :0.5625    Train Loss :0.0005652282852679491    Test Loss :0.00213908520527184\n",
      "Epoch :0.575    Train Loss :0.0005577457486651838    Test Loss :0.0019759354181587696\n",
      "Epoch :0.5875    Train Loss :0.0005614054389297962    Test Loss :0.0017751367995515466\n",
      "Epoch :0.6    Train Loss :0.0005187832866795361    Test Loss :0.001617331407032907\n",
      "Epoch :0.6125    Train Loss :0.0005219473969191313    Test Loss :0.001614707289263606\n",
      "Epoch :0.625    Train Loss :0.0005113810766488314    Test Loss :0.0016790389781817794\n",
      "Epoch :0.6375    Train Loss :0.0004889349220320582    Test Loss :0.0014692200347781181\n",
      "Epoch :0.65    Train Loss :0.000462986616184935    Test Loss :0.0013278742553666234\n",
      "Epoch :0.6625    Train Loss :0.0004587567818816751    Test Loss :0.0013654386857524514\n",
      "Epoch :0.675    Train Loss :0.0004354871343821287    Test Loss :0.0012635489692911506\n",
      "Epoch :0.6875    Train Loss :0.00045638252049684525    Test Loss :0.0011801623040810227\n",
      "Epoch :0.7    Train Loss :0.00043921239557676017    Test Loss :0.0012131028342992067\n",
      "Epoch :0.7125    Train Loss :0.00041550598689354956    Test Loss :0.0011937097879126668\n",
      "Epoch :0.725    Train Loss :0.00042941822903230786    Test Loss :0.0011172582162544131\n",
      "Epoch :0.7375    Train Loss :0.00041055577457882464    Test Loss :0.0011724558426067233\n",
      "Epoch :0.75    Train Loss :0.0004338202124927193    Test Loss :0.001168418675661087\n",
      "Epoch :0.7625    Train Loss :0.0004064076056238264    Test Loss :0.001102348556742072\n",
      "Epoch :0.775    Train Loss :0.0008665169007144868    Test Loss :0.004312450066208839\n",
      "Epoch :0.7875    Train Loss :0.000810294586699456    Test Loss :0.002757799345999956\n",
      "Epoch :0.8    Train Loss :0.0005049065803177655    Test Loss :0.0018697499763220549\n",
      "Epoch :0.8125    Train Loss :0.0005038714734837413    Test Loss :0.0017316454323008657\n",
      "Epoch :0.825    Train Loss :0.00048302829964086413    Test Loss :0.0013308150228112936\n",
      "Epoch :0.8375    Train Loss :0.00043354061199352145    Test Loss :0.001340598100796342\n",
      "Epoch :0.85    Train Loss :0.00040821998845785856    Test Loss :0.0013529144925996661\n",
      "Epoch :0.8625    Train Loss :0.0003907966602127999    Test Loss :0.0015035878168419003\n",
      "Epoch :0.875    Train Loss :0.0004049572453368455    Test Loss :0.001182500272989273\n",
      "Epoch :0.8875    Train Loss :0.00039979099528864026    Test Loss :0.0011669083032757044\n",
      "Epoch :0.9    Train Loss :0.00039760483196005225    Test Loss :0.001378002343699336\n",
      "Epoch :0.9125    Train Loss :0.00036466712481342256    Test Loss :0.0014044307172298431\n",
      "Epoch :0.925    Train Loss :0.00035996202495880425    Test Loss :0.001227905391715467\n",
      "Epoch :0.9375    Train Loss :0.00041311007225885987    Test Loss :0.0012375727528706193\n",
      "Epoch :0.95    Train Loss :0.00043268740409985185    Test Loss :0.0014876879286020994\n",
      "Epoch :0.9625    Train Loss :0.00037885914207436144    Test Loss :0.0011633886024355888\n",
      "Epoch :0.975    Train Loss :0.0003461067099124193    Test Loss :0.0011544505832716823\n",
      "Epoch :0.9875    Train Loss :0.0007655415683984756    Test Loss :0.0030327751301229\n",
      "Epoch :1.0    Train Loss :0.00048619098379276693    Test Loss :0.0017541467677801847\n",
      "RMSE: 52.670236006795555\n",
      "MAE: 49.37699677562647\n",
      "MAPE: 43.79191715189698%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 400, 'hid_size': 50, 'lr': 0.02, 'num_layers': 2}\n",
      "total:  59.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.050182946026325226    Test Loss :0.2034674733877182\n",
      "Epoch :0.025    Train Loss :0.04329756647348404    Test Loss :0.19376234710216522\n",
      "Epoch :0.0375    Train Loss :0.041207991540431976    Test Loss :0.23403041064739227\n",
      "Epoch :0.05    Train Loss :0.04089495912194252    Test Loss :0.20643234252929688\n",
      "Epoch :0.0625    Train Loss :0.04082106426358223    Test Loss :0.22840218245983124\n",
      "Epoch :0.075    Train Loss :0.040511354804039    Test Loss :0.2106393426656723\n",
      "Epoch :0.0875    Train Loss :0.040537796914577484    Test Loss :0.22439588606357574\n",
      "Epoch :0.1    Train Loss :0.04045514017343521    Test Loss :0.21491585671901703\n",
      "Epoch :0.1125    Train Loss :0.0404893197119236    Test Loss :0.2203672230243683\n",
      "Epoch :0.125    Train Loss :0.04051835834980011    Test Loss :0.21725352108478546\n",
      "Epoch :0.1375    Train Loss :0.040445830672979355    Test Loss :0.2188543826341629\n",
      "Epoch :0.15    Train Loss :0.04053300619125366    Test Loss :0.21862123906612396\n",
      "Epoch :0.1625    Train Loss :0.04050321877002716    Test Loss :0.2184298038482666\n",
      "Epoch :0.175    Train Loss :0.040475063025951385    Test Loss :0.218604177236557\n",
      "Epoch :0.1875    Train Loss :0.04049193114042282    Test Loss :0.21828031539916992\n",
      "Epoch :0.2    Train Loss :0.04047844186425209    Test Loss :0.21832838654518127\n",
      "Epoch :0.2125    Train Loss :0.04052017629146576    Test Loss :0.21865668892860413\n",
      "Epoch :0.225    Train Loss :0.04050575941801071    Test Loss :0.21816901862621307\n",
      "Epoch :0.2375    Train Loss :0.0404512919485569    Test Loss :0.21842554211616516\n",
      "Epoch :0.25    Train Loss :0.04047856107354164    Test Loss :0.21837866306304932\n",
      "Epoch :0.2625    Train Loss :0.04051169008016586    Test Loss :0.21848873794078827\n",
      "Epoch :0.275    Train Loss :0.04048154503107071    Test Loss :0.2186952531337738\n",
      "Epoch :0.2875    Train Loss :0.04043086618185043    Test Loss :0.2180807739496231\n",
      "Epoch :0.3    Train Loss :0.04046272858977318    Test Loss :0.21854622662067413\n",
      "Epoch :0.3125    Train Loss :0.040451884269714355    Test Loss :0.21799243986606598\n",
      "Epoch :0.325    Train Loss :0.04042896628379822    Test Loss :0.21805168688297272\n",
      "Epoch :0.3375    Train Loss :0.04024611413478851    Test Loss :0.21683086454868317\n",
      "Epoch :0.35    Train Loss :0.03959398344159126    Test Loss :0.20975454151630402\n",
      "Epoch :0.3625    Train Loss :0.03919411823153496    Test Loss :0.12946641445159912\n",
      "Epoch :0.375    Train Loss :0.04855191335082054    Test Loss :0.2736426293849945\n",
      "Epoch :0.3875    Train Loss :0.043659698218107224    Test Loss :0.1795942485332489\n",
      "Epoch :0.4    Train Loss :0.04176054522395134    Test Loss :0.25692933797836304\n",
      "Epoch :0.4125    Train Loss :0.04053415358066559    Test Loss :0.2034415602684021\n",
      "Epoch :0.425    Train Loss :0.040682509541511536    Test Loss :0.21478304266929626\n",
      "Epoch :0.4375    Train Loss :0.040850043296813965    Test Loss :0.23307712376117706\n",
      "Epoch :0.45    Train Loss :0.040613748133182526    Test Loss :0.20723310112953186\n",
      "Epoch :0.4625    Train Loss :0.040501270443201065    Test Loss :0.2216452956199646\n",
      "Epoch :0.475    Train Loss :0.040535394102334976    Test Loss :0.22196851670742035\n",
      "Epoch :0.4875    Train Loss :0.04051566496491432    Test Loss :0.2129240483045578\n",
      "Epoch :0.5    Train Loss :0.04047764837741852    Test Loss :0.22177526354789734\n",
      "Epoch :0.5125    Train Loss :0.04045497626066208    Test Loss :0.2181902378797531\n",
      "Epoch :0.525    Train Loss :0.04044723138213158    Test Loss :0.2165888249874115\n",
      "Epoch :0.5375    Train Loss :0.04042262211441994    Test Loss :0.22011171281337738\n",
      "Epoch :0.55    Train Loss :0.04047153517603874    Test Loss :0.21737155318260193\n",
      "Epoch :0.5625    Train Loss :0.04046272858977318    Test Loss :0.2183021605014801\n",
      "Epoch :0.575    Train Loss :0.040465373545885086    Test Loss :0.21887876093387604\n",
      "Epoch :0.5875    Train Loss :0.04047373682260513    Test Loss :0.21757632493972778\n",
      "Epoch :0.6    Train Loss :0.0404524989426136    Test Loss :0.21879695355892181\n",
      "Epoch :0.6125    Train Loss :0.04046221077442169    Test Loss :0.2183319628238678\n",
      "Epoch :0.625    Train Loss :0.04048383980989456    Test Loss :0.21811546385288239\n",
      "Epoch :0.6375    Train Loss :0.04045802354812622    Test Loss :0.21849916875362396\n",
      "Epoch :0.65    Train Loss :0.04043228551745415    Test Loss :0.218179389834404\n",
      "Epoch :0.6625    Train Loss :0.04044506326317787    Test Loss :0.2184908092021942\n",
      "Epoch :0.675    Train Loss :0.0404697060585022    Test Loss :0.21840737760066986\n",
      "Epoch :0.6875    Train Loss :0.0404689759016037    Test Loss :0.21821776032447815\n",
      "Epoch :0.7    Train Loss :0.04047020524740219    Test Loss :0.21840143203735352\n",
      "Epoch :0.7125    Train Loss :0.04046713933348656    Test Loss :0.21836811304092407\n",
      "Epoch :0.725    Train Loss :0.04046018421649933    Test Loss :0.21844132244586945\n",
      "Epoch :0.7375    Train Loss :0.04047893360257149    Test Loss :0.21842482686042786\n",
      "Epoch :0.75    Train Loss :0.04044356569647789    Test Loss :0.21842801570892334\n",
      "Epoch :0.7625    Train Loss :0.040479689836502075    Test Loss :0.21840250492095947\n",
      "Epoch :0.775    Train Loss :0.0404554046690464    Test Loss :0.21844051778316498\n",
      "Epoch :0.7875    Train Loss :0.040464580059051514    Test Loss :0.21833866834640503\n",
      "Epoch :0.8    Train Loss :0.040464337915182114    Test Loss :0.2183779776096344\n",
      "Epoch :0.8125    Train Loss :0.0404711477458477    Test Loss :0.21845419704914093\n",
      "Epoch :0.825    Train Loss :0.04046101123094559    Test Loss :0.21818171441555023\n",
      "Epoch :0.8375    Train Loss :0.04046149179339409    Test Loss :0.21847479045391083\n",
      "Epoch :0.85    Train Loss :0.04044453427195549    Test Loss :0.21832671761512756\n",
      "Epoch :0.8625    Train Loss :0.04047046974301338    Test Loss :0.21858392655849457\n",
      "Epoch :0.875    Train Loss :0.040439486503601074    Test Loss :0.2183193415403366\n",
      "Epoch :0.8875    Train Loss :0.04046875983476639    Test Loss :0.2184467613697052\n",
      "Epoch :0.9    Train Loss :0.04045981168746948    Test Loss :0.21841861307621002\n",
      "Epoch :0.9125    Train Loss :0.04047849029302597    Test Loss :0.21837803721427917\n",
      "Epoch :0.925    Train Loss :0.04047263786196709    Test Loss :0.21833448112010956\n",
      "Epoch :0.9375    Train Loss :0.040463779121637344    Test Loss :0.21848277747631073\n",
      "Epoch :0.95    Train Loss :0.04045557975769043    Test Loss :0.2184057980775833\n",
      "Epoch :0.9625    Train Loss :0.040436793118715286    Test Loss :0.21840323507785797\n",
      "Epoch :0.975    Train Loss :0.04045625403523445    Test Loss :0.21834461390972137\n",
      "Epoch :0.9875    Train Loss :0.04049862548708916    Test Loss :0.21832841634750366\n",
      "Epoch :1.0    Train Loss :0.04047426953911781    Test Loss :0.218448668718338\n",
      "RMSE: 45.18875522757963\n",
      "MAE: 44.5023292707608\n",
      "MAPE: 38.99182546213559%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 400, 'hid_size': 50, 'lr': 0.02, 'num_layers': 5}\n",
      "total:  62.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.042265549302101135    Test Loss :0.18902836740016937\n",
      "Epoch :0.025    Train Loss :0.019914917647838593    Test Loss :0.05647759512066841\n",
      "Epoch :0.0375    Train Loss :0.005653519183397293    Test Loss :0.012325446121394634\n",
      "Epoch :0.05    Train Loss :0.006083248648792505    Test Loss :0.03449670225381851\n",
      "Epoch :0.0625    Train Loss :0.004274125210940838    Test Loss :0.006441543810069561\n",
      "Epoch :0.075    Train Loss :0.0022813924588263035    Test Loss :0.011294624768197536\n",
      "Epoch :0.0875    Train Loss :0.0018549702363088727    Test Loss :0.005258639343082905\n",
      "Epoch :0.1    Train Loss :0.0012390889460220933    Test Loss :0.006220568437129259\n",
      "Epoch :0.1125    Train Loss :0.0010032088030129671    Test Loss :0.0037617655470967293\n",
      "Epoch :0.125    Train Loss :0.001016398542560637    Test Loss :0.0036154158879071474\n",
      "Epoch :0.1375    Train Loss :0.0008032057667151093    Test Loss :0.0032093438785523176\n",
      "Epoch :0.15    Train Loss :0.0008651542593725026    Test Loss :0.002787194214761257\n",
      "Epoch :0.1625    Train Loss :0.0007420741603709757    Test Loss :0.0027640042826533318\n",
      "Epoch :0.175    Train Loss :0.0007256464450620115    Test Loss :0.003069712547585368\n",
      "Epoch :0.1875    Train Loss :0.0007173680933192372    Test Loss :0.0029165525920689106\n",
      "Epoch :0.2    Train Loss :0.0006585513474419713    Test Loss :0.0025852094404399395\n",
      "Epoch :0.2125    Train Loss :0.0006529869278892875    Test Loss :0.0023225450422614813\n",
      "Epoch :0.225    Train Loss :0.0006293733022175729    Test Loss :0.0023366466630250216\n",
      "Epoch :0.2375    Train Loss :0.0005982855800539255    Test Loss :0.002202014671638608\n",
      "Epoch :0.25    Train Loss :0.0006080257589928806    Test Loss :0.002210519975051284\n",
      "Epoch :0.2625    Train Loss :0.0005784290842711926    Test Loss :0.0021005088929086924\n",
      "Epoch :0.275    Train Loss :0.000579774787183851    Test Loss :0.0020501017570495605\n",
      "Epoch :0.2875    Train Loss :0.0005617510760203004    Test Loss :0.0018029323546215892\n",
      "Epoch :0.3    Train Loss :0.0005345443496480584    Test Loss :0.0019784991163760424\n",
      "Epoch :0.3125    Train Loss :0.0005251704715192318    Test Loss :0.001905073062516749\n",
      "Epoch :0.325    Train Loss :0.0005178002174943686    Test Loss :0.0016696207458153367\n",
      "Epoch :0.3375    Train Loss :0.0005084334989078343    Test Loss :0.0016546213300898671\n",
      "Epoch :0.35    Train Loss :0.00047795046702958643    Test Loss :0.001763520180247724\n",
      "Epoch :0.3625    Train Loss :0.0004778802103828639    Test Loss :0.0015412885695695877\n",
      "Epoch :0.375    Train Loss :0.0004485736135393381    Test Loss :0.0014731807168573141\n",
      "Epoch :0.3875    Train Loss :0.00045389647129923105    Test Loss :0.001365695963613689\n",
      "Epoch :0.4    Train Loss :0.0004415569710545242    Test Loss :0.0014867059653624892\n",
      "Epoch :0.4125    Train Loss :0.00042122259037569165    Test Loss :0.001624893513508141\n",
      "Epoch :0.425    Train Loss :0.00040777630056254566    Test Loss :0.0015328398440033197\n",
      "Epoch :0.4375    Train Loss :0.0004085575928911567    Test Loss :0.0012865735916420817\n",
      "Epoch :0.45    Train Loss :0.00044573910417966545    Test Loss :0.001228348002769053\n",
      "Epoch :0.4625    Train Loss :0.0005996579420752823    Test Loss :0.002378848847001791\n",
      "Epoch :0.475    Train Loss :0.0004559658409561962    Test Loss :0.0018391793128103018\n",
      "Epoch :0.4875    Train Loss :0.00044446688843891025    Test Loss :0.0014592221705242991\n",
      "Epoch :0.5    Train Loss :0.00040155573515221477    Test Loss :0.0012393224751576781\n",
      "Epoch :0.5125    Train Loss :0.000407166953664273    Test Loss :0.0012588280951604247\n",
      "Epoch :0.525    Train Loss :0.00039366204873658717    Test Loss :0.0012854434316977859\n",
      "Epoch :0.5375    Train Loss :0.0003900114097632468    Test Loss :0.0012246918631717563\n",
      "Epoch :0.55    Train Loss :0.0012118890881538391    Test Loss :0.006603349465876818\n",
      "Epoch :0.5625    Train Loss :0.001036078785546124    Test Loss :0.0022739137057214975\n",
      "Epoch :0.575    Train Loss :0.0006774511421099305    Test Loss :0.0018112673424184322\n",
      "Epoch :0.5875    Train Loss :0.0004241388523951173    Test Loss :0.0013845491921529174\n",
      "Epoch :0.6    Train Loss :0.0004914140445180237    Test Loss :0.0016493838047608733\n",
      "Epoch :0.6125    Train Loss :0.00043163917143829167    Test Loss :0.0017027693102136254\n",
      "Epoch :0.625    Train Loss :0.00042268302058801055    Test Loss :0.0013521286891773343\n",
      "Epoch :0.6375    Train Loss :0.00040042269392870367    Test Loss :0.001268803607672453\n",
      "Epoch :0.65    Train Loss :0.00036831258330494165    Test Loss :0.0013412975240498781\n",
      "Epoch :0.6625    Train Loss :0.00037796312244609    Test Loss :0.0012948454823344946\n",
      "Epoch :0.675    Train Loss :0.00036874908255413175    Test Loss :0.0011367209954187274\n",
      "Epoch :0.6875    Train Loss :0.00034128903644159436    Test Loss :0.001057908171787858\n",
      "Epoch :0.7    Train Loss :0.0003682661917991936    Test Loss :0.001023830147460103\n",
      "Epoch :0.7125    Train Loss :0.00034834351390600204    Test Loss :0.0009693003376014531\n",
      "Epoch :0.725    Train Loss :0.0003585138183552772    Test Loss :0.0009105101344175637\n",
      "Epoch :0.7375    Train Loss :0.0003792052739299834    Test Loss :0.000980988028459251\n",
      "Epoch :0.75    Train Loss :0.0003269516455475241    Test Loss :0.0008933197823353112\n",
      "Epoch :0.7625    Train Loss :0.00036149390507489443    Test Loss :0.0010416314471513033\n",
      "Epoch :0.775    Train Loss :0.00033390894532203674    Test Loss :0.0008351941942237318\n",
      "Epoch :0.7875    Train Loss :0.0003230580477975309    Test Loss :0.0008220921736210585\n",
      "Epoch :0.8    Train Loss :0.0009411926148459315    Test Loss :0.0075968545861542225\n",
      "Epoch :0.8125    Train Loss :0.0011612905655056238    Test Loss :0.0034470006357878447\n",
      "Epoch :0.825    Train Loss :0.0009097818401642144    Test Loss :0.0012134002754464746\n",
      "Epoch :0.8375    Train Loss :0.0004263117734808475    Test Loss :0.0015817645471543074\n",
      "Epoch :0.85    Train Loss :0.000545077899005264    Test Loss :0.0015386370941996574\n",
      "Epoch :0.8625    Train Loss :0.00040454984991811216    Test Loss :0.0020318052265793085\n",
      "Epoch :0.875    Train Loss :0.0004071851435583085    Test Loss :0.0016898110043257475\n",
      "Epoch :0.8875    Train Loss :0.00039131002267822623    Test Loss :0.0012059452710673213\n",
      "Epoch :0.9    Train Loss :0.0003545854415278882    Test Loss :0.0011164241004735231\n",
      "Epoch :0.9125    Train Loss :0.00036038411781191826    Test Loss :0.0012043729657307267\n",
      "Epoch :0.925    Train Loss :0.0003671165613923222    Test Loss :0.0010429680114611983\n",
      "Epoch :0.9375    Train Loss :0.00034596925252117217    Test Loss :0.0009928083745762706\n",
      "Epoch :0.95    Train Loss :0.00031738250982016325    Test Loss :0.0010607985313981771\n",
      "Epoch :0.9625    Train Loss :0.00033149999217130244    Test Loss :0.0009158228058367968\n",
      "Epoch :0.975    Train Loss :0.0003076144203078002    Test Loss :0.0009225914836861193\n",
      "Epoch :0.9875    Train Loss :0.00030008397880010307    Test Loss :0.0007710792124271393\n",
      "Epoch :1.0    Train Loss :0.0003124649520032108    Test Loss :0.0007242257124744356\n",
      "RMSE: 10.758646195055125\n",
      "MAE: 9.128965805285842\n",
      "MAPE: 8.33830884843698%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 400, 'hid_size': 100, 'lr': 0.01, 'num_layers': 2}\n",
      "total:  66.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.04209526628255844    Test Loss :0.20381589233875275\n",
      "Epoch :0.025    Train Loss :0.02243347279727459    Test Loss :0.03596312925219536\n",
      "Epoch :0.0375    Train Loss :0.017227185890078545    Test Loss :0.040787458419799805\n",
      "Epoch :0.05    Train Loss :0.011785531416535378    Test Loss :0.01550261490046978\n",
      "Epoch :0.0625    Train Loss :0.007917973212897778    Test Loss :0.04399961605668068\n",
      "Epoch :0.075    Train Loss :0.005552527029067278    Test Loss :0.017237545922398567\n",
      "Epoch :0.0875    Train Loss :0.004335516132414341    Test Loss :0.029243595898151398\n",
      "Epoch :0.1    Train Loss :0.0034617322962731123    Test Loss :0.006572555750608444\n",
      "Epoch :0.1125    Train Loss :0.002122177742421627    Test Loss :0.005780504550784826\n",
      "Epoch :0.125    Train Loss :0.0024948373902589083    Test Loss :0.007923581637442112\n",
      "Epoch :0.1375    Train Loss :0.0018463677261024714    Test Loss :0.006769130937755108\n",
      "Epoch :0.15    Train Loss :0.001795386546291411    Test Loss :0.005384256597608328\n",
      "Epoch :0.1625    Train Loss :0.0015292261959984899    Test Loss :0.0052352226339280605\n",
      "Epoch :0.175    Train Loss :0.0015075564151629806    Test Loss :0.005071184132248163\n",
      "Epoch :0.1875    Train Loss :0.001369621604681015    Test Loss :0.004757726099342108\n",
      "Epoch :0.2    Train Loss :0.0012308057630434632    Test Loss :0.004264707211405039\n",
      "Epoch :0.2125    Train Loss :0.001223698491230607    Test Loss :0.003955546300858259\n",
      "Epoch :0.225    Train Loss :0.0011279999744147062    Test Loss :0.0036968972999602556\n",
      "Epoch :0.2375    Train Loss :0.0011184203904122114    Test Loss :0.003301247488707304\n",
      "Epoch :0.25    Train Loss :0.0010904420632869005    Test Loss :0.0032690786756575108\n",
      "Epoch :0.2625    Train Loss :0.0010457189055159688    Test Loss :0.0033729986753314734\n",
      "Epoch :0.275    Train Loss :0.001093912753276527    Test Loss :0.003334929933771491\n",
      "Epoch :0.2875    Train Loss :0.001004786347039044    Test Loss :0.003335621440783143\n",
      "Epoch :0.3    Train Loss :0.0010701576247811317    Test Loss :0.0030346389394253492\n",
      "Epoch :0.3125    Train Loss :0.0010886454256251454    Test Loss :0.0028670597821474075\n",
      "Epoch :0.325    Train Loss :0.0009304929990321398    Test Loss :0.0028887817170470953\n",
      "Epoch :0.3375    Train Loss :0.0008171417866833508    Test Loss :0.002713755937293172\n",
      "Epoch :0.35    Train Loss :0.0008505533915013075    Test Loss :0.0029829542618244886\n",
      "Epoch :0.3625    Train Loss :0.0007813964039087296    Test Loss :0.002815171843394637\n",
      "Epoch :0.375    Train Loss :0.0025627349968999624    Test Loss :0.0042931376956403255\n",
      "Epoch :0.3875    Train Loss :0.0010859360918402672    Test Loss :0.006882688961923122\n",
      "Epoch :0.4    Train Loss :0.0009342616540379822    Test Loss :0.003319651586934924\n",
      "Epoch :0.4125    Train Loss :0.0009898401331156492    Test Loss :0.0033819086384028196\n",
      "Epoch :0.425    Train Loss :0.0008872718899510801    Test Loss :0.0033721302170306444\n",
      "Epoch :0.4375    Train Loss :0.0007671097409911454    Test Loss :0.00289630563929677\n",
      "Epoch :0.45    Train Loss :0.0007287046173587441    Test Loss :0.002720901509746909\n",
      "Epoch :0.4625    Train Loss :0.0006952069816179574    Test Loss :0.002407281193882227\n",
      "Epoch :0.475    Train Loss :0.0006812112987972796    Test Loss :0.0026156827807426453\n",
      "Epoch :0.4875    Train Loss :0.0006462095770984888    Test Loss :0.0027228884864598513\n",
      "Epoch :0.5    Train Loss :0.0006277969223447144    Test Loss :0.00244409148581326\n",
      "Epoch :0.5125    Train Loss :0.0006061755120754242    Test Loss :0.0024133012630045414\n",
      "Epoch :0.525    Train Loss :0.0006136005977168679    Test Loss :0.002718371106311679\n",
      "Epoch :0.5375    Train Loss :0.0013152849860489368    Test Loss :0.003767546033486724\n",
      "Epoch :0.55    Train Loss :0.0007392201805487275    Test Loss :0.002714125206694007\n",
      "Epoch :0.5625    Train Loss :0.0007634295034222305    Test Loss :0.0025061871856451035\n",
      "Epoch :0.575    Train Loss :0.0006285975687205791    Test Loss :0.002678678836673498\n",
      "Epoch :0.5875    Train Loss :0.0005637853173539042    Test Loss :0.0024350390303879976\n",
      "Epoch :0.6    Train Loss :0.0005366377299651504    Test Loss :0.002348010428249836\n",
      "Epoch :0.6125    Train Loss :0.000553176854737103    Test Loss :0.002544497139751911\n",
      "Epoch :0.625    Train Loss :0.0005297057796269655    Test Loss :0.0024075964465737343\n",
      "Epoch :0.6375    Train Loss :0.0007205117144621909    Test Loss :0.002723135519772768\n",
      "Epoch :0.65    Train Loss :0.0006936233839951456    Test Loss :0.002724169287830591\n",
      "Epoch :0.6625    Train Loss :0.0005454180645756423    Test Loss :0.002524301642552018\n",
      "Epoch :0.675    Train Loss :0.0005673986161127687    Test Loss :0.002604634966701269\n",
      "Epoch :0.6875    Train Loss :0.0005917721427977085    Test Loss :0.002624114044010639\n",
      "Epoch :0.7    Train Loss :0.0005580225843004882    Test Loss :0.0023996757809072733\n",
      "Epoch :0.7125    Train Loss :0.0004905819660052657    Test Loss :0.002236448461189866\n",
      "Epoch :0.725    Train Loss :0.0005112517392262816    Test Loss :0.0023540854454040527\n",
      "Epoch :0.7375    Train Loss :0.000950703164562583    Test Loss :0.0033747972920536995\n",
      "Epoch :0.75    Train Loss :0.0007154098711907864    Test Loss :0.002724660327658057\n",
      "Epoch :0.7625    Train Loss :0.0006067137001082301    Test Loss :0.002372168470174074\n",
      "Epoch :0.775    Train Loss :0.0005623509641736746    Test Loss :0.0023972538765519857\n",
      "Epoch :0.7875    Train Loss :0.0005077588721178472    Test Loss :0.002584416652098298\n",
      "Epoch :0.8    Train Loss :0.0005184449255466461    Test Loss :0.00221344199962914\n",
      "Epoch :0.8125    Train Loss :0.000507261254824698    Test Loss :0.002634790726006031\n",
      "Epoch :0.825    Train Loss :0.0004972965107299387    Test Loss :0.0024354993365705013\n",
      "Epoch :0.8375    Train Loss :0.0005407514981925488    Test Loss :0.0022737071849405766\n",
      "Epoch :0.85    Train Loss :0.0005062748678028584    Test Loss :0.0023318936582654715\n",
      "Epoch :0.8625    Train Loss :0.000443274067947641    Test Loss :0.002189410151913762\n",
      "Epoch :0.875    Train Loss :0.0005626104539260268    Test Loss :0.002539859851822257\n",
      "Epoch :0.8875    Train Loss :0.0004541049711406231    Test Loss :0.002536928281188011\n",
      "Epoch :0.9    Train Loss :0.0005320865893736482    Test Loss :0.002214198699221015\n",
      "Epoch :0.9125    Train Loss :0.0005262633203528821    Test Loss :0.002435342175886035\n",
      "Epoch :0.925    Train Loss :0.0006166240200400352    Test Loss :0.002513948129490018\n",
      "Epoch :0.9375    Train Loss :0.0004957709461450577    Test Loss :0.0023241015151143074\n",
      "Epoch :0.95    Train Loss :0.00046310664038173854    Test Loss :0.0020724369678646326\n",
      "Epoch :0.9625    Train Loss :0.000453381595434621    Test Loss :0.002110036090016365\n",
      "Epoch :0.975    Train Loss :0.0004653184150811285    Test Loss :0.002291938988491893\n",
      "Epoch :0.9875    Train Loss :0.0005092595238238573    Test Loss :0.002494078828021884\n",
      "Epoch :1.0    Train Loss :0.0005581016885116696    Test Loss :0.0023074899800121784\n",
      "RMSE: 8.839505088383559\n",
      "MAE: 7.203613827347163\n",
      "MAPE: 6.56031593176572%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 400, 'hid_size': 100, 'lr': 0.01, 'num_layers': 5}\n",
      "total:  69.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.16980305314064026    Test Loss :0.5319293737411499\n",
      "Epoch :0.025    Train Loss :0.05425114184617996    Test Loss :0.35983479022979736\n",
      "Epoch :0.0375    Train Loss :0.03632574900984764    Test Loss :0.21153785288333893\n",
      "Epoch :0.05    Train Loss :0.031246915459632874    Test Loss :0.09589868783950806\n",
      "Epoch :0.0625    Train Loss :0.01509072259068489    Test Loss :0.059565819799900055\n",
      "Epoch :0.075    Train Loss :0.006548861972987652    Test Loss :0.02056220918893814\n",
      "Epoch :0.0875    Train Loss :0.004826498683542013    Test Loss :0.017078807577490807\n",
      "Epoch :0.1    Train Loss :0.0021783581469208    Test Loss :0.006937940139323473\n",
      "Epoch :0.1125    Train Loss :0.00216201925650239    Test Loss :0.004129487089812756\n",
      "Epoch :0.125    Train Loss :0.001818447490222752    Test Loss :0.00438538147136569\n",
      "Epoch :0.1375    Train Loss :0.0015575289726257324    Test Loss :0.007893679663538933\n",
      "Epoch :0.15    Train Loss :0.0012965122004970908    Test Loss :0.003953887615352869\n",
      "Epoch :0.1625    Train Loss :0.001247211592271924    Test Loss :0.0032059899531304836\n",
      "Epoch :0.175    Train Loss :0.0011180072324350476    Test Loss :0.003322316100820899\n",
      "Epoch :0.1875    Train Loss :0.0011088467435911298    Test Loss :0.003643101081252098\n",
      "Epoch :0.2    Train Loss :0.0010919066844508052    Test Loss :0.0032042015809565783\n",
      "Epoch :0.2125    Train Loss :0.000970476889051497    Test Loss :0.003147430717945099\n",
      "Epoch :0.225    Train Loss :0.0010148113360628486    Test Loss :0.002940493868663907\n",
      "Epoch :0.2375    Train Loss :0.0009852766525000334    Test Loss :0.0030395567882806063\n",
      "Epoch :0.25    Train Loss :0.0009320809622295201    Test Loss :0.002969411900267005\n",
      "Epoch :0.2625    Train Loss :0.0009037135750986636    Test Loss :0.0028527386020869017\n",
      "Epoch :0.275    Train Loss :0.000856340688187629    Test Loss :0.002793801948428154\n",
      "Epoch :0.2875    Train Loss :0.0008748433901928365    Test Loss :0.0027729819994419813\n",
      "Epoch :0.3    Train Loss :0.0008520409464836121    Test Loss :0.0028258797246962786\n",
      "Epoch :0.3125    Train Loss :0.0008562608854845166    Test Loss :0.0027687924448400736\n",
      "Epoch :0.325    Train Loss :0.0007826533983461559    Test Loss :0.0026002044323831797\n",
      "Epoch :0.3375    Train Loss :0.0007667854079045355    Test Loss :0.0026966126170009375\n",
      "Epoch :0.35    Train Loss :0.0007967699784785509    Test Loss :0.002502267947420478\n",
      "Epoch :0.3625    Train Loss :0.000770003825891763    Test Loss :0.0026914728805422783\n",
      "Epoch :0.375    Train Loss :0.00078012136509642    Test Loss :0.002586348680779338\n",
      "Epoch :0.3875    Train Loss :0.0007612366462126374    Test Loss :0.0027135421987622976\n",
      "Epoch :0.4    Train Loss :0.0007583397091366351    Test Loss :0.0024904333986341953\n",
      "Epoch :0.4125    Train Loss :0.0007466677343472838    Test Loss :0.0025489386171102524\n",
      "Epoch :0.425    Train Loss :0.0007095764158293605    Test Loss :0.002505545038729906\n",
      "Epoch :0.4375    Train Loss :0.0007351771928369999    Test Loss :0.0023846642579883337\n",
      "Epoch :0.45    Train Loss :0.0007229845505207777    Test Loss :0.002403023187071085\n",
      "Epoch :0.4625    Train Loss :0.000702262856066227    Test Loss :0.002404930302873254\n",
      "Epoch :0.475    Train Loss :0.0007244143052957952    Test Loss :0.0023720767349004745\n",
      "Epoch :0.4875    Train Loss :0.0007202085689641535    Test Loss :0.002377043478190899\n",
      "Epoch :0.5    Train Loss :0.0006861827569082379    Test Loss :0.002351105445995927\n",
      "Epoch :0.5125    Train Loss :0.0006789193721488118    Test Loss :0.0023501745890825987\n",
      "Epoch :0.525    Train Loss :0.0006880779983475804    Test Loss :0.002327712718397379\n",
      "Epoch :0.5375    Train Loss :0.000668466032948345    Test Loss :0.002317246748134494\n",
      "Epoch :0.55    Train Loss :0.000658049713820219    Test Loss :0.002486607525497675\n",
      "Epoch :0.5625    Train Loss :0.0006707942811772227    Test Loss :0.0024142644833773375\n",
      "Epoch :0.575    Train Loss :0.0006403803708963096    Test Loss :0.002357573714107275\n",
      "Epoch :0.5875    Train Loss :0.0006584226503036916    Test Loss :0.002221703063696623\n",
      "Epoch :0.6    Train Loss :0.0006388508481904864    Test Loss :0.002295676851645112\n",
      "Epoch :0.6125    Train Loss :0.0006268398137763143    Test Loss :0.0021883961744606495\n",
      "Epoch :0.625    Train Loss :0.0006190213607624173    Test Loss :0.002362931612879038\n",
      "Epoch :0.6375    Train Loss :0.0006525004282593727    Test Loss :0.00237905397079885\n",
      "Epoch :0.65    Train Loss :0.0006452209781855345    Test Loss :0.0022873126436024904\n",
      "Epoch :0.6625    Train Loss :0.0006322826375253499    Test Loss :0.0022376906126737595\n",
      "Epoch :0.675    Train Loss :0.0006279666558839381    Test Loss :0.002188503509387374\n",
      "Epoch :0.6875    Train Loss :0.0006028367788530886    Test Loss :0.002321729902178049\n",
      "Epoch :0.7    Train Loss :0.0005811796290799975    Test Loss :0.0021664355881512165\n",
      "Epoch :0.7125    Train Loss :0.0006099864258430898    Test Loss :0.0022709437180310488\n",
      "Epoch :0.725    Train Loss :0.0005924010765738785    Test Loss :0.0023966985754668713\n",
      "Epoch :0.7375    Train Loss :0.0005867459112778306    Test Loss :0.002261148067191243\n",
      "Epoch :0.75    Train Loss :0.0005844690022058785    Test Loss :0.002141515025869012\n",
      "Epoch :0.7625    Train Loss :0.0005655527929775417    Test Loss :0.0021345429122447968\n",
      "Epoch :0.775    Train Loss :0.0005875168135389686    Test Loss :0.0021245426032692194\n",
      "Epoch :0.7875    Train Loss :0.000594653538428247    Test Loss :0.0021139357704669237\n",
      "Epoch :0.8    Train Loss :0.0005614268011413515    Test Loss :0.002223441144451499\n",
      "Epoch :0.8125    Train Loss :0.0005844428087584674    Test Loss :0.002153953770175576\n",
      "Epoch :0.825    Train Loss :0.0005610455409623682    Test Loss :0.0022781649604439735\n",
      "Epoch :0.8375    Train Loss :0.0005467907176353037    Test Loss :0.002174608176574111\n",
      "Epoch :0.85    Train Loss :0.0005514207878150046    Test Loss :0.0022381755989044905\n",
      "Epoch :0.8625    Train Loss :0.0005542680155485868    Test Loss :0.002193942666053772\n",
      "Epoch :0.875    Train Loss :0.0005294449510984123    Test Loss :0.00220009358599782\n",
      "Epoch :0.8875    Train Loss :0.0005205987836234272    Test Loss :0.002141897799447179\n",
      "Epoch :0.9    Train Loss :0.0005376755725592375    Test Loss :0.00206787814386189\n",
      "Epoch :0.9125    Train Loss :0.0005309069529175758    Test Loss :0.0021766244899481535\n",
      "Epoch :0.925    Train Loss :0.0005395613261498511    Test Loss :0.0020579949487000704\n",
      "Epoch :0.9375    Train Loss :0.0005246962537057698    Test Loss :0.0021003319416195154\n",
      "Epoch :0.95    Train Loss :0.0005189276416786015    Test Loss :0.002097948919981718\n",
      "Epoch :0.9625    Train Loss :0.0005355111788958311    Test Loss :0.0020937942899763584\n",
      "Epoch :0.975    Train Loss :0.0004997537471354008    Test Loss :0.0021593375131487846\n",
      "Epoch :0.9875    Train Loss :0.0004966019187122583    Test Loss :0.002169710351154208\n",
      "Epoch :1.0    Train Loss :0.0005133375525474548    Test Loss :0.0020441561937332153\n",
      "RMSE: 8.507792206152088\n",
      "MAE: 6.9448718967540435\n",
      "MAPE: 6.2948590293040825%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 400, 'hid_size': 100, 'lr': 0.02, 'num_layers': 2}\n",
      "total:  72.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.20273251831531525    Test Loss :2.7154805660247803\n",
      "Epoch :0.025    Train Loss :0.04301157221198082    Test Loss :0.28197333216667175\n",
      "Epoch :0.0375    Train Loss :0.04589135944843292    Test Loss :0.1706390529870987\n",
      "Epoch :0.05    Train Loss :0.04165929555892944    Test Loss :0.19305439293384552\n",
      "Epoch :0.0625    Train Loss :0.04126403108239174    Test Loss :0.24755415320396423\n",
      "Epoch :0.075    Train Loss :0.04134455695748329    Test Loss :0.20931608974933624\n",
      "Epoch :0.0875    Train Loss :0.04079756885766983    Test Loss :0.21372312307357788\n",
      "Epoch :0.1    Train Loss :0.040650784969329834    Test Loss :0.22997765243053436\n",
      "Epoch :0.1125    Train Loss :0.04067739471793175    Test Loss :0.20994974672794342\n",
      "Epoch :0.125    Train Loss :0.040508370846509933    Test Loss :0.22262772917747498\n",
      "Epoch :0.1375    Train Loss :0.040705278515815735    Test Loss :0.21834547817707062\n",
      "Epoch :0.15    Train Loss :0.04054231196641922    Test Loss :0.21477316319942474\n",
      "Epoch :0.1625    Train Loss :0.04049077257514    Test Loss :0.22190089523792267\n",
      "Epoch :0.175    Train Loss :0.040623847395181656    Test Loss :0.216790109872818\n",
      "Epoch :0.1875    Train Loss :0.04052937775850296    Test Loss :0.21812865138053894\n",
      "Epoch :0.2    Train Loss :0.04054027050733566    Test Loss :0.2198367565870285\n",
      "Epoch :0.2125    Train Loss :0.040460698306560516    Test Loss :0.21762649714946747\n",
      "Epoch :0.225    Train Loss :0.040360577404499054    Test Loss :0.21870550513267517\n",
      "Epoch :0.2375    Train Loss :0.04058597609400749    Test Loss :0.21863801777362823\n",
      "Epoch :0.25    Train Loss :0.04052731394767761    Test Loss :0.21847796440124512\n",
      "Epoch :0.2625    Train Loss :0.0405176542699337    Test Loss :0.21879354119300842\n",
      "Epoch :0.275    Train Loss :0.040505360811948776    Test Loss :0.21825383603572845\n",
      "Epoch :0.2875    Train Loss :0.040484968572854996    Test Loss :0.21856682002544403\n",
      "Epoch :0.3    Train Loss :0.040497709065675735    Test Loss :0.21821348369121552\n",
      "Epoch :0.3125    Train Loss :0.04061330854892731    Test Loss :0.21770818531513214\n",
      "Epoch :0.325    Train Loss :0.04051734879612923    Test Loss :0.21872372925281525\n",
      "Epoch :0.3375    Train Loss :0.04054165259003639    Test Loss :0.21801985800266266\n",
      "Epoch :0.35    Train Loss :0.04048427566885948    Test Loss :0.21838203072547913\n",
      "Epoch :0.3625    Train Loss :0.04047722369432449    Test Loss :0.21829259395599365\n",
      "Epoch :0.375    Train Loss :0.04047562554478645    Test Loss :0.2185436338186264\n",
      "Epoch :0.3875    Train Loss :0.040485430508852005    Test Loss :0.21804872155189514\n",
      "Epoch :0.4    Train Loss :0.04048968851566315    Test Loss :0.21837399899959564\n",
      "Epoch :0.4125    Train Loss :0.04044032096862793    Test Loss :0.2183903455734253\n",
      "Epoch :0.425    Train Loss :0.040462251752614975    Test Loss :0.21858757734298706\n",
      "Epoch :0.4375    Train Loss :0.040433600544929504    Test Loss :0.21854878962039948\n",
      "Epoch :0.45    Train Loss :0.04047737270593643    Test Loss :0.21853391826152802\n",
      "Epoch :0.4625    Train Loss :0.04045158252120018    Test Loss :0.21827782690525055\n",
      "Epoch :0.475    Train Loss :0.040514349937438965    Test Loss :0.21835128962993622\n",
      "Epoch :0.4875    Train Loss :0.04048686474561691    Test Loss :0.21819457411766052\n",
      "Epoch :0.5    Train Loss :0.04053947702050209    Test Loss :0.2185361385345459\n",
      "Epoch :0.5125    Train Loss :0.04052416980266571    Test Loss :0.21835646033287048\n",
      "Epoch :0.525    Train Loss :0.04046911001205444    Test Loss :0.21827636659145355\n",
      "Epoch :0.5375    Train Loss :0.04051901027560234    Test Loss :0.21866030991077423\n",
      "Epoch :0.55    Train Loss :0.04049789160490036    Test Loss :0.21821118891239166\n",
      "Epoch :0.5625    Train Loss :0.04048654064536095    Test Loss :0.2184094339609146\n",
      "Epoch :0.575    Train Loss :0.04048158973455429    Test Loss :0.21881867945194244\n",
      "Epoch :0.5875    Train Loss :0.04043855890631676    Test Loss :0.21830785274505615\n",
      "Epoch :0.6    Train Loss :0.04042859002947807    Test Loss :0.2183104157447815\n",
      "Epoch :0.6125    Train Loss :0.040483709424734116    Test Loss :0.21857360005378723\n",
      "Epoch :0.625    Train Loss :0.04046642407774925    Test Loss :0.21854011714458466\n",
      "Epoch :0.6375    Train Loss :0.040508098900318146    Test Loss :0.2185140997171402\n",
      "Epoch :0.65    Train Loss :0.040477171540260315    Test Loss :0.21850956976413727\n",
      "Epoch :0.6625    Train Loss :0.04045034572482109    Test Loss :0.21831929683685303\n",
      "Epoch :0.675    Train Loss :0.04044099152088165    Test Loss :0.21827763319015503\n",
      "Epoch :0.6875    Train Loss :0.04045287147164345    Test Loss :0.21843485534191132\n",
      "Epoch :0.7    Train Loss :0.04048106446862221    Test Loss :0.2184375673532486\n",
      "Epoch :0.7125    Train Loss :0.04044798016548157    Test Loss :0.21826039254665375\n",
      "Epoch :0.725    Train Loss :0.04044802486896515    Test Loss :0.21837088465690613\n",
      "Epoch :0.7375    Train Loss :0.04042498767375946    Test Loss :0.21866284310817719\n",
      "Epoch :0.75    Train Loss :0.04047391936182976    Test Loss :0.21863286197185516\n",
      "Epoch :0.7625    Train Loss :0.04045967385172844    Test Loss :0.2185722440481186\n",
      "Epoch :0.775    Train Loss :0.04045161232352257    Test Loss :0.21829213201999664\n",
      "Epoch :0.7875    Train Loss :0.04043981805443764    Test Loss :0.21829108893871307\n",
      "Epoch :0.8    Train Loss :0.04048929363489151    Test Loss :0.2184552252292633\n",
      "Epoch :0.8125    Train Loss :0.04042680934071541    Test Loss :0.21835629642009735\n",
      "Epoch :0.825    Train Loss :0.040478453040122986    Test Loss :0.2181878387928009\n",
      "Epoch :0.8375    Train Loss :0.04044916108250618    Test Loss :0.21828874945640564\n",
      "Epoch :0.85    Train Loss :0.040440771728754044    Test Loss :0.21851100027561188\n",
      "Epoch :0.8625    Train Loss :0.04043370485305786    Test Loss :0.21851590275764465\n",
      "Epoch :0.875    Train Loss :0.040481120347976685    Test Loss :0.2182936668395996\n",
      "Epoch :0.8875    Train Loss :0.040464483201503754    Test Loss :0.2182433158159256\n",
      "Epoch :0.9    Train Loss :0.04044390097260475    Test Loss :0.21828147768974304\n",
      "Epoch :0.9125    Train Loss :0.04048598185181618    Test Loss :0.21805235743522644\n",
      "Epoch :0.925    Train Loss :0.040433626621961594    Test Loss :0.2183803915977478\n",
      "Epoch :0.9375    Train Loss :0.040453556925058365    Test Loss :0.2185220569372177\n",
      "Epoch :0.95    Train Loss :0.040474243462085724    Test Loss :0.21861621737480164\n",
      "Epoch :0.9625    Train Loss :0.040451012551784515    Test Loss :0.21874624490737915\n",
      "Epoch :0.975    Train Loss :0.040473029017448425    Test Loss :0.21844805777072906\n",
      "Epoch :0.9875    Train Loss :0.04047274589538574    Test Loss :0.21814946830272675\n",
      "Epoch :1.0    Train Loss :0.04048169404268265    Test Loss :0.21818022429943085\n",
      "RMSE: 45.16978272048797\n",
      "MAE: 44.48185135070841\n",
      "MAPE: 38.97347962351215%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 400, 'hid_size': 100, 'lr': 0.02, 'num_layers': 5}\n",
      "total:  75.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.036567557603120804    Test Loss :0.27184993028640747\n",
      "Epoch :0.016666666666666666    Train Loss :0.031075989827513695    Test Loss :0.10768205672502518\n",
      "Epoch :0.025    Train Loss :0.018343713134527206    Test Loss :0.09099344164133072\n",
      "Epoch :0.03333333333333333    Train Loss :0.008779001422226429    Test Loss :0.022467799484729767\n",
      "Epoch :0.041666666666666664    Train Loss :0.006139152683317661    Test Loss :0.0133065739646554\n",
      "Epoch :0.05    Train Loss :0.00541391596198082    Test Loss :0.027073422446846962\n",
      "Epoch :0.058333333333333334    Train Loss :0.003167299088090658    Test Loss :0.00767857488244772\n",
      "Epoch :0.06666666666666667    Train Loss :0.0017407818231731653    Test Loss :0.008255569264292717\n",
      "Epoch :0.075    Train Loss :0.0018781975377351046    Test Loss :0.013208327814936638\n",
      "Epoch :0.08333333333333333    Train Loss :0.0014946991577744484    Test Loss :0.004932294599711895\n",
      "Epoch :0.09166666666666666    Train Loss :0.001492991577833891    Test Loss :0.006752329412847757\n",
      "Epoch :0.1    Train Loss :0.0012976890429854393    Test Loss :0.006126076448708773\n",
      "Epoch :0.10833333333333334    Train Loss :0.0011567127658054233    Test Loss :0.0037440897431224585\n",
      "Epoch :0.11666666666666667    Train Loss :0.0011843821266666055    Test Loss :0.005292603280395269\n",
      "Epoch :0.125    Train Loss :0.0011220870073884726    Test Loss :0.0044978284277021885\n",
      "Epoch :0.13333333333333333    Train Loss :0.0010346566559746861    Test Loss :0.0038252270314842463\n",
      "Epoch :0.14166666666666666    Train Loss :0.0009733050828799605    Test Loss :0.004546474665403366\n",
      "Epoch :0.15    Train Loss :0.0009427478071302176    Test Loss :0.003963441587984562\n",
      "Epoch :0.15833333333333333    Train Loss :0.0009745278512127697    Test Loss :0.0038342701736837626\n",
      "Epoch :0.16666666666666666    Train Loss :0.0009321316028945148    Test Loss :0.003901571035385132\n",
      "Epoch :0.175    Train Loss :0.0009191762655973434    Test Loss :0.003623574273660779\n",
      "Epoch :0.18333333333333332    Train Loss :0.0008848609286360443    Test Loss :0.004077855963259935\n",
      "Epoch :0.19166666666666668    Train Loss :0.0008351446595042944    Test Loss :0.0034330510534346104\n",
      "Epoch :0.2    Train Loss :0.0008777069742791355    Test Loss :0.003224167972803116\n",
      "Epoch :0.20833333333333334    Train Loss :0.0008256634464487433    Test Loss :0.003632579930126667\n",
      "Epoch :0.21666666666666667    Train Loss :0.0008281514164991677    Test Loss :0.0035689028445631266\n",
      "Epoch :0.225    Train Loss :0.000837336468975991    Test Loss :0.0032759588211774826\n",
      "Epoch :0.23333333333333334    Train Loss :0.0008086711750365794    Test Loss :0.00319813285022974\n",
      "Epoch :0.24166666666666667    Train Loss :0.0007627353188581765    Test Loss :0.0031130709685385227\n",
      "Epoch :0.25    Train Loss :0.0007064058445394039    Test Loss :0.0028483879286795855\n",
      "Epoch :0.25833333333333336    Train Loss :0.0007530699949711561    Test Loss :0.002604189794510603\n",
      "Epoch :0.26666666666666666    Train Loss :0.0006851220969110727    Test Loss :0.0025918930768966675\n",
      "Epoch :0.275    Train Loss :0.0006861675065010786    Test Loss :0.0028066292870789766\n",
      "Epoch :0.2833333333333333    Train Loss :0.0007152752950787544    Test Loss :0.0026143912691622972\n",
      "Epoch :0.2916666666666667    Train Loss :0.0006976707372814417    Test Loss :0.0024105808697640896\n",
      "Epoch :0.3    Train Loss :0.0006640498177148402    Test Loss :0.0022948989644646645\n",
      "Epoch :0.30833333333333335    Train Loss :0.0006498636794276536    Test Loss :0.0023399062920361757\n",
      "Epoch :0.31666666666666665    Train Loss :0.0006337624508887529    Test Loss :0.002204627962782979\n",
      "Epoch :0.325    Train Loss :0.0006310354801826179    Test Loss :0.0021588453091681004\n",
      "Epoch :0.3333333333333333    Train Loss :0.0005863527185283601    Test Loss :0.0021812638733536005\n",
      "Epoch :0.3416666666666667    Train Loss :0.0005892537883482873    Test Loss :0.0019819848239421844\n",
      "Epoch :0.35    Train Loss :0.000565942085813731    Test Loss :0.001891297404654324\n",
      "Epoch :0.35833333333333334    Train Loss :0.0005605552578344941    Test Loss :0.0017967065796256065\n",
      "Epoch :0.36666666666666664    Train Loss :0.0005450176540762186    Test Loss :0.0017837461782619357\n",
      "Epoch :0.375    Train Loss :0.0005337960901670158    Test Loss :0.0019099084893241525\n",
      "Epoch :0.38333333333333336    Train Loss :0.0006947140209376812    Test Loss :0.0014582989970222116\n",
      "Epoch :0.39166666666666666    Train Loss :0.0005087862373329699    Test Loss :0.0014945750590413809\n",
      "Epoch :0.4    Train Loss :0.0005294466973282397    Test Loss :0.0016365438932552934\n",
      "Epoch :0.4083333333333333    Train Loss :0.0005251211114227772    Test Loss :0.0014227337669581175\n",
      "Epoch :0.4166666666666667    Train Loss :0.0004964037798345089    Test Loss :0.0017316860612481833\n",
      "Epoch :0.425    Train Loss :0.0005072752246633172    Test Loss :0.0017133475048467517\n",
      "Epoch :0.43333333333333335    Train Loss :0.0004911135183647275    Test Loss :0.0014797396725043654\n",
      "Epoch :0.44166666666666665    Train Loss :0.00045778226922266185    Test Loss :0.0013240425614640117\n",
      "Epoch :0.45    Train Loss :0.00045790348667651415    Test Loss :0.0012835871893912554\n",
      "Epoch :0.4583333333333333    Train Loss :0.0005226804642006755    Test Loss :0.0010912279831245542\n",
      "Epoch :0.4666666666666667    Train Loss :0.0005812138551846147    Test Loss :0.001304294797591865\n",
      "Epoch :0.475    Train Loss :0.00047002374776639044    Test Loss :0.0013043635990470648\n",
      "Epoch :0.48333333333333334    Train Loss :0.0004727455379907042    Test Loss :0.0011700121685862541\n",
      "Epoch :0.49166666666666664    Train Loss :0.00044882475049234927    Test Loss :0.001120383501984179\n",
      "Epoch :0.5    Train Loss :0.0004326526541262865    Test Loss :0.0011440068483352661\n",
      "Epoch :0.5083333333333333    Train Loss :0.00042748518171720207    Test Loss :0.0010809573577716947\n",
      "Epoch :0.5166666666666667    Train Loss :0.0004985572886653244    Test Loss :0.0012741698883473873\n",
      "Epoch :0.525    Train Loss :0.000440058734966442    Test Loss :0.0010783728212118149\n",
      "Epoch :0.5333333333333333    Train Loss :0.0004204366123303771    Test Loss :0.0010779680451378226\n",
      "Epoch :0.5416666666666666    Train Loss :0.00039943645242601633    Test Loss :0.0011092063505202532\n",
      "Epoch :0.55    Train Loss :0.0003970357938669622    Test Loss :0.0011933674104511738\n",
      "Epoch :0.5583333333333333    Train Loss :0.00038955232594162226    Test Loss :0.00106815027538687\n",
      "Epoch :0.5666666666666667    Train Loss :0.0004367367073427886    Test Loss :0.001045483979396522\n",
      "Epoch :0.575    Train Loss :0.00038118063821457326    Test Loss :0.0010307291522622108\n",
      "Epoch :0.5833333333333334    Train Loss :0.00036750585422851145    Test Loss :0.0009999261237680912\n",
      "Epoch :0.5916666666666667    Train Loss :0.0003942247130908072    Test Loss :0.001127431052736938\n",
      "Epoch :0.6    Train Loss :0.00044679080019705    Test Loss :0.0012823425931856036\n",
      "Epoch :0.6083333333333333    Train Loss :0.0006614670855924487    Test Loss :0.0011676365975290537\n",
      "Epoch :0.6166666666666667    Train Loss :0.0007278053089976311    Test Loss :0.0015383833087980747\n",
      "Epoch :0.625    Train Loss :0.0006160694756545126    Test Loss :0.0014481224352493882\n",
      "Epoch :0.6333333333333333    Train Loss :0.0004451708809938282    Test Loss :0.0013577599311247468\n",
      "Epoch :0.6416666666666667    Train Loss :0.00040881827590055764    Test Loss :0.0012695923214778304\n",
      "Epoch :0.65    Train Loss :0.0003976253210566938    Test Loss :0.0013989908620715141\n",
      "Epoch :0.6583333333333333    Train Loss :0.00037405118928290904    Test Loss :0.0011080418480560184\n",
      "Epoch :0.6666666666666666    Train Loss :0.00040559208719059825    Test Loss :0.0011517585953697562\n",
      "Epoch :0.675    Train Loss :0.000354410003637895    Test Loss :0.001249143504537642\n",
      "Epoch :0.6833333333333333    Train Loss :0.0003610346175264567    Test Loss :0.0010800979798659682\n",
      "Epoch :0.6916666666666667    Train Loss :0.0003564044600352645    Test Loss :0.0010189623571932316\n",
      "Epoch :0.7    Train Loss :0.00036839404492639005    Test Loss :0.0010981843806803226\n",
      "Epoch :0.7083333333333334    Train Loss :0.0003693457110784948    Test Loss :0.0010922130895778537\n",
      "Epoch :0.7166666666666667    Train Loss :0.00035720953019335866    Test Loss :0.0010710495989769697\n",
      "Epoch :0.725    Train Loss :0.00035677841515280306    Test Loss :0.0010530473664402962\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7333333333333333    Train Loss :0.00034478501765988767    Test Loss :0.0010648955358192325\n",
      "Epoch :0.7416666666666667    Train Loss :0.0003518340818118304    Test Loss :0.0010512167355045676\n",
      "Epoch :0.75    Train Loss :0.00034033480915240943    Test Loss :0.0010139562655240297\n",
      "Epoch :0.7583333333333333    Train Loss :0.000349710404407233    Test Loss :0.000981967314146459\n",
      "Epoch :0.7666666666666667    Train Loss :0.0004189105238765478    Test Loss :0.0011938861571252346\n",
      "Epoch :0.775    Train Loss :0.0004017177561763674    Test Loss :0.001537370146252215\n",
      "Epoch :0.7833333333333333    Train Loss :0.00034906904329545796    Test Loss :0.0013858303427696228\n",
      "Epoch :0.7916666666666666    Train Loss :0.00034211858292110264    Test Loss :0.0011668126098811626\n",
      "Epoch :0.8    Train Loss :0.0003446179907768965    Test Loss :0.0013261662097647786\n",
      "Epoch :0.8083333333333333    Train Loss :0.00034823588794097304    Test Loss :0.0011551531497389078\n",
      "Epoch :0.8166666666666667    Train Loss :0.0003263112448621541    Test Loss :0.0012096548452973366\n",
      "Epoch :0.825    Train Loss :0.00032677751732990146    Test Loss :0.0013231876073405147\n",
      "Epoch :0.8333333333333334    Train Loss :0.0003323503478895873    Test Loss :0.0011829258874058723\n",
      "Epoch :0.8416666666666667    Train Loss :0.00032917954376898706    Test Loss :0.001118417363613844\n",
      "Epoch :0.85    Train Loss :0.00034914445132017136    Test Loss :0.0010687528410926461\n",
      "Epoch :0.8583333333333333    Train Loss :0.000319732294883579    Test Loss :0.0011101557174697518\n",
      "Epoch :0.8666666666666667    Train Loss :0.00040411492227576673    Test Loss :0.0009942755568772554\n",
      "Epoch :0.875    Train Loss :0.00033462184364907444    Test Loss :0.0009770025499165058\n",
      "Epoch :0.8833333333333333    Train Loss :0.00032507386640645564    Test Loss :0.0009719196823425591\n",
      "Epoch :0.8916666666666667    Train Loss :0.0003196686157025397    Test Loss :0.000963983649853617\n",
      "Epoch :0.9    Train Loss :0.0003371798084117472    Test Loss :0.0012189381523057818\n",
      "Epoch :0.9083333333333333    Train Loss :0.0004539832007139921    Test Loss :0.0010843346826732159\n",
      "Epoch :0.9166666666666666    Train Loss :0.00035405499511398375    Test Loss :0.0010542803211137652\n",
      "Epoch :0.925    Train Loss :0.0003280095988884568    Test Loss :0.0011112374486401677\n",
      "Epoch :0.9333333333333333    Train Loss :0.0003532157279551029    Test Loss :0.0010558008216321468\n",
      "Epoch :0.9416666666666667    Train Loss :0.00034660231904126704    Test Loss :0.0009796461090445518\n",
      "Epoch :0.95    Train Loss :0.0003192950680386275    Test Loss :0.0008751098066568375\n",
      "Epoch :0.9583333333333334    Train Loss :0.0003212196461390704    Test Loss :0.0010424167849123478\n",
      "Epoch :0.9666666666666667    Train Loss :0.00031394240795634687    Test Loss :0.0009334553615190089\n",
      "Epoch :0.975    Train Loss :0.0004908709088340402    Test Loss :0.002093742834404111\n",
      "Epoch :0.9833333333333333    Train Loss :0.00041836523450911045    Test Loss :0.0015136618167161942\n",
      "Epoch :0.9916666666666667    Train Loss :0.00042212402331642807    Test Loss :0.0013252724893391132\n",
      "Epoch :1.0    Train Loss :0.0003477920836303383    Test Loss :0.000948130211327225\n",
      "RMSE: 8.183677009402915\n",
      "MAE: 6.7002931377400925\n",
      "MAPE: 5.99646917362818%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 50, 'lr': 0.01, 'num_layers': 2}\n",
      "total:  78.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.055998895317316055    Test Loss :0.4082409143447876\n",
      "Epoch :0.016666666666666666    Train Loss :0.058864787220954895    Test Loss :0.2954719364643097\n",
      "Epoch :0.025    Train Loss :0.043901026248931885    Test Loss :0.23984737694263458\n",
      "Epoch :0.03333333333333333    Train Loss :0.042163703590631485    Test Loss :0.23812739551067352\n",
      "Epoch :0.041666666666666664    Train Loss :0.041530001908540726    Test Loss :0.19685055315494537\n",
      "Epoch :0.05    Train Loss :0.04054878279566765    Test Loss :0.22545377910137177\n",
      "Epoch :0.058333333333333334    Train Loss :0.040553804486989975    Test Loss :0.22686439752578735\n",
      "Epoch :0.06666666666666667    Train Loss :0.03847990557551384    Test Loss :0.19684076309204102\n",
      "Epoch :0.075    Train Loss :0.02952764369547367    Test Loss :0.1362958550453186\n",
      "Epoch :0.08333333333333333    Train Loss :0.014440574683248997    Test Loss :0.03647153824567795\n",
      "Epoch :0.09166666666666666    Train Loss :0.011586659587919712    Test Loss :0.029358388856053352\n",
      "Epoch :0.1    Train Loss :0.008465606719255447    Test Loss :0.0367107130587101\n",
      "Epoch :0.10833333333333334    Train Loss :0.007686967495828867    Test Loss :0.035678815096616745\n",
      "Epoch :0.11666666666666667    Train Loss :0.006071410141885281    Test Loss :0.01646365225315094\n",
      "Epoch :0.125    Train Loss :0.004865133203566074    Test Loss :0.013110960833728313\n",
      "Epoch :0.13333333333333333    Train Loss :0.00411054166033864    Test Loss :0.01582905650138855\n",
      "Epoch :0.14166666666666666    Train Loss :0.003374718828126788    Test Loss :0.012957864440977573\n",
      "Epoch :0.15    Train Loss :0.0031147352419793606    Test Loss :0.010629200376570225\n",
      "Epoch :0.15833333333333333    Train Loss :0.0028452202677726746    Test Loss :0.00992978923022747\n",
      "Epoch :0.16666666666666666    Train Loss :0.0027756360359489918    Test Loss :0.010143037885427475\n",
      "Epoch :0.175    Train Loss :0.0024811653420329094    Test Loss :0.00930505059659481\n",
      "Epoch :0.18333333333333332    Train Loss :0.002407300053164363    Test Loss :0.006450452841818333\n",
      "Epoch :0.19166666666666668    Train Loss :0.00235715857706964    Test Loss :0.007157726678997278\n",
      "Epoch :0.2    Train Loss :0.0022004093043506145    Test Loss :0.007593424059450626\n",
      "Epoch :0.20833333333333334    Train Loss :0.002200062619522214    Test Loss :0.006889835000038147\n",
      "Epoch :0.21666666666666667    Train Loss :0.001993804005905986    Test Loss :0.006957303266972303\n",
      "Epoch :0.225    Train Loss :0.0019976890180259943    Test Loss :0.006302933674305677\n",
      "Epoch :0.23333333333333334    Train Loss :0.0018546220380812883    Test Loss :0.005655371118336916\n",
      "Epoch :0.24166666666666667    Train Loss :0.0018552090041339397    Test Loss :0.00527183199301362\n",
      "Epoch :0.25    Train Loss :0.0018028778722509742    Test Loss :0.00612347899004817\n",
      "Epoch :0.25833333333333336    Train Loss :0.0017173083033412695    Test Loss :0.0055135455913841724\n",
      "Epoch :0.26666666666666666    Train Loss :0.0017627946799620986    Test Loss :0.00464197201654315\n",
      "Epoch :0.275    Train Loss :0.001636515255086124    Test Loss :0.0051195272244513035\n",
      "Epoch :0.2833333333333333    Train Loss :0.0016127816634252667    Test Loss :0.005297196097671986\n",
      "Epoch :0.2916666666666667    Train Loss :0.0017464003758504987    Test Loss :0.0054651242680847645\n",
      "Epoch :0.3    Train Loss :0.0014966558665037155    Test Loss :0.00449878117069602\n",
      "Epoch :0.30833333333333335    Train Loss :0.0014975896338000894    Test Loss :0.004480722825974226\n",
      "Epoch :0.31666666666666665    Train Loss :0.0014059431850910187    Test Loss :0.004509866703301668\n",
      "Epoch :0.325    Train Loss :0.0014387986157089472    Test Loss :0.00394686171784997\n",
      "Epoch :0.3333333333333333    Train Loss :0.0013360378798097372    Test Loss :0.004368362482637167\n",
      "Epoch :0.3416666666666667    Train Loss :0.0013321606675162911    Test Loss :0.004883835557848215\n",
      "Epoch :0.35    Train Loss :0.0015231581637635827    Test Loss :0.006667045410722494\n",
      "Epoch :0.35833333333333334    Train Loss :0.00143256108276546    Test Loss :0.004794920329004526\n",
      "Epoch :0.36666666666666664    Train Loss :0.001290694111958146    Test Loss :0.00453506363555789\n",
      "Epoch :0.375    Train Loss :0.0012967947404831648    Test Loss :0.00398620031774044\n",
      "Epoch :0.38333333333333336    Train Loss :0.0013325219042599201    Test Loss :0.0050081294029951096\n",
      "Epoch :0.39166666666666666    Train Loss :0.0012627370888367295    Test Loss :0.004461998585611582\n",
      "Epoch :0.4    Train Loss :0.0012230519205331802    Test Loss :0.004422360099852085\n",
      "Epoch :0.4083333333333333    Train Loss :0.0011608622735366225    Test Loss :0.004021362867206335\n",
      "Epoch :0.4166666666666667    Train Loss :0.0012330827303230762    Test Loss :0.004109245724976063\n",
      "Epoch :0.425    Train Loss :0.0011513811768963933    Test Loss :0.004033413715660572\n",
      "Epoch :0.43333333333333335    Train Loss :0.001087925978936255    Test Loss :0.00400569336488843\n",
      "Epoch :0.44166666666666665    Train Loss :0.0011433613253757358    Test Loss :0.003932824824005365\n",
      "Epoch :0.45    Train Loss :0.0011040959507226944    Test Loss :0.004086065571755171\n",
      "Epoch :0.4583333333333333    Train Loss :0.0010833409614861012    Test Loss :0.003656121902167797\n",
      "Epoch :0.4666666666666667    Train Loss :0.001013432047329843    Test Loss :0.003747448790818453\n",
      "Epoch :0.475    Train Loss :0.0010638971580192447    Test Loss :0.003848221618682146\n",
      "Epoch :0.48333333333333334    Train Loss :0.0010457157623022795    Test Loss :0.004086087457835674\n",
      "Epoch :0.49166666666666664    Train Loss :0.00103383872192353    Test Loss :0.003895676927641034\n",
      "Epoch :0.5    Train Loss :0.001120955916121602    Test Loss :0.004248923156410456\n",
      "Epoch :0.5083333333333333    Train Loss :0.0009582622442394495    Test Loss :0.0037912921980023384\n",
      "Epoch :0.5166666666666667    Train Loss :0.0011966185411438346    Test Loss :0.004168331623077393\n",
      "Epoch :0.525    Train Loss :0.0010241155978292227    Test Loss :0.004655450116842985\n",
      "Epoch :0.5333333333333333    Train Loss :0.000991982058621943    Test Loss :0.00371443759649992\n",
      "Epoch :0.5416666666666666    Train Loss :0.0009987055091187358    Test Loss :0.004148099105805159\n",
      "Epoch :0.55    Train Loss :0.0009796916274353862    Test Loss :0.0036771357990801334\n",
      "Epoch :0.5583333333333333    Train Loss :0.0009445883915759623    Test Loss :0.003676577005535364\n",
      "Epoch :0.5666666666666667    Train Loss :0.0009271567687392235    Test Loss :0.003338817274197936\n",
      "Epoch :0.575    Train Loss :0.0009206610848195851    Test Loss :0.0036332302261143923\n",
      "Epoch :0.5833333333333334    Train Loss :0.000890120049007237    Test Loss :0.0035048406571149826\n",
      "Epoch :0.5916666666666667    Train Loss :0.0009762800764292479    Test Loss :0.0035561120603233576\n",
      "Epoch :0.6    Train Loss :0.0008807536214590073    Test Loss :0.003777920501306653\n",
      "Epoch :0.6083333333333333    Train Loss :0.0008638154831714928    Test Loss :0.003723656991496682\n",
      "Epoch :0.6166666666666667    Train Loss :0.000891556148417294    Test Loss :0.003348446683958173\n",
      "Epoch :0.625    Train Loss :0.000838605163153261    Test Loss :0.0035321421455591917\n",
      "Epoch :0.6333333333333333    Train Loss :0.000838042760733515    Test Loss :0.003595637856051326\n",
      "Epoch :0.6416666666666667    Train Loss :0.0008755987510085106    Test Loss :0.003219612641260028\n",
      "Epoch :0.65    Train Loss :0.0008876786450855434    Test Loss :0.0033047290053218603\n",
      "Epoch :0.6583333333333333    Train Loss :0.0008538435795344412    Test Loss :0.003636595094576478\n",
      "Epoch :0.6666666666666666    Train Loss :0.000830243167001754    Test Loss :0.0035348800010979176\n",
      "Epoch :0.675    Train Loss :0.0008186747436411679    Test Loss :0.00330635579302907\n",
      "Epoch :0.6833333333333333    Train Loss :0.000845134723931551    Test Loss :0.0034140560310333967\n",
      "Epoch :0.6916666666666667    Train Loss :0.0007439774344675243    Test Loss :0.00333785661496222\n",
      "Epoch :0.7    Train Loss :0.0007962374365888536    Test Loss :0.0031125342939049006\n",
      "Epoch :0.7083333333333334    Train Loss :0.0007891505374573171    Test Loss :0.0032437103800475597\n",
      "Epoch :0.7166666666666667    Train Loss :0.0007651306805200875    Test Loss :0.003343313466757536\n",
      "Epoch :0.725    Train Loss :0.0008891337784007192    Test Loss :0.0034608168061822653\n",
      "Epoch :0.7333333333333333    Train Loss :0.0008470887551084161    Test Loss :0.003385529387742281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7416666666666667    Train Loss :0.0007702001021243632    Test Loss :0.0036569039802998304\n",
      "Epoch :0.75    Train Loss :0.0007727653719484806    Test Loss :0.0031677919905632734\n",
      "Epoch :0.7583333333333333    Train Loss :0.0007649603066965938    Test Loss :0.003116447711363435\n",
      "Epoch :0.7666666666666667    Train Loss :0.0008052561315707862    Test Loss :0.0032751562539488077\n",
      "Epoch :0.775    Train Loss :0.0008725497755222023    Test Loss :0.003136943094432354\n",
      "Epoch :0.7833333333333333    Train Loss :0.000834811944514513    Test Loss :0.0034664839040488005\n",
      "Epoch :0.7916666666666666    Train Loss :0.0008196883136406541    Test Loss :0.003540489124134183\n",
      "Epoch :0.8    Train Loss :0.0008133405353873968    Test Loss :0.0029527107253670692\n",
      "Epoch :0.8083333333333333    Train Loss :0.0007222371059469879    Test Loss :0.0035713466349989176\n",
      "Epoch :0.8166666666666667    Train Loss :0.0008380125509575009    Test Loss :0.0033330887090414762\n",
      "Epoch :0.825    Train Loss :0.0007973681786097586    Test Loss :0.003005665959790349\n",
      "Epoch :0.8333333333333334    Train Loss :0.0008011856698431075    Test Loss :0.003163060639053583\n",
      "Epoch :0.8416666666666667    Train Loss :0.0006922130123712122    Test Loss :0.0031756844837218523\n",
      "Epoch :0.85    Train Loss :0.000740622344892472    Test Loss :0.003029139246791601\n",
      "Epoch :0.8583333333333333    Train Loss :0.0007696761167608202    Test Loss :0.0033432841300964355\n",
      "Epoch :0.8666666666666667    Train Loss :0.000779339112341404    Test Loss :0.0030376571230590343\n",
      "Epoch :0.875    Train Loss :0.0007020559860393405    Test Loss :0.003148976480588317\n",
      "Epoch :0.8833333333333333    Train Loss :0.0006524368072859943    Test Loss :0.0032955599017441273\n",
      "Epoch :0.8916666666666667    Train Loss :0.0006731815519742668    Test Loss :0.003190504154190421\n",
      "Epoch :0.9    Train Loss :0.0007047188119031489    Test Loss :0.003266687970608473\n",
      "Epoch :0.9083333333333333    Train Loss :0.0006903224275447428    Test Loss :0.003062332747504115\n",
      "Epoch :0.9166666666666666    Train Loss :0.0006569377728737891    Test Loss :0.0031285667791962624\n",
      "Epoch :0.925    Train Loss :0.000672043883241713    Test Loss :0.0031649142038077116\n",
      "Epoch :0.9333333333333333    Train Loss :0.0006327915471047163    Test Loss :0.0030402601696550846\n",
      "Epoch :0.9416666666666667    Train Loss :0.0006603229558095336    Test Loss :0.002999979304149747\n",
      "Epoch :0.95    Train Loss :0.0006216831388883293    Test Loss :0.003149748081341386\n",
      "Epoch :0.9583333333333334    Train Loss :0.0006741514662280679    Test Loss :0.003180944360792637\n",
      "Epoch :0.9666666666666667    Train Loss :0.000670389796141535    Test Loss :0.003247176995500922\n",
      "Epoch :0.975    Train Loss :0.0006441834848374128    Test Loss :0.003014912595972419\n",
      "Epoch :0.9833333333333333    Train Loss :0.0007606662693433464    Test Loss :0.0028739559929817915\n",
      "Epoch :0.9916666666666667    Train Loss :0.000678371696267277    Test Loss :0.003224516287446022\n",
      "Epoch :1.0    Train Loss :0.000698418531101197    Test Loss :0.0027033432852476835\n",
      "RMSE: 8.970266639663919\n",
      "MAE: 7.380871235850734\n",
      "MAPE: 6.712194264515918%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 50, 'lr': 0.01, 'num_layers': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  81.0\n",
      "Epoch :0.008333333333333333    Train Loss :0.03943494334816933    Test Loss :0.18190516531467438\n",
      "Epoch :0.016666666666666666    Train Loss :0.011845085769891739    Test Loss :0.09212761372327805\n",
      "Epoch :0.025    Train Loss :0.01108265295624733    Test Loss :0.023550909012556076\n",
      "Epoch :0.03333333333333333    Train Loss :0.0036405089776962996    Test Loss :0.009494183585047722\n",
      "Epoch :0.041666666666666664    Train Loss :0.003606353187933564    Test Loss :0.01723267324268818\n",
      "Epoch :0.05    Train Loss :0.003085152246057987    Test Loss :0.021590448915958405\n",
      "Epoch :0.058333333333333334    Train Loss :0.002645900472998619    Test Loss :0.004837607964873314\n",
      "Epoch :0.06666666666666667    Train Loss :0.0018772347830235958    Test Loss :0.011228570714592934\n",
      "Epoch :0.075    Train Loss :0.0016835483256727457    Test Loss :0.006305730435997248\n",
      "Epoch :0.08333333333333333    Train Loss :0.0014453877229243517    Test Loss :0.005886281840503216\n",
      "Epoch :0.09166666666666666    Train Loss :0.0013341271551325917    Test Loss :0.005961223971098661\n",
      "Epoch :0.1    Train Loss :0.0012287599965929985    Test Loss :0.004574565216898918\n",
      "Epoch :0.10833333333333334    Train Loss :0.0011120998533442616    Test Loss :0.005406318232417107\n",
      "Epoch :0.11666666666666667    Train Loss :0.0009969959501177073    Test Loss :0.004345959518104792\n",
      "Epoch :0.125    Train Loss :0.0009567408123984933    Test Loss :0.0040237633511424065\n",
      "Epoch :0.13333333333333333    Train Loss :0.0008573870291002095    Test Loss :0.003927235025912523\n",
      "Epoch :0.14166666666666666    Train Loss :0.0008128585177473724    Test Loss :0.003331598825752735\n",
      "Epoch :0.15    Train Loss :0.0007794459233991802    Test Loss :0.0033413157798349857\n",
      "Epoch :0.15833333333333333    Train Loss :0.0007279067649506032    Test Loss :0.003096825210377574\n",
      "Epoch :0.16666666666666666    Train Loss :0.0007102150120772421    Test Loss :0.0029013995081186295\n",
      "Epoch :0.175    Train Loss :0.0007036717142909765    Test Loss :0.0025728060863912106\n",
      "Epoch :0.18333333333333332    Train Loss :0.0006480309530161321    Test Loss :0.0023376804310828447\n",
      "Epoch :0.19166666666666668    Train Loss :0.0006413650698959827    Test Loss :0.0023345251102000475\n",
      "Epoch :0.2    Train Loss :0.0006627477123402059    Test Loss :0.0020314001012593508\n",
      "Epoch :0.20833333333333334    Train Loss :0.0008004485280252993    Test Loss :0.00205790507607162\n",
      "Epoch :0.21666666666666667    Train Loss :0.000991875771433115    Test Loss :0.002706833416596055\n",
      "Epoch :0.225    Train Loss :0.0008457126095890999    Test Loss :0.002911765594035387\n",
      "Epoch :0.23333333333333334    Train Loss :0.0006808922626078129    Test Loss :0.0033328726422041655\n",
      "Epoch :0.24166666666666667    Train Loss :0.00063361506909132    Test Loss :0.002243403345346451\n",
      "Epoch :0.25    Train Loss :0.0006371567142196    Test Loss :0.002441405551508069\n",
      "Epoch :0.25833333333333336    Train Loss :0.0006066750502213836    Test Loss :0.002009061863645911\n",
      "Epoch :0.26666666666666666    Train Loss :0.0005698165041394532    Test Loss :0.002122570062056184\n",
      "Epoch :0.275    Train Loss :0.0005818438366986811    Test Loss :0.0019134754547849298\n",
      "Epoch :0.2833333333333333    Train Loss :0.0005415559280663729    Test Loss :0.0019040916813537478\n",
      "Epoch :0.2916666666666667    Train Loss :0.00054495147196576    Test Loss :0.00186741107609123\n",
      "Epoch :0.3    Train Loss :0.0006372486823238432    Test Loss :0.0015556471189484\n",
      "Epoch :0.30833333333333335    Train Loss :0.0005620850715786219    Test Loss :0.001835057744756341\n",
      "Epoch :0.31666666666666665    Train Loss :0.0005998979322612286    Test Loss :0.0015857145190238953\n",
      "Epoch :0.325    Train Loss :0.00045773666352033615    Test Loss :0.0016798150027170777\n",
      "Epoch :0.3333333333333333    Train Loss :0.000534216349478811    Test Loss :0.0019550644792616367\n",
      "Epoch :0.3416666666666667    Train Loss :0.0024380709510296583    Test Loss :0.0019575951155275106\n",
      "Epoch :0.35    Train Loss :0.000877298996783793    Test Loss :0.003759930143132806\n",
      "Epoch :0.35833333333333334    Train Loss :0.000846570183057338    Test Loss :0.002681245096027851\n",
      "Epoch :0.36666666666666664    Train Loss :0.0005528590991161764    Test Loss :0.0019527217373251915\n",
      "Epoch :0.375    Train Loss :0.0005401548696681857    Test Loss :0.0022445127833634615\n",
      "Epoch :0.38333333333333336    Train Loss :0.0005106796161271632    Test Loss :0.0015078420983627439\n",
      "Epoch :0.39166666666666666    Train Loss :0.00047952603199519217    Test Loss :0.0015117406146600842\n",
      "Epoch :0.4    Train Loss :0.00045592887909151614    Test Loss :0.001360340160317719\n",
      "Epoch :0.4083333333333333    Train Loss :0.00043594001908786595    Test Loss :0.0017232251120731235\n",
      "Epoch :0.4166666666666667    Train Loss :0.0004105608386453241    Test Loss :0.0012371051125228405\n",
      "Epoch :0.425    Train Loss :0.0003949505335185677    Test Loss :0.0011603018501773477\n",
      "Epoch :0.43333333333333335    Train Loss :0.000382598111173138    Test Loss :0.0011018960503861308\n",
      "Epoch :0.44166666666666665    Train Loss :0.0003921707975678146    Test Loss :0.00118377641774714\n",
      "Epoch :0.45    Train Loss :0.0003838851989712566    Test Loss :0.0011812149314209819\n",
      "Epoch :0.4583333333333333    Train Loss :0.0003666179836727679    Test Loss :0.001116762519814074\n",
      "Epoch :0.4666666666666667    Train Loss :0.00039229681715369225    Test Loss :0.0010719706770032644\n",
      "Epoch :0.475    Train Loss :0.0005923046264797449    Test Loss :0.0018443088047206402\n",
      "Epoch :0.48333333333333334    Train Loss :0.0003944914788007736    Test Loss :0.0017958164680749178\n",
      "Epoch :0.49166666666666664    Train Loss :0.0004141446843277663    Test Loss :0.0015465293545275927\n",
      "Epoch :0.5    Train Loss :0.0004536531923804432    Test Loss :0.0014671111712232232\n",
      "Epoch :0.5083333333333333    Train Loss :0.00037880908348597586    Test Loss :0.0012900261208415031\n",
      "Epoch :0.5166666666666667    Train Loss :0.00038691633380949497    Test Loss :0.001229761284776032\n",
      "Epoch :0.525    Train Loss :0.0003720957029145211    Test Loss :0.0011166437761858106\n",
      "Epoch :0.5333333333333333    Train Loss :0.00034732799394987524    Test Loss :0.0010489027481526136\n",
      "Epoch :0.5416666666666666    Train Loss :0.00044658337719738483    Test Loss :0.001171414740383625\n",
      "Epoch :0.55    Train Loss :0.0005567885236814618    Test Loss :0.0017262912588194013\n",
      "Epoch :0.5583333333333333    Train Loss :0.00037053311825729907    Test Loss :0.0010685038287192583\n",
      "Epoch :0.5666666666666667    Train Loss :0.00036885280860587955    Test Loss :0.001098282402381301\n",
      "Epoch :0.575    Train Loss :0.0003411782090552151    Test Loss :0.0010765868937596679\n",
      "Epoch :0.5833333333333334    Train Loss :0.00035470182774588466    Test Loss :0.0009700082591734827\n",
      "Epoch :0.5916666666666667    Train Loss :0.00033307226840406656    Test Loss :0.0008392537711188197\n",
      "Epoch :0.6    Train Loss :0.0003607543185353279    Test Loss :0.0009396062232553959\n",
      "Epoch :0.6083333333333333    Train Loss :0.0015719139482825994    Test Loss :0.003979996312409639\n",
      "Epoch :0.6166666666666667    Train Loss :0.0004152558685746044    Test Loss :0.0016262440476566553\n",
      "Epoch :0.625    Train Loss :0.00040205553523264825    Test Loss :0.0023277373984456062\n",
      "Epoch :0.6333333333333333    Train Loss :0.0004564595874398947    Test Loss :0.0018465517787262797\n",
      "Epoch :0.6416666666666667    Train Loss :0.0004591462202370167    Test Loss :0.0014383516972884536\n",
      "Epoch :0.65    Train Loss :0.00045329667045734823    Test Loss :0.0012227596016600728\n",
      "Epoch :0.6583333333333333    Train Loss :0.00042813632171601057    Test Loss :0.0012255220208317041\n",
      "Epoch :0.6666666666666666    Train Loss :0.00038309558294713497    Test Loss :0.0008661215542815626\n",
      "Epoch :0.675    Train Loss :0.00036989458021707833    Test Loss :0.0011431630700826645\n",
      "Epoch :0.6833333333333333    Train Loss :0.00032030476722866297    Test Loss :0.0008608236676082015\n",
      "Epoch :0.6916666666666667    Train Loss :0.0003227781562600285    Test Loss :0.0008901609107851982\n",
      "Epoch :0.7    Train Loss :0.00033726257970556617    Test Loss :0.000991560285910964\n",
      "Epoch :0.7083333333333334    Train Loss :0.0003193073207512498    Test Loss :0.000820977846160531\n",
      "Epoch :0.7166666666666667    Train Loss :0.0003100557951256633    Test Loss :0.0008544412557967007\n",
      "Epoch :0.725    Train Loss :0.00031441953615285456    Test Loss :0.000820080516859889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7333333333333333    Train Loss :0.0003149551630485803    Test Loss :0.0008075993973761797\n",
      "Epoch :0.7416666666666667    Train Loss :0.00031125551322475076    Test Loss :0.0009034062968567014\n",
      "Epoch :0.75    Train Loss :0.0003078531881328672    Test Loss :0.0008084036526270211\n",
      "Epoch :0.7583333333333333    Train Loss :0.000302081840345636    Test Loss :0.0008031277102418244\n",
      "Epoch :0.7666666666666667    Train Loss :0.0004749041981995106    Test Loss :0.002596519188955426\n",
      "Epoch :0.775    Train Loss :0.00042824086267501116    Test Loss :0.0010994585463777184\n",
      "Epoch :0.7833333333333333    Train Loss :0.00036534538958221674    Test Loss :0.0008945728768594563\n",
      "Epoch :0.7916666666666666    Train Loss :0.0002959240518976003    Test Loss :0.0009033935493789613\n",
      "Epoch :0.8    Train Loss :0.000308301649056375    Test Loss :0.0007917557959444821\n",
      "Epoch :0.8083333333333333    Train Loss :0.00033281417563557625    Test Loss :0.000900633167475462\n",
      "Epoch :0.8166666666666667    Train Loss :0.00032681625452823937    Test Loss :0.0008839771035127342\n",
      "Epoch :0.825    Train Loss :0.00029101717518642545    Test Loss :0.0008546342141926289\n",
      "Epoch :0.8333333333333334    Train Loss :0.00028466092771850526    Test Loss :0.0007828859379515052\n",
      "Epoch :0.8416666666666667    Train Loss :0.00028357282280921936    Test Loss :0.000824832241050899\n",
      "Epoch :0.85    Train Loss :0.00028482908965088427    Test Loss :0.0007557987701147795\n",
      "Epoch :0.8583333333333333    Train Loss :0.0006540213944390416    Test Loss :0.0033018861431628466\n",
      "Epoch :0.8666666666666667    Train Loss :0.0005141461733728647    Test Loss :0.0012856645043939352\n",
      "Epoch :0.875    Train Loss :0.00039245153311640024    Test Loss :0.0009687480633147061\n",
      "Epoch :0.8833333333333333    Train Loss :0.00030379692907445133    Test Loss :0.0010190026368945837\n",
      "Epoch :0.8916666666666667    Train Loss :0.00031911718542687595    Test Loss :0.0009881726000458002\n",
      "Epoch :0.9    Train Loss :0.00029114229255355895    Test Loss :0.0009766387520357966\n",
      "Epoch :0.9083333333333333    Train Loss :0.00028126646066084504    Test Loss :0.000978156109340489\n",
      "Epoch :0.9166666666666666    Train Loss :0.00028149434365332127    Test Loss :0.0009739584638737142\n",
      "Epoch :0.925    Train Loss :0.0002751631254795939    Test Loss :0.0008727830718271434\n",
      "Epoch :0.9333333333333333    Train Loss :0.0002698838070500642    Test Loss :0.0009230782743543386\n",
      "Epoch :0.9416666666666667    Train Loss :0.0002747034013736993    Test Loss :0.0007708666380494833\n",
      "Epoch :0.95    Train Loss :0.0002778139605652541    Test Loss :0.0007679273257963359\n",
      "Epoch :0.9583333333333334    Train Loss :0.0002761331561487168    Test Loss :0.0009386019082739949\n",
      "Epoch :0.9666666666666667    Train Loss :0.00026342144701629877    Test Loss :0.0008199081639759243\n",
      "Epoch :0.975    Train Loss :0.0002704924554564059    Test Loss :0.0009244366665370762\n",
      "Epoch :0.9833333333333333    Train Loss :0.0006943181506358087    Test Loss :0.0023022834211587906\n",
      "Epoch :0.9916666666666667    Train Loss :0.0005823633982799947    Test Loss :0.0013138804351910949\n",
      "Epoch :1.0    Train Loss :0.0003926372737623751    Test Loss :0.0015591942938044667\n",
      "RMSE: 56.9316608935575\n",
      "MAE: 55.50216109404132\n",
      "MAPE: 49.01786442915076%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 50, 'lr': 0.02, 'num_layers': 2}\n",
      "total:  84.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.043853890150785446    Test Loss :0.28257524967193604\n",
      "Epoch :0.016666666666666666    Train Loss :0.04137548431754112    Test Loss :0.20484581589698792\n",
      "Epoch :0.025    Train Loss :0.03680649399757385    Test Loss :0.1784212589263916\n",
      "Epoch :0.03333333333333333    Train Loss :0.014877083711326122    Test Loss :0.046777721494436264\n",
      "Epoch :0.041666666666666664    Train Loss :0.011904940940439701    Test Loss :0.041076332330703735\n",
      "Epoch :0.05    Train Loss :0.009027897380292416    Test Loss :0.05475754663348198\n",
      "Epoch :0.058333333333333334    Train Loss :0.006410668138414621    Test Loss :0.016574647277593613\n",
      "Epoch :0.06666666666666667    Train Loss :0.004986589774489403    Test Loss :0.012450984679162502\n",
      "Epoch :0.075    Train Loss :0.003984782379120588    Test Loss :0.007755720522254705\n",
      "Epoch :0.08333333333333333    Train Loss :0.0032859419006854296    Test Loss :0.01069464348256588\n",
      "Epoch :0.09166666666666666    Train Loss :0.002675466937944293    Test Loss :0.010212020948529243\n",
      "Epoch :0.1    Train Loss :0.0024702444206923246    Test Loss :0.00850413553416729\n",
      "Epoch :0.10833333333333334    Train Loss :0.0021079201251268387    Test Loss :0.008868280798196793\n",
      "Epoch :0.11666666666666667    Train Loss :0.0018920607399195433    Test Loss :0.006416755262762308\n",
      "Epoch :0.125    Train Loss :0.0018416891107335687    Test Loss :0.005816223099827766\n",
      "Epoch :0.13333333333333333    Train Loss :0.0016968450509011745    Test Loss :0.005946325603872538\n",
      "Epoch :0.14166666666666666    Train Loss :0.0016445412766188383    Test Loss :0.005422275047749281\n",
      "Epoch :0.15    Train Loss :0.0017116809030994773    Test Loss :0.0048715886659920216\n",
      "Epoch :0.15833333333333333    Train Loss :0.0014667583163827658    Test Loss :0.00458661001175642\n",
      "Epoch :0.16666666666666666    Train Loss :0.001482539577409625    Test Loss :0.005214340519160032\n",
      "Epoch :0.175    Train Loss :0.001394164632074535    Test Loss :0.004826917313039303\n",
      "Epoch :0.18333333333333332    Train Loss :0.0013347280910238624    Test Loss :0.005037541966885328\n",
      "Epoch :0.19166666666666668    Train Loss :0.0014551251661032438    Test Loss :0.004822809714823961\n",
      "Epoch :0.2    Train Loss :0.0014164053136482835    Test Loss :0.004157012794166803\n",
      "Epoch :0.20833333333333334    Train Loss :0.0013367909705266356    Test Loss :0.004132092464715242\n",
      "Epoch :0.21666666666666667    Train Loss :0.0011986367171630263    Test Loss :0.004256816580891609\n",
      "Epoch :0.225    Train Loss :0.0011618612334132195    Test Loss :0.004013425670564175\n",
      "Epoch :0.23333333333333334    Train Loss :0.0013083864469081163    Test Loss :0.003493126016110182\n",
      "Epoch :0.24166666666666667    Train Loss :0.0011274012504145503    Test Loss :0.00379987433552742\n",
      "Epoch :0.25    Train Loss :0.0010715577518567443    Test Loss :0.004029305651783943\n",
      "Epoch :0.25833333333333336    Train Loss :0.0010777210118249059    Test Loss :0.003786760848015547\n",
      "Epoch :0.26666666666666666    Train Loss :0.001061821822077036    Test Loss :0.0037509577814489603\n",
      "Epoch :0.275    Train Loss :0.001390427234582603    Test Loss :0.0035609451588243246\n",
      "Epoch :0.2833333333333333    Train Loss :0.0011426112614572048    Test Loss :0.003516577184200287\n",
      "Epoch :0.2916666666666667    Train Loss :0.0011691603576764464    Test Loss :0.0035659857094287872\n",
      "Epoch :0.3    Train Loss :0.0010565713746473193    Test Loss :0.003418754553422332\n",
      "Epoch :0.30833333333333335    Train Loss :0.000974795431829989    Test Loss :0.0036114323884248734\n",
      "Epoch :0.31666666666666665    Train Loss :0.001017707516439259    Test Loss :0.0034713444765657187\n",
      "Epoch :0.325    Train Loss :0.0009031535591930151    Test Loss :0.0031943125650286674\n",
      "Epoch :0.3333333333333333    Train Loss :0.0008942799177020788    Test Loss :0.003405618714168668\n",
      "Epoch :0.3416666666666667    Train Loss :0.0008796720067039132    Test Loss :0.002950680209323764\n",
      "Epoch :0.35    Train Loss :0.0008283565985038877    Test Loss :0.0032386367674916983\n",
      "Epoch :0.35833333333333334    Train Loss :0.0008932301425375044    Test Loss :0.0031237471848726273\n",
      "Epoch :0.36666666666666664    Train Loss :0.0009212908917106688    Test Loss :0.0038640224374830723\n",
      "Epoch :0.375    Train Loss :0.000987426727078855    Test Loss :0.0031925577204674482\n",
      "Epoch :0.38333333333333336    Train Loss :0.0009518577135168016    Test Loss :0.003036555601283908\n",
      "Epoch :0.39166666666666666    Train Loss :0.0008713217684999108    Test Loss :0.003179000923410058\n",
      "Epoch :0.4    Train Loss :0.0009707420249469578    Test Loss :0.0034104506485164165\n",
      "Epoch :0.4083333333333333    Train Loss :0.0008620432927273214    Test Loss :0.0030662810895591974\n",
      "Epoch :0.4166666666666667    Train Loss :0.001094939187169075    Test Loss :0.0032920141238719225\n",
      "Epoch :0.425    Train Loss :0.0008438979275524616    Test Loss :0.0029548488091677427\n",
      "Epoch :0.43333333333333335    Train Loss :0.0007741113076917827    Test Loss :0.0031275898218154907\n",
      "Epoch :0.44166666666666665    Train Loss :0.0007755119586363435    Test Loss :0.003123323665931821\n",
      "Epoch :0.45    Train Loss :0.000772574101574719    Test Loss :0.00313601759262383\n",
      "Epoch :0.4583333333333333    Train Loss :0.0006965827778913081    Test Loss :0.0026605280581861734\n",
      "Epoch :0.4666666666666667    Train Loss :0.0007447482785210013    Test Loss :0.0032754880376160145\n",
      "Epoch :0.475    Train Loss :0.0006908808718435466    Test Loss :0.002809034427627921\n",
      "Epoch :0.48333333333333334    Train Loss :0.001647351193241775    Test Loss :0.006495811976492405\n",
      "Epoch :0.49166666666666664    Train Loss :0.0009280608501285315    Test Loss :0.0029978170059621334\n",
      "Epoch :0.5    Train Loss :0.0011303428327664733    Test Loss :0.0034022261388599873\n",
      "Epoch :0.5083333333333333    Train Loss :0.0009926288621500134    Test Loss :0.003028420265763998\n",
      "Epoch :0.5166666666666667    Train Loss :0.0007676505483686924    Test Loss :0.0033034097868949175\n",
      "Epoch :0.525    Train Loss :0.0007585835410282016    Test Loss :0.0030257722828537226\n",
      "Epoch :0.5333333333333333    Train Loss :0.0006758515955880284    Test Loss :0.0029916949570178986\n",
      "Epoch :0.5416666666666666    Train Loss :0.0006558992899954319    Test Loss :0.002622411586344242\n",
      "Epoch :0.55    Train Loss :0.0006269302102737129    Test Loss :0.0029236148111522198\n",
      "Epoch :0.5583333333333333    Train Loss :0.0005955492961220443    Test Loss :0.002896255813539028\n",
      "Epoch :0.5666666666666667    Train Loss :0.000603150692768395    Test Loss :0.002857743063941598\n",
      "Epoch :0.575    Train Loss :0.0005822192179039121    Test Loss :0.002841031411662698\n",
      "Epoch :0.5833333333333334    Train Loss :0.0005906662554480135    Test Loss :0.0027267299592494965\n",
      "Epoch :0.5916666666666667    Train Loss :0.0006746600847691298    Test Loss :0.0028980684000998735\n",
      "Epoch :0.6    Train Loss :0.0009977201698347926    Test Loss :0.0033807267900556326\n",
      "Epoch :0.6083333333333333    Train Loss :0.000621346291154623    Test Loss :0.002798004075884819\n",
      "Epoch :0.6166666666666667    Train Loss :0.0005893874331377447    Test Loss :0.0028258857782930136\n",
      "Epoch :0.625    Train Loss :0.0005812997696921229    Test Loss :0.0028518913313746452\n",
      "Epoch :0.6333333333333333    Train Loss :0.0005717806634493172    Test Loss :0.002833507489413023\n",
      "Epoch :0.6416666666666667    Train Loss :0.0005767007824033499    Test Loss :0.002763568889349699\n",
      "Epoch :0.65    Train Loss :0.000773476145695895    Test Loss :0.0026185866445302963\n",
      "Epoch :0.6583333333333333    Train Loss :0.0005921741831116378    Test Loss :0.0027810160536319017\n",
      "Epoch :0.6666666666666666    Train Loss :0.0005533479852601886    Test Loss :0.0027364036068320274\n",
      "Epoch :0.675    Train Loss :0.0005990659119561315    Test Loss :0.0027456283569335938\n",
      "Epoch :0.6833333333333333    Train Loss :0.0005541758146136999    Test Loss :0.0028092232532799244\n",
      "Epoch :0.6916666666666667    Train Loss :0.0005682121263816953    Test Loss :0.002740422496572137\n",
      "Epoch :0.7    Train Loss :0.0006772544584237039    Test Loss :0.002521977061405778\n",
      "Epoch :0.7083333333333334    Train Loss :0.0006624062079936266    Test Loss :0.0026597068645060062\n",
      "Epoch :0.7166666666666667    Train Loss :0.0005442273104563355    Test Loss :0.002704860642552376\n",
      "Epoch :0.725    Train Loss :0.0005360741633921862    Test Loss :0.0028376621194183826\n",
      "Epoch :0.7333333333333333    Train Loss :0.0005550734349526465    Test Loss :0.0025990516878664494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7416666666666667    Train Loss :0.0005963850417174399    Test Loss :0.002841597655788064\n",
      "Epoch :0.75    Train Loss :0.0005377153283916414    Test Loss :0.0025349457282572985\n",
      "Epoch :0.7583333333333333    Train Loss :0.0005174918915145099    Test Loss :0.0025467367377132177\n",
      "Epoch :0.7666666666666667    Train Loss :0.0005216367426328361    Test Loss :0.0025804664473980665\n",
      "Epoch :0.775    Train Loss :0.002970918780192733    Test Loss :0.003465278074145317\n",
      "Epoch :0.7833333333333333    Train Loss :0.0013926008250564337    Test Loss :0.00941195897758007\n",
      "Epoch :0.7916666666666666    Train Loss :0.000848670897539705    Test Loss :0.004584732931107283\n",
      "Epoch :0.8    Train Loss :0.000855614198371768    Test Loss :0.0026688326615840197\n",
      "Epoch :0.8083333333333333    Train Loss :0.0009564885403960943    Test Loss :0.0026974908541888\n",
      "Epoch :0.8166666666666667    Train Loss :0.0006936139543540776    Test Loss :0.003180021420121193\n",
      "Epoch :0.825    Train Loss :0.0006174137815833092    Test Loss :0.0031547085382044315\n",
      "Epoch :0.8333333333333334    Train Loss :0.0006195331807248294    Test Loss :0.0027609451208263636\n",
      "Epoch :0.8416666666666667    Train Loss :0.000548400916159153    Test Loss :0.002650346839800477\n",
      "Epoch :0.85    Train Loss :0.0005470828618854284    Test Loss :0.0025788326747715473\n",
      "Epoch :0.8583333333333333    Train Loss :0.0005150718498043716    Test Loss :0.002872481243684888\n",
      "Epoch :0.8666666666666667    Train Loss :0.0005022643017582595    Test Loss :0.0027993747498840094\n",
      "Epoch :0.875    Train Loss :0.0005091407801955938    Test Loss :0.002778946189209819\n",
      "Epoch :0.8833333333333333    Train Loss :0.0005211703828535974    Test Loss :0.002623421372845769\n",
      "Epoch :0.8916666666666667    Train Loss :0.0004634142096620053    Test Loss :0.00257689505815506\n",
      "Epoch :0.9    Train Loss :0.0004884757800027728    Test Loss :0.00251393741928041\n",
      "Epoch :0.9083333333333333    Train Loss :0.0004948673886246979    Test Loss :0.0025156945921480656\n",
      "Epoch :0.9166666666666666    Train Loss :0.000591487914789468    Test Loss :0.0023330743424594402\n",
      "Epoch :0.925    Train Loss :0.0004930907161906362    Test Loss :0.0024314960464835167\n",
      "Epoch :0.9333333333333333    Train Loss :0.0005035442300140858    Test Loss :0.002726414008066058\n",
      "Epoch :0.9416666666666667    Train Loss :0.0004599791136570275    Test Loss :0.002542929956689477\n",
      "Epoch :0.95    Train Loss :0.00046720163663849235    Test Loss :0.002395663410425186\n",
      "Epoch :0.9583333333333334    Train Loss :0.0005079021211713552    Test Loss :0.002448251936584711\n",
      "Epoch :0.9666666666666667    Train Loss :0.000833848025649786    Test Loss :0.003089309437200427\n",
      "Epoch :0.975    Train Loss :0.0006740571116097271    Test Loss :0.0028564585372805595\n",
      "Epoch :0.9833333333333333    Train Loss :0.0005012626643292606    Test Loss :0.0024814698845148087\n",
      "Epoch :0.9916666666666667    Train Loss :0.0005855640629306436    Test Loss :0.0025976644828915596\n",
      "Epoch :1.0    Train Loss :0.0004882125649601221    Test Loss :0.0026820774655789137\n",
      "RMSE: 8.43107376147703\n",
      "MAE: 6.841751416862249\n",
      "MAPE: 6.161572201847299%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 50, 'lr': 0.02, 'num_layers': 5}\n",
      "total:  88.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.047568317502737045    Test Loss :0.23639117181301117\n",
      "Epoch :0.016666666666666666    Train Loss :0.026570305228233337    Test Loss :0.1262911558151245\n",
      "Epoch :0.025    Train Loss :0.009863326326012611    Test Loss :0.10715407878160477\n",
      "Epoch :0.03333333333333333    Train Loss :0.012267448008060455    Test Loss :0.037639301270246506\n",
      "Epoch :0.041666666666666664    Train Loss :0.007022895850241184    Test Loss :0.027556760236620903\n",
      "Epoch :0.05    Train Loss :0.0034452099353075027    Test Loss :0.008495387621223927\n",
      "Epoch :0.058333333333333334    Train Loss :0.00184407620690763    Test Loss :0.016423001885414124\n",
      "Epoch :0.06666666666666667    Train Loss :0.002202190924435854    Test Loss :0.0066439369693398476\n",
      "Epoch :0.075    Train Loss :0.0016395744169130921    Test Loss :0.005537339951843023\n",
      "Epoch :0.08333333333333333    Train Loss :0.0013724049786105752    Test Loss :0.0064734346233308315\n",
      "Epoch :0.09166666666666666    Train Loss :0.0011908502783626318    Test Loss :0.005312575958669186\n",
      "Epoch :0.1    Train Loss :0.001123901573009789    Test Loss :0.0039687505923211575\n",
      "Epoch :0.10833333333333334    Train Loss :0.0009800418047234416    Test Loss :0.00516355037689209\n",
      "Epoch :0.11666666666666667    Train Loss :0.0008905165013857186    Test Loss :0.0032798112370073795\n",
      "Epoch :0.125    Train Loss :0.000861571985296905    Test Loss :0.004015438724309206\n",
      "Epoch :0.13333333333333333    Train Loss :0.0008568771299906075    Test Loss :0.0036459711845964193\n",
      "Epoch :0.14166666666666666    Train Loss :0.0008193888352252543    Test Loss :0.0034681586548686028\n",
      "Epoch :0.15    Train Loss :0.0008192905224859715    Test Loss :0.003274095943197608\n",
      "Epoch :0.15833333333333333    Train Loss :0.0007633445202372968    Test Loss :0.003182861488312483\n",
      "Epoch :0.16666666666666666    Train Loss :0.0007253236253745854    Test Loss :0.0030864120926707983\n",
      "Epoch :0.175    Train Loss :0.0007163129630498588    Test Loss :0.0027915341779589653\n",
      "Epoch :0.18333333333333332    Train Loss :0.0006886923220008612    Test Loss :0.0027616173028945923\n",
      "Epoch :0.19166666666666668    Train Loss :0.0006713173352181911    Test Loss :0.0027724874671548605\n",
      "Epoch :0.2    Train Loss :0.0006590834818780422    Test Loss :0.0025890262331813574\n",
      "Epoch :0.20833333333333334    Train Loss :0.0006381563725881279    Test Loss :0.0025677012745290995\n",
      "Epoch :0.21666666666666667    Train Loss :0.0006496297428384423    Test Loss :0.002397838979959488\n",
      "Epoch :0.225    Train Loss :0.0006295969942584634    Test Loss :0.0023091291077435017\n",
      "Epoch :0.23333333333333334    Train Loss :0.0006011266959831119    Test Loss :0.0021518925204873085\n",
      "Epoch :0.24166666666666667    Train Loss :0.0006100237951613963    Test Loss :0.0021909724455326796\n",
      "Epoch :0.25    Train Loss :0.0005577549454756081    Test Loss :0.002047449816018343\n",
      "Epoch :0.25833333333333336    Train Loss :0.0005447742296382785    Test Loss :0.0018445203313603997\n",
      "Epoch :0.26666666666666666    Train Loss :0.0005501118139363825    Test Loss :0.0018994470592588186\n",
      "Epoch :0.275    Train Loss :0.0005161904846318066    Test Loss :0.0018898645648732781\n",
      "Epoch :0.2833333333333333    Train Loss :0.0004845343064516783    Test Loss :0.001942432951182127\n",
      "Epoch :0.2916666666666667    Train Loss :0.0005009970045648515    Test Loss :0.0017212110105901957\n",
      "Epoch :0.3    Train Loss :0.0004869677941314876    Test Loss :0.0016393123660236597\n",
      "Epoch :0.30833333333333335    Train Loss :0.0004906673566438258    Test Loss :0.0016249538166448474\n",
      "Epoch :0.31666666666666665    Train Loss :0.00048332306323572993    Test Loss :0.0015463464660570025\n",
      "Epoch :0.325    Train Loss :0.00046782303252257407    Test Loss :0.0015693578170612454\n",
      "Epoch :0.3333333333333333    Train Loss :0.00046400559949688613    Test Loss :0.001559979049488902\n",
      "Epoch :0.3416666666666667    Train Loss :0.00045555862016044557    Test Loss :0.0015010526403784752\n",
      "Epoch :0.35    Train Loss :0.00043826416367664933    Test Loss :0.0016153204487636685\n",
      "Epoch :0.35833333333333334    Train Loss :0.0004285521281417459    Test Loss :0.001440158230252564\n",
      "Epoch :0.36666666666666664    Train Loss :0.00042959253187291324    Test Loss :0.0014684997731819749\n",
      "Epoch :0.375    Train Loss :0.00041797841549851    Test Loss :0.0013377615250647068\n",
      "Epoch :0.38333333333333336    Train Loss :0.00040147898835130036    Test Loss :0.0013328103814274073\n",
      "Epoch :0.39166666666666666    Train Loss :0.0004252913349773735    Test Loss :0.001227218541316688\n",
      "Epoch :0.4    Train Loss :0.0003909110964741558    Test Loss :0.0012847823090851307\n",
      "Epoch :0.4083333333333333    Train Loss :0.00039273276343010366    Test Loss :0.0012337405933067203\n",
      "Epoch :0.4166666666666667    Train Loss :0.00038363735075108707    Test Loss :0.0013077387120574713\n",
      "Epoch :0.425    Train Loss :0.00037517541204579175    Test Loss :0.0012215465540066361\n",
      "Epoch :0.43333333333333335    Train Loss :0.0005609507788904011    Test Loss :0.0017191406805068254\n",
      "Epoch :0.44166666666666665    Train Loss :0.00043372085201554    Test Loss :0.0013632638147100806\n",
      "Epoch :0.45    Train Loss :0.0003688543220050633    Test Loss :0.0012243470409885049\n",
      "Epoch :0.4583333333333333    Train Loss :0.0004199039831291884    Test Loss :0.0011007789289578795\n",
      "Epoch :0.4666666666666667    Train Loss :0.0003593052097130567    Test Loss :0.0012864199234172702\n",
      "Epoch :0.475    Train Loss :0.00038279168074950576    Test Loss :0.0012081227032467723\n",
      "Epoch :0.48333333333333334    Train Loss :0.00036619254387915134    Test Loss :0.0012484474573284388\n",
      "Epoch :0.49166666666666664    Train Loss :0.00034611576120369136    Test Loss :0.0012723576510325074\n",
      "Epoch :0.5    Train Loss :0.000339046266162768    Test Loss :0.0009930505184456706\n",
      "Epoch :0.5083333333333333    Train Loss :0.0004634668002836406    Test Loss :0.0010550800943747163\n",
      "Epoch :0.5166666666666667    Train Loss :0.00033298085327260196    Test Loss :0.0009553100098855793\n",
      "Epoch :0.525    Train Loss :0.0003561419143807143    Test Loss :0.001071099890395999\n",
      "Epoch :0.5333333333333333    Train Loss :0.0003699373628478497    Test Loss :0.0009681310039013624\n",
      "Epoch :0.5416666666666666    Train Loss :0.0003243899263907224    Test Loss :0.0009703093091957271\n",
      "Epoch :0.55    Train Loss :0.0003003669553436339    Test Loss :0.001028616912662983\n",
      "Epoch :0.5583333333333333    Train Loss :0.0003249671426601708    Test Loss :0.0008820531074889004\n",
      "Epoch :0.5666666666666667    Train Loss :0.0003537509765010327    Test Loss :0.00087295490084216\n",
      "Epoch :0.575    Train Loss :0.0010263386648148298    Test Loss :0.002643622225150466\n",
      "Epoch :0.5833333333333334    Train Loss :0.00034230470191687346    Test Loss :0.0010335523402318358\n",
      "Epoch :0.5916666666666667    Train Loss :0.0003452851378824562    Test Loss :0.0015268113929778337\n",
      "Epoch :0.6    Train Loss :0.0003811464994214475    Test Loss :0.0010234590154141188\n",
      "Epoch :0.6083333333333333    Train Loss :0.0003485933120828122    Test Loss :0.0010707232868298888\n",
      "Epoch :0.6166666666666667    Train Loss :0.0003277554642409086    Test Loss :0.000943817023653537\n",
      "Epoch :0.625    Train Loss :0.00031652566394768655    Test Loss :0.0008786176331341267\n",
      "Epoch :0.6333333333333333    Train Loss :0.00031393434619531035    Test Loss :0.0009204524685628712\n",
      "Epoch :0.6416666666666667    Train Loss :0.0003207622794434428    Test Loss :0.0008766971877776086\n",
      "Epoch :0.65    Train Loss :0.0002913678181357682    Test Loss :0.0008271090919151902\n",
      "Epoch :0.6583333333333333    Train Loss :0.0002728918334469199    Test Loss :0.0007528451387770474\n",
      "Epoch :0.6666666666666666    Train Loss :0.0002844804839696735    Test Loss :0.0007511713774874806\n",
      "Epoch :0.675    Train Loss :0.0002757295733317733    Test Loss :0.0006894780090078712\n",
      "Epoch :0.6833333333333333    Train Loss :0.0003445933689363301    Test Loss :0.001132045523263514\n",
      "Epoch :0.6916666666666667    Train Loss :0.0002723661600612104    Test Loss :0.0007540856604464352\n",
      "Epoch :0.7    Train Loss :0.0004923930391669273    Test Loss :0.0010962653905153275\n",
      "Epoch :0.7083333333333334    Train Loss :0.0003404653398320079    Test Loss :0.0008734630537219346\n",
      "Epoch :0.7166666666666667    Train Loss :0.0003013468813151121    Test Loss :0.0007973749889060855\n",
      "Epoch :0.725    Train Loss :0.0002738298790063709    Test Loss :0.0008894441998563707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7333333333333333    Train Loss :0.00029057380743324757    Test Loss :0.0009147245436906815\n",
      "Epoch :0.7416666666666667    Train Loss :0.0002709610271267593    Test Loss :0.0008318840991705656\n",
      "Epoch :0.75    Train Loss :0.0002806770207826048    Test Loss :0.0008219340234063566\n",
      "Epoch :0.7583333333333333    Train Loss :0.00028047492378391325    Test Loss :0.000843594316393137\n",
      "Epoch :0.7666666666666667    Train Loss :0.0004742855380754918    Test Loss :0.0009280722006224096\n",
      "Epoch :0.775    Train Loss :0.0003472892858553678    Test Loss :0.00082063750596717\n",
      "Epoch :0.7833333333333333    Train Loss :0.0002999332791659981    Test Loss :0.0007909173727966845\n",
      "Epoch :0.7916666666666666    Train Loss :0.000253230711678043    Test Loss :0.0007413350394926965\n",
      "Epoch :0.8    Train Loss :0.00027974109980277717    Test Loss :0.0008608879870735109\n",
      "Epoch :0.8083333333333333    Train Loss :0.000254900602158159    Test Loss :0.0008155258256010711\n",
      "Epoch :0.8166666666666667    Train Loss :0.00026452765450812876    Test Loss :0.00066382996737957\n",
      "Epoch :0.825    Train Loss :0.0003028398787137121    Test Loss :0.001060436712577939\n",
      "Epoch :0.8333333333333334    Train Loss :0.0003298733208794147    Test Loss :0.0006994478171691298\n",
      "Epoch :0.8416666666666667    Train Loss :0.00027978461002931    Test Loss :0.0007098077330738306\n",
      "Epoch :0.85    Train Loss :0.000269649870460853    Test Loss :0.0006567852688021958\n",
      "Epoch :0.8583333333333333    Train Loss :0.0002506342134438455    Test Loss :0.0007614647038280964\n",
      "Epoch :0.8666666666666667    Train Loss :0.0002499064721632749    Test Loss :0.0006796063389629126\n",
      "Epoch :0.875    Train Loss :0.00026824165252037346    Test Loss :0.0006659768987447023\n",
      "Epoch :0.8833333333333333    Train Loss :0.0002461911062709987    Test Loss :0.000613384647294879\n",
      "Epoch :0.8916666666666667    Train Loss :0.00031104456866160035    Test Loss :0.0008937494130805135\n",
      "Epoch :0.9    Train Loss :0.0003608497208915651    Test Loss :0.0006630839197896421\n",
      "Epoch :0.9083333333333333    Train Loss :0.00028116448083892465    Test Loss :0.0007398186135105789\n",
      "Epoch :0.9166666666666666    Train Loss :0.00024856632808223367    Test Loss :0.00082658656174317\n",
      "Epoch :0.925    Train Loss :0.00025108736008405685    Test Loss :0.0008578072302043438\n",
      "Epoch :0.9333333333333333    Train Loss :0.000249072298174724    Test Loss :0.0008359474595636129\n",
      "Epoch :0.9416666666666667    Train Loss :0.0002842276298906654    Test Loss :0.0010244464501738548\n",
      "Epoch :0.95    Train Loss :0.0002424331905785948    Test Loss :0.0007431089179590344\n",
      "Epoch :0.9583333333333334    Train Loss :0.00025159597862511873    Test Loss :0.0008344044326804578\n",
      "Epoch :0.9666666666666667    Train Loss :0.00023976493685040623    Test Loss :0.0007185090216808021\n",
      "Epoch :0.975    Train Loss :0.00023628595226909965    Test Loss :0.0006405234453268349\n",
      "Epoch :0.9833333333333333    Train Loss :0.00024401547852903605    Test Loss :0.0006286698044277728\n",
      "Epoch :0.9916666666666667    Train Loss :0.0002409164299024269    Test Loss :0.0006066617206670344\n",
      "Epoch :1.0    Train Loss :0.00024180402397178113    Test Loss :0.0006517875008285046\n",
      "RMSE: 28.913680831030476\n",
      "MAE: 27.06704847929356\n",
      "MAPE: 23.662203946904548%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 100, 'lr': 0.01, 'num_layers': 2}\n",
      "total:  91.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.04277774691581726    Test Loss :0.04066425561904907\n",
      "Epoch :0.016666666666666666    Train Loss :0.06397082656621933    Test Loss :0.3477150797843933\n",
      "Epoch :0.025    Train Loss :0.0479404553771019    Test Loss :0.1699177473783493\n",
      "Epoch :0.03333333333333333    Train Loss :0.04070001468062401    Test Loss :0.23934783041477203\n",
      "Epoch :0.041666666666666664    Train Loss :0.01646597497165203    Test Loss :0.061115335673093796\n",
      "Epoch :0.05    Train Loss :0.01141022052615881    Test Loss :0.008023333735764027\n",
      "Epoch :0.058333333333333334    Train Loss :0.00593619653955102    Test Loss :0.014646709896624088\n",
      "Epoch :0.06666666666666667    Train Loss :0.005290046334266663    Test Loss :0.030029594898223877\n",
      "Epoch :0.075    Train Loss :0.0035549935419112444    Test Loss :0.008825195021927357\n",
      "Epoch :0.08333333333333333    Train Loss :0.0032351829577237368    Test Loss :0.007017115596681833\n",
      "Epoch :0.09166666666666666    Train Loss :0.002546492265537381    Test Loss :0.010207725688815117\n",
      "Epoch :0.1    Train Loss :0.0023019122891128063    Test Loss :0.007937541231513023\n",
      "Epoch :0.10833333333333334    Train Loss :0.0020582007709890604    Test Loss :0.007631376385688782\n",
      "Epoch :0.11666666666666667    Train Loss :0.0018908320926129818    Test Loss :0.005627884063869715\n",
      "Epoch :0.125    Train Loss :0.0017660381272435188    Test Loss :0.0068719047121703625\n",
      "Epoch :0.13333333333333333    Train Loss :0.0017048964509740472    Test Loss :0.007080011069774628\n",
      "Epoch :0.14166666666666666    Train Loss :0.001564960926771164    Test Loss :0.0056406911462545395\n",
      "Epoch :0.15    Train Loss :0.0014550683554261923    Test Loss :0.005035449285060167\n",
      "Epoch :0.15833333333333333    Train Loss :0.0013745689066126943    Test Loss :0.005316224414855242\n",
      "Epoch :0.16666666666666666    Train Loss :0.0013206545263528824    Test Loss :0.004314646124839783\n",
      "Epoch :0.175    Train Loss :0.001220659469254315    Test Loss :0.004561478737741709\n",
      "Epoch :0.18333333333333332    Train Loss :0.0012190163834020495    Test Loss :0.004249501042068005\n",
      "Epoch :0.19166666666666668    Train Loss :0.0012044354807585478    Test Loss :0.004507909528911114\n",
      "Epoch :0.2    Train Loss :0.0011048788437619805    Test Loss :0.004478194285184145\n",
      "Epoch :0.20833333333333334    Train Loss :0.001050780527293682    Test Loss :0.0041062175296247005\n",
      "Epoch :0.21666666666666667    Train Loss :0.0009733928018249571    Test Loss :0.0033711649011820555\n",
      "Epoch :0.225    Train Loss :0.0011081702541559935    Test Loss :0.004666242282837629\n",
      "Epoch :0.23333333333333334    Train Loss :0.0011029341258108616    Test Loss :0.004229612182825804\n",
      "Epoch :0.24166666666666667    Train Loss :0.001010893378406763    Test Loss :0.005377642344683409\n",
      "Epoch :0.25    Train Loss :0.0010027913376688957    Test Loss :0.0034900554455816746\n",
      "Epoch :0.25833333333333336    Train Loss :0.00087450206046924    Test Loss :0.0033732771407812834\n",
      "Epoch :0.26666666666666666    Train Loss :0.000901899766176939    Test Loss :0.0036485870368778706\n",
      "Epoch :0.275    Train Loss :0.0008369751740247011    Test Loss :0.0034114241134375334\n",
      "Epoch :0.2833333333333333    Train Loss :0.0007991476450115442    Test Loss :0.003301771357655525\n",
      "Epoch :0.2916666666666667    Train Loss :0.0007775310077704489    Test Loss :0.0031885462813079357\n",
      "Epoch :0.3    Train Loss :0.0008169193752110004    Test Loss :0.003130215685814619\n",
      "Epoch :0.30833333333333335    Train Loss :0.0007485408568754792    Test Loss :0.0027779629454016685\n",
      "Epoch :0.31666666666666665    Train Loss :0.0007119140354916453    Test Loss :0.0035397489555180073\n",
      "Epoch :0.325    Train Loss :0.001625709468498826    Test Loss :0.003352648578584194\n",
      "Epoch :0.3333333333333333    Train Loss :0.0009109575767070055    Test Loss :0.002945215441286564\n",
      "Epoch :0.3416666666666667    Train Loss :0.0008390509174205363    Test Loss :0.0035182414576411247\n",
      "Epoch :0.35    Train Loss :0.0007067900733090937    Test Loss :0.0032791015692055225\n",
      "Epoch :0.35833333333333334    Train Loss :0.0007648569880984724    Test Loss :0.002981439931318164\n",
      "Epoch :0.36666666666666664    Train Loss :0.0007313097594305873    Test Loss :0.002786366268992424\n",
      "Epoch :0.375    Train Loss :0.0006995139410719275    Test Loss :0.002986700739711523\n",
      "Epoch :0.38333333333333336    Train Loss :0.0006436639232560992    Test Loss :0.0028000888414680958\n",
      "Epoch :0.39166666666666666    Train Loss :0.0006531309336423874    Test Loss :0.002757120644673705\n",
      "Epoch :0.4    Train Loss :0.0006744127022102475    Test Loss :0.002721578348428011\n",
      "Epoch :0.4083333333333333    Train Loss :0.0006558164022862911    Test Loss :0.0030223208013921976\n",
      "Epoch :0.4166666666666667    Train Loss :0.0005868474254384637    Test Loss :0.002634575590491295\n",
      "Epoch :0.425    Train Loss :0.0005812625167891383    Test Loss :0.002765705343335867\n",
      "Epoch :0.43333333333333335    Train Loss :0.0005631347885355353    Test Loss :0.0027770819142460823\n",
      "Epoch :0.44166666666666665    Train Loss :0.0010531590087339282    Test Loss :0.004667625762522221\n",
      "Epoch :0.45    Train Loss :0.001109806471504271    Test Loss :0.00406470475718379\n",
      "Epoch :0.4583333333333333    Train Loss :0.0007477895123884082    Test Loss :0.0027994150295853615\n",
      "Epoch :0.4666666666666667    Train Loss :0.0007818890153430402    Test Loss :0.00282108667306602\n",
      "Epoch :0.475    Train Loss :0.0006542013725265861    Test Loss :0.003558306721970439\n",
      "Epoch :0.48333333333333334    Train Loss :0.0006771274493075907    Test Loss :0.0025191225577145815\n",
      "Epoch :0.49166666666666664    Train Loss :0.0005808189162053168    Test Loss :0.0025493092834949493\n",
      "Epoch :0.5    Train Loss :0.0005931920604780316    Test Loss :0.002828270895406604\n",
      "Epoch :0.5083333333333333    Train Loss :0.0005448833107948303    Test Loss :0.002696548355743289\n",
      "Epoch :0.5166666666666667    Train Loss :0.0005382439121603966    Test Loss :0.002666151151061058\n",
      "Epoch :0.525    Train Loss :0.0005532801733352244    Test Loss :0.0026017932686954737\n",
      "Epoch :0.5333333333333333    Train Loss :0.0005442497204057872    Test Loss :0.0024632399436086416\n",
      "Epoch :0.5416666666666666    Train Loss :0.0005183718167245388    Test Loss :0.002568800700828433\n",
      "Epoch :0.55    Train Loss :0.000514145300257951    Test Loss :0.002443533856421709\n",
      "Epoch :0.5583333333333333    Train Loss :0.0005392369930632412    Test Loss :0.002393504371866584\n",
      "Epoch :0.5666666666666667    Train Loss :0.0005615501431748271    Test Loss :0.002498440444469452\n",
      "Epoch :0.575    Train Loss :0.0005048548919148743    Test Loss :0.0022777754347771406\n",
      "Epoch :0.5833333333333334    Train Loss :0.0006188452243804932    Test Loss :0.0028107904363423586\n",
      "Epoch :0.5916666666666667    Train Loss :0.0005095871165394783    Test Loss :0.0026130755431950092\n",
      "Epoch :0.6    Train Loss :0.0005747138638980687    Test Loss :0.0023690196685492992\n",
      "Epoch :0.6083333333333333    Train Loss :0.0005245460197329521    Test Loss :0.0024666348472237587\n",
      "Epoch :0.6166666666666667    Train Loss :0.0004809290112461895    Test Loss :0.0026391514111310244\n",
      "Epoch :0.625    Train Loss :0.00048008764861151576    Test Loss :0.002350686816498637\n",
      "Epoch :0.6333333333333333    Train Loss :0.0004785797791555524    Test Loss :0.0020809394773095846\n",
      "Epoch :0.6416666666666667    Train Loss :0.0004663769796025008    Test Loss :0.0021301722154021263\n",
      "Epoch :0.65    Train Loss :0.0004495494067668915    Test Loss :0.0022533624432981014\n",
      "Epoch :0.6583333333333333    Train Loss :0.0005668819649145007    Test Loss :0.002504366682842374\n",
      "Epoch :0.6666666666666666    Train Loss :0.00045903970021754503    Test Loss :0.002318129176273942\n",
      "Epoch :0.675    Train Loss :0.0004688740591518581    Test Loss :0.0022846003994345665\n",
      "Epoch :0.6833333333333333    Train Loss :0.00044897067709825933    Test Loss :0.002417192794382572\n",
      "Epoch :0.6916666666666667    Train Loss :0.0005075485678389668    Test Loss :0.0021911454387009144\n",
      "Epoch :0.7    Train Loss :0.0006366357556544244    Test Loss :0.002234541578218341\n",
      "Epoch :0.7083333333333334    Train Loss :0.0004876935563515872    Test Loss :0.0022464015055447817\n",
      "Epoch :0.7166666666666667    Train Loss :0.0005431706085801125    Test Loss :0.0023278393782675266\n",
      "Epoch :0.725    Train Loss :0.0004883999354206026    Test Loss :0.002566860755905509\n",
      "Epoch :0.7333333333333333    Train Loss :0.00046593285514973104    Test Loss :0.0023772914428263903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7416666666666667    Train Loss :0.0004322013119235635    Test Loss :0.002173009095713496\n",
      "Epoch :0.75    Train Loss :0.00045080904965288937    Test Loss :0.0020920501556247473\n",
      "Epoch :0.7583333333333333    Train Loss :0.0005552662187255919    Test Loss :0.002850176068022847\n",
      "Epoch :0.7666666666666667    Train Loss :0.00046406188630498946    Test Loss :0.002465711673721671\n",
      "Epoch :0.775    Train Loss :0.00046025938354432583    Test Loss :0.00210491381585598\n",
      "Epoch :0.7833333333333333    Train Loss :0.00043713892227970064    Test Loss :0.002422875026240945\n",
      "Epoch :0.7916666666666666    Train Loss :0.00042554110405035317    Test Loss :0.002139280317351222\n",
      "Epoch :0.8    Train Loss :0.000457442132756114    Test Loss :0.001998181687667966\n",
      "Epoch :0.8083333333333333    Train Loss :0.0006362698040902615    Test Loss :0.003357337787747383\n",
      "Epoch :0.8166666666666667    Train Loss :0.00044951995369046926    Test Loss :0.0024483695160597563\n",
      "Epoch :0.825    Train Loss :0.0005121823633089662    Test Loss :0.0023783103097230196\n",
      "Epoch :0.8333333333333334    Train Loss :0.00044563962728716433    Test Loss :0.0024216354358941317\n",
      "Epoch :0.8416666666666667    Train Loss :0.0004093900788575411    Test Loss :0.002130677690729499\n",
      "Epoch :0.85    Train Loss :0.00042542576557025313    Test Loss :0.002040258841589093\n",
      "Epoch :0.8583333333333333    Train Loss :0.00040448387153446674    Test Loss :0.0019283288856968284\n",
      "Epoch :0.8666666666666667    Train Loss :0.000482650677440688    Test Loss :0.002404146594926715\n",
      "Epoch :0.875    Train Loss :0.0005154983373358846    Test Loss :0.00255378894507885\n",
      "Epoch :0.8833333333333333    Train Loss :0.00042059499537572265    Test Loss :0.002100966405123472\n",
      "Epoch :0.8916666666666667    Train Loss :0.00042539305286481977    Test Loss :0.0021296096965670586\n",
      "Epoch :0.9    Train Loss :0.00044319359585642815    Test Loss :0.0023093009367585182\n",
      "Epoch :0.9083333333333333    Train Loss :0.00042516743997111917    Test Loss :0.0023339842446148396\n",
      "Epoch :0.9166666666666666    Train Loss :0.0004290323704481125    Test Loss :0.0021155155263841152\n",
      "Epoch :0.925    Train Loss :0.0004206244193483144    Test Loss :0.001941085560247302\n",
      "Epoch :0.9333333333333333    Train Loss :0.0004224102303851396    Test Loss :0.002203762298449874\n",
      "Epoch :0.9416666666666667    Train Loss :0.00038995538488961756    Test Loss :0.0020273663103580475\n",
      "Epoch :0.95    Train Loss :0.00039646393270231783    Test Loss :0.0017865573754534125\n",
      "Epoch :0.9583333333333334    Train Loss :0.00041740588494576514    Test Loss :0.0020785497035831213\n",
      "Epoch :0.9666666666666667    Train Loss :0.0004406581283546984    Test Loss :0.0020182491280138493\n",
      "Epoch :0.975    Train Loss :0.00043365309829823673    Test Loss :0.0021179744508117437\n",
      "Epoch :0.9833333333333333    Train Loss :0.0004615552315954119    Test Loss :0.0022663187701255083\n",
      "Epoch :0.9916666666666667    Train Loss :0.0003985726507380605    Test Loss :0.0022039557807147503\n",
      "Epoch :1.0    Train Loss :0.00040453296969644725    Test Loss :0.0021565898787230253\n",
      "RMSE: 52.35572152031298\n",
      "MAE: 50.94629619458572\n",
      "MAPE: 44.99486185127732%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 100, 'lr': 0.01, 'num_layers': 5}\n",
      "total:  94.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.140053853392601    Test Loss :0.5274045467376709\n",
      "Epoch :0.016666666666666666    Train Loss :0.04309356212615967    Test Loss :0.33360719680786133\n",
      "Epoch :0.025    Train Loss :0.02826204150915146    Test Loss :0.14587000012397766\n",
      "Epoch :0.03333333333333333    Train Loss :0.015770360827445984    Test Loss :0.08871050179004669\n",
      "Epoch :0.041666666666666664    Train Loss :0.009071649052202702    Test Loss :0.03848602995276451\n",
      "Epoch :0.05    Train Loss :0.008984414860606194    Test Loss :0.025577569380402565\n",
      "Epoch :0.058333333333333334    Train Loss :0.00674776267260313    Test Loss :0.03778543323278427\n",
      "Epoch :0.06666666666666667    Train Loss :0.0044427551329135895    Test Loss :0.028158368542790413\n",
      "Epoch :0.075    Train Loss :0.003029654733836651    Test Loss :0.011715841479599476\n",
      "Epoch :0.08333333333333333    Train Loss :0.0027644860092550516    Test Loss :0.009111536666750908\n",
      "Epoch :0.09166666666666666    Train Loss :0.0024402400013059378    Test Loss :0.010438126511871815\n",
      "Epoch :0.1    Train Loss :0.0019474567379802465    Test Loss :0.007725080940872431\n",
      "Epoch :0.10833333333333334    Train Loss :0.0018569208914414048    Test Loss :0.007883955724537373\n",
      "Epoch :0.11666666666666667    Train Loss :0.0016527683474123478    Test Loss :0.007125557865947485\n",
      "Epoch :0.125    Train Loss :0.0015920537989586592    Test Loss :0.0062241763807833195\n",
      "Epoch :0.13333333333333333    Train Loss :0.0015240747015923262    Test Loss :0.005603200756013393\n",
      "Epoch :0.14166666666666666    Train Loss :0.0014096322702243924    Test Loss :0.005173611920326948\n",
      "Epoch :0.15    Train Loss :0.0013547613052651286    Test Loss :0.004824184812605381\n",
      "Epoch :0.15833333333333333    Train Loss :0.0013349571963772178    Test Loss :0.005063850432634354\n",
      "Epoch :0.16666666666666666    Train Loss :0.0012494506081566215    Test Loss :0.004768542014062405\n",
      "Epoch :0.175    Train Loss :0.0012454098323360085    Test Loss :0.0043504205532372\n",
      "Epoch :0.18333333333333332    Train Loss :0.0011995801469311118    Test Loss :0.003888813080266118\n",
      "Epoch :0.19166666666666668    Train Loss :0.0011692973785102367    Test Loss :0.004165176767855883\n",
      "Epoch :0.2    Train Loss :0.0010832311818376184    Test Loss :0.003692464204505086\n",
      "Epoch :0.20833333333333334    Train Loss :0.0010214956710115075    Test Loss :0.0037842891179025173\n",
      "Epoch :0.21666666666666667    Train Loss :0.0010323774768039584    Test Loss :0.0033835293725132942\n",
      "Epoch :0.225    Train Loss :0.0010399656603112817    Test Loss :0.003484602551907301\n",
      "Epoch :0.23333333333333334    Train Loss :0.000996676622889936    Test Loss :0.0035166810266673565\n",
      "Epoch :0.24166666666666667    Train Loss :0.0009124128264375031    Test Loss :0.0033435355871915817\n",
      "Epoch :0.25    Train Loss :0.0009030183427967131    Test Loss :0.003234494710341096\n",
      "Epoch :0.25833333333333336    Train Loss :0.0008817152702249587    Test Loss :0.002993423491716385\n",
      "Epoch :0.26666666666666666    Train Loss :0.0008713050046935678    Test Loss :0.0030865082517266273\n",
      "Epoch :0.275    Train Loss :0.0008380311192013323    Test Loss :0.003020643023774028\n",
      "Epoch :0.2833333333333333    Train Loss :0.0008193753892555833    Test Loss :0.002864088164642453\n",
      "Epoch :0.2916666666666667    Train Loss :0.0007770882220938802    Test Loss :0.003080118913203478\n",
      "Epoch :0.3    Train Loss :0.0007635706569999456    Test Loss :0.0031032131519168615\n",
      "Epoch :0.30833333333333335    Train Loss :0.0007709076162427664    Test Loss :0.0031172202434390783\n",
      "Epoch :0.31666666666666665    Train Loss :0.0007470326963812113    Test Loss :0.0029844329692423344\n",
      "Epoch :0.325    Train Loss :0.0007299369317479432    Test Loss :0.0030505096074193716\n",
      "Epoch :0.3333333333333333    Train Loss :0.0006675058975815773    Test Loss :0.003290454624220729\n",
      "Epoch :0.3416666666666667    Train Loss :0.0007243373547680676    Test Loss :0.0032648013439029455\n",
      "Epoch :0.35    Train Loss :0.0006968089728616178    Test Loss :0.0029588660690933466\n",
      "Epoch :0.35833333333333334    Train Loss :0.0006678252248093486    Test Loss :0.003073783591389656\n",
      "Epoch :0.36666666666666664    Train Loss :0.0006906177732162178    Test Loss :0.0030253964941948652\n",
      "Epoch :0.375    Train Loss :0.0006298665539361537    Test Loss :0.0029419101774692535\n",
      "Epoch :0.38333333333333336    Train Loss :0.0006280226516537368    Test Loss :0.002940045902505517\n",
      "Epoch :0.39166666666666666    Train Loss :0.0005869314190931618    Test Loss :0.003013558918610215\n",
      "Epoch :0.4    Train Loss :0.0006025265320204198    Test Loss :0.0029436328914016485\n",
      "Epoch :0.4083333333333333    Train Loss :0.0006203761440701783    Test Loss :0.002731209620833397\n",
      "Epoch :0.4166666666666667    Train Loss :0.0006054270779713988    Test Loss :0.0028227849397808313\n",
      "Epoch :0.425    Train Loss :0.0005970366764813662    Test Loss :0.002837483072653413\n",
      "Epoch :0.43333333333333335    Train Loss :0.0005742499488405883    Test Loss :0.0029043422546237707\n",
      "Epoch :0.44166666666666665    Train Loss :0.0005393255851231515    Test Loss :0.0030204704962670803\n",
      "Epoch :0.45    Train Loss :0.0007403278141282499    Test Loss :0.003130156546831131\n",
      "Epoch :0.4583333333333333    Train Loss :0.0005875159404240549    Test Loss :0.0026753335259854794\n",
      "Epoch :0.4666666666666667    Train Loss :0.0006272826576605439    Test Loss :0.002697098534554243\n",
      "Epoch :0.475    Train Loss :0.000529130978975445    Test Loss :0.0026634179521352053\n",
      "Epoch :0.48333333333333334    Train Loss :0.0005479020182974637    Test Loss :0.002747584832832217\n",
      "Epoch :0.49166666666666664    Train Loss :0.0005835547344759107    Test Loss :0.002681066282093525\n",
      "Epoch :0.5    Train Loss :0.0005197689752094448    Test Loss :0.0023753733839839697\n",
      "Epoch :0.5083333333333333    Train Loss :0.0005140940193086863    Test Loss :0.002477678470313549\n",
      "Epoch :0.5166666666666667    Train Loss :0.0007433299324475229    Test Loss :0.003024242352694273\n",
      "Epoch :0.525    Train Loss :0.001510327565483749    Test Loss :0.0030783081892877817\n",
      "Epoch :0.5333333333333333    Train Loss :0.0007384665659628808    Test Loss :0.0025984863750636578\n",
      "Epoch :0.5416666666666666    Train Loss :0.0005249511450529099    Test Loss :0.00265075103379786\n",
      "Epoch :0.55    Train Loss :0.0005322305951267481    Test Loss :0.002273697406053543\n",
      "Epoch :0.5583333333333333    Train Loss :0.0004957772325724363    Test Loss :0.0025481076445430517\n",
      "Epoch :0.5666666666666667    Train Loss :0.0005140735884197056    Test Loss :0.002515423111617565\n",
      "Epoch :0.575    Train Loss :0.0004963001701980829    Test Loss :0.002301320433616638\n",
      "Epoch :0.5833333333333334    Train Loss :0.0004957497003488243    Test Loss :0.0023739913012832403\n",
      "Epoch :0.5916666666666667    Train Loss :0.00047404062934219837    Test Loss :0.002356054726988077\n",
      "Epoch :0.6    Train Loss :0.00045642806799151003    Test Loss :0.0020894005428999662\n",
      "Epoch :0.6083333333333333    Train Loss :0.00046604621456936    Test Loss :0.0019958443008363247\n",
      "Epoch :0.6166666666666667    Train Loss :0.00046692718751728535    Test Loss :0.0020238442812114954\n",
      "Epoch :0.625    Train Loss :0.0004950970178470016    Test Loss :0.0020508873276412487\n",
      "Epoch :0.6333333333333333    Train Loss :0.0004661319253500551    Test Loss :0.0018427392933517694\n",
      "Epoch :0.6416666666666667    Train Loss :0.0004529122670646757    Test Loss :0.002091062720865011\n",
      "Epoch :0.65    Train Loss :0.00044109064037911594    Test Loss :0.0019435903523117304\n",
      "Epoch :0.6583333333333333    Train Loss :0.0008454476483166218    Test Loss :0.0030870346818119287\n",
      "Epoch :0.6666666666666666    Train Loss :0.0007734193350188434    Test Loss :0.002287279348820448\n",
      "Epoch :0.675    Train Loss :0.00046028115320950747    Test Loss :0.0023091787006706\n",
      "Epoch :0.6833333333333333    Train Loss :0.000595157325733453    Test Loss :0.0022034645080566406\n",
      "Epoch :0.6916666666666667    Train Loss :0.00043267206638120115    Test Loss :0.0024866322055459023\n",
      "Epoch :0.7    Train Loss :0.00044505708501674235    Test Loss :0.002502585295587778\n",
      "Epoch :0.7083333333333334    Train Loss :0.000450297724455595    Test Loss :0.002312146592885256\n",
      "Epoch :0.7166666666666667    Train Loss :0.00042461833800189197    Test Loss :0.0019733808003365993\n",
      "Epoch :0.725    Train Loss :0.00040614005411043763    Test Loss :0.0019301293650642037\n",
      "Epoch :0.7333333333333333    Train Loss :0.00041843715007416904    Test Loss :0.0019773223903030157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7416666666666667    Train Loss :0.00041294560651294887    Test Loss :0.001860140124335885\n",
      "Epoch :0.75    Train Loss :0.0004130234301555902    Test Loss :0.0018878240371122956\n",
      "Epoch :0.7583333333333333    Train Loss :0.0004154363996349275    Test Loss :0.001822229940444231\n",
      "Epoch :0.7666666666666667    Train Loss :0.00041818656609393656    Test Loss :0.0018176862504333258\n",
      "Epoch :0.775    Train Loss :0.00041750911623239517    Test Loss :0.0017517186934128404\n",
      "Epoch :0.7833333333333333    Train Loss :0.000402179139200598    Test Loss :0.0017527771415188909\n",
      "Epoch :0.7916666666666666    Train Loss :0.0003769045870285481    Test Loss :0.0017531168414279819\n",
      "Epoch :0.8    Train Loss :0.0004081666993442923    Test Loss :0.001806999440304935\n",
      "Epoch :0.8083333333333333    Train Loss :0.0004882989451289177    Test Loss :0.001933768973685801\n",
      "Epoch :0.8166666666666667    Train Loss :0.0005983270239084959    Test Loss :0.0020531448535621166\n",
      "Epoch :0.825    Train Loss :0.0005238518351688981    Test Loss :0.0020781653001904488\n",
      "Epoch :0.8333333333333334    Train Loss :0.0005679639871232212    Test Loss :0.0020632960367947817\n",
      "Epoch :0.8416666666666667    Train Loss :0.0005309886764734983    Test Loss :0.002179019618779421\n",
      "Epoch :0.85    Train Loss :0.0004160438838880509    Test Loss :0.002148101106286049\n",
      "Epoch :0.8583333333333333    Train Loss :0.00037272623740136623    Test Loss :0.0020357188768684864\n",
      "Epoch :0.8666666666666667    Train Loss :0.0003925354976672679    Test Loss :0.0018384845461696386\n",
      "Epoch :0.875    Train Loss :0.00040164534584619105    Test Loss :0.0018078433349728584\n",
      "Epoch :0.8833333333333333    Train Loss :0.0003780059632845223    Test Loss :0.0017223975155502558\n",
      "Epoch :0.8916666666666667    Train Loss :0.0003828209009952843    Test Loss :0.0016087718540802598\n",
      "Epoch :0.9    Train Loss :0.00037174602039158344    Test Loss :0.0017524395370855927\n",
      "Epoch :0.9083333333333333    Train Loss :0.0003887842467520386    Test Loss :0.0016837394796311855\n",
      "Epoch :0.9166666666666666    Train Loss :0.0004215879598632455    Test Loss :0.0016527962870895863\n",
      "Epoch :0.925    Train Loss :0.0003800505364779383    Test Loss :0.0017270405078306794\n",
      "Epoch :0.9333333333333333    Train Loss :0.0003743991255760193    Test Loss :0.0017301016487181187\n",
      "Epoch :0.9416666666666667    Train Loss :0.0003718234656844288    Test Loss :0.0017521638656035066\n",
      "Epoch :0.95    Train Loss :0.0005064663710072637    Test Loss :0.001922543509863317\n",
      "Epoch :0.9583333333333334    Train Loss :0.0005268177483230829    Test Loss :0.0016386955976486206\n",
      "Epoch :0.9666666666666667    Train Loss :0.0004188104358036071    Test Loss :0.0016006823861971498\n",
      "Epoch :0.975    Train Loss :0.0003600665368139744    Test Loss :0.0017808920238167048\n",
      "Epoch :0.9833333333333333    Train Loss :0.0003799395344685763    Test Loss :0.0017668106593191624\n",
      "Epoch :0.9916666666666667    Train Loss :0.0003686950367409736    Test Loss :0.0017456503119319677\n",
      "Epoch :1.0    Train Loss :0.0004272721998859197    Test Loss :0.0016108015552163124\n",
      "RMSE: 9.554325662923638\n",
      "MAE: 7.844784400222921\n",
      "MAPE: 7.188398825819725%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 100, 'lr': 0.02, 'num_layers': 2}\n",
      "total:  97.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.08110085874795914    Test Loss :0.46675366163253784\n",
      "Epoch :0.016666666666666666    Train Loss :0.08612794429063797    Test Loss :0.3859691917896271\n",
      "Epoch :0.025    Train Loss :0.055372122675180435    Test Loss :0.17605817317962646\n",
      "Epoch :0.03333333333333333    Train Loss :0.047909658402204514    Test Loss :0.25964221358299255\n",
      "Epoch :0.041666666666666664    Train Loss :0.04289769381284714    Test Loss :0.22561220824718475\n",
      "Epoch :0.05    Train Loss :0.04111073538661003    Test Loss :0.20803958177566528\n",
      "Epoch :0.058333333333333334    Train Loss :0.040007006376981735    Test Loss :0.2139662802219391\n",
      "Epoch :0.06666666666666667    Train Loss :0.034263093024492264    Test Loss :0.1700143963098526\n",
      "Epoch :0.075    Train Loss :0.017834847792983055    Test Loss :0.057320062071084976\n",
      "Epoch :0.08333333333333333    Train Loss :0.011328323744237423    Test Loss :0.039716511964797974\n",
      "Epoch :0.09166666666666666    Train Loss :0.00837654247879982    Test Loss :0.019559700042009354\n",
      "Epoch :0.1    Train Loss :0.005589441861957312    Test Loss :0.02965879999101162\n",
      "Epoch :0.10833333333333334    Train Loss :0.007113306783139706    Test Loss :0.025699520483613014\n",
      "Epoch :0.11666666666666667    Train Loss :0.0052140336483716965    Test Loss :0.011248908936977386\n",
      "Epoch :0.125    Train Loss :0.005229335743933916    Test Loss :0.012082370929419994\n",
      "Epoch :0.13333333333333333    Train Loss :0.003957530949264765    Test Loss :0.015329832211136818\n",
      "Epoch :0.14166666666666666    Train Loss :0.00365718943066895    Test Loss :0.015698770061135292\n",
      "Epoch :0.15    Train Loss :0.003515698481351137    Test Loss :0.014614451676607132\n",
      "Epoch :0.15833333333333333    Train Loss :0.0029793912544846535    Test Loss :0.011426467448472977\n",
      "Epoch :0.16666666666666666    Train Loss :0.0030085835605859756    Test Loss :0.012190484441816807\n",
      "Epoch :0.175    Train Loss :0.0027770232409238815    Test Loss :0.01028453279286623\n",
      "Epoch :0.18333333333333332    Train Loss :0.0024895675014704466    Test Loss :0.009145574644207954\n",
      "Epoch :0.19166666666666668    Train Loss :0.0025047226808965206    Test Loss :0.011784860864281654\n",
      "Epoch :0.2    Train Loss :0.002336399629712105    Test Loss :0.010951356031000614\n",
      "Epoch :0.20833333333333334    Train Loss :0.002506838645786047    Test Loss :0.009185216389596462\n",
      "Epoch :0.21666666666666667    Train Loss :0.0021334688644856215    Test Loss :0.008614416234195232\n",
      "Epoch :0.225    Train Loss :0.002245148876681924    Test Loss :0.008973303250968456\n",
      "Epoch :0.23333333333333334    Train Loss :0.0020552517380565405    Test Loss :0.00929404329508543\n",
      "Epoch :0.24166666666666667    Train Loss :0.0019716578535735607    Test Loss :0.01051636878401041\n",
      "Epoch :0.25    Train Loss :0.0022509736008942127    Test Loss :0.009312497451901436\n",
      "Epoch :0.25833333333333336    Train Loss :0.002580942353233695    Test Loss :0.011770393699407578\n",
      "Epoch :0.26666666666666666    Train Loss :0.001999834319576621    Test Loss :0.006304020993411541\n",
      "Epoch :0.275    Train Loss :0.002096263226121664    Test Loss :0.008975790813565254\n",
      "Epoch :0.2833333333333333    Train Loss :0.001782825798727572    Test Loss :0.008065718226134777\n",
      "Epoch :0.2916666666666667    Train Loss :0.0017791313584893942    Test Loss :0.009445404633879662\n",
      "Epoch :0.3    Train Loss :0.0017964286962524056    Test Loss :0.007913410663604736\n",
      "Epoch :0.30833333333333335    Train Loss :0.0017964327707886696    Test Loss :0.008700177073478699\n",
      "Epoch :0.31666666666666665    Train Loss :0.0016219926765188575    Test Loss :0.0076821292750537395\n",
      "Epoch :0.325    Train Loss :0.001585090532898903    Test Loss :0.008207570761442184\n",
      "Epoch :0.3333333333333333    Train Loss :0.001588291022926569    Test Loss :0.005866229999810457\n",
      "Epoch :0.3416666666666667    Train Loss :0.002207141602411866    Test Loss :0.009696980938315392\n",
      "Epoch :0.35    Train Loss :0.0017484934069216251    Test Loss :0.00890045054256916\n",
      "Epoch :0.35833333333333334    Train Loss :0.0017460776725783944    Test Loss :0.011427192017436028\n",
      "Epoch :0.36666666666666664    Train Loss :0.0017587008187547326    Test Loss :0.007865036837756634\n",
      "Epoch :0.375    Train Loss :0.0015173929277807474    Test Loss :0.006372465752065182\n",
      "Epoch :0.38333333333333336    Train Loss :0.0015088029904291034    Test Loss :0.006917164660990238\n",
      "Epoch :0.39166666666666666    Train Loss :0.0016595545457676053    Test Loss :0.006493457593023777\n",
      "Epoch :0.4    Train Loss :0.00149962876457721    Test Loss :0.005806485190987587\n",
      "Epoch :0.4083333333333333    Train Loss :0.0014926344156265259    Test Loss :0.007400890812277794\n",
      "Epoch :0.4166666666666667    Train Loss :0.0015047920169308782    Test Loss :0.006022189743816853\n",
      "Epoch :0.425    Train Loss :0.0014206392224878073    Test Loss :0.005980795715004206\n",
      "Epoch :0.43333333333333335    Train Loss :0.0014982017455622554    Test Loss :0.007609936874359846\n",
      "Epoch :0.44166666666666665    Train Loss :0.0015882709994912148    Test Loss :0.0053382194600999355\n",
      "Epoch :0.45    Train Loss :0.0013744920725002885    Test Loss :0.00498472573235631\n",
      "Epoch :0.4583333333333333    Train Loss :0.001357659581117332    Test Loss :0.004620127845555544\n",
      "Epoch :0.4666666666666667    Train Loss :0.0013985894620418549    Test Loss :0.004817208740860224\n",
      "Epoch :0.475    Train Loss :0.001335117151029408    Test Loss :0.00615446362644434\n",
      "Epoch :0.48333333333333334    Train Loss :0.0013740499271079898    Test Loss :0.0063155535608530045\n",
      "Epoch :0.49166666666666664    Train Loss :0.0014090987388044596    Test Loss :0.007403873372823\n",
      "Epoch :0.5    Train Loss :0.0011955449590459466    Test Loss :0.006940433755517006\n",
      "Epoch :0.5083333333333333    Train Loss :0.0012434355448931456    Test Loss :0.006320539861917496\n",
      "Epoch :0.5166666666666667    Train Loss :0.0011859498918056488    Test Loss :0.006718785502016544\n",
      "Epoch :0.525    Train Loss :0.0019382768077775836    Test Loss :0.010592599399387836\n",
      "Epoch :0.5333333333333333    Train Loss :0.0015855062520131469    Test Loss :0.00860143918544054\n",
      "Epoch :0.5416666666666666    Train Loss :0.0011901363031938672    Test Loss :0.0051087187603116035\n",
      "Epoch :0.55    Train Loss :0.0012401975691318512    Test Loss :0.004738757852464914\n",
      "Epoch :0.5583333333333333    Train Loss :0.001338048605248332    Test Loss :0.006699295714497566\n",
      "Epoch :0.5666666666666667    Train Loss :0.0014379261992871761    Test Loss :0.006852434482425451\n",
      "Epoch :0.575    Train Loss :0.0020017698407173157    Test Loss :0.010093309916555882\n",
      "Epoch :0.5833333333333334    Train Loss :0.0015440165298059583    Test Loss :0.009428982622921467\n",
      "Epoch :0.5916666666666667    Train Loss :0.001397833228111267    Test Loss :0.011697407811880112\n",
      "Epoch :0.6    Train Loss :0.001322592725045979    Test Loss :0.011148178949952126\n",
      "Epoch :0.6083333333333333    Train Loss :0.001211257535032928    Test Loss :0.005939933005720377\n",
      "Epoch :0.6166666666666667    Train Loss :0.0011199285509064794    Test Loss :0.005273652262985706\n",
      "Epoch :0.625    Train Loss :0.0010660404805094004    Test Loss :0.00979224406182766\n",
      "Epoch :0.6333333333333333    Train Loss :0.00113832694478333    Test Loss :0.006360875442624092\n",
      "Epoch :0.6416666666666667    Train Loss :0.0010987877612933517    Test Loss :0.009553024545311928\n",
      "Epoch :0.65    Train Loss :0.0010633568745106459    Test Loss :0.010101684369146824\n",
      "Epoch :0.6583333333333333    Train Loss :0.001091948477551341    Test Loss :0.011575169861316681\n",
      "Epoch :0.6666666666666666    Train Loss :0.0010766031919047236    Test Loss :0.004539303947240114\n",
      "Epoch :0.675    Train Loss :0.001510145142674446    Test Loss :0.004578710999339819\n",
      "Epoch :0.6833333333333333    Train Loss :0.0010892028221860528    Test Loss :0.003904914017766714\n",
      "Epoch :0.6916666666666667    Train Loss :0.0010215201182290912    Test Loss :0.004801676142960787\n",
      "Epoch :0.7    Train Loss :0.0010300701251253486    Test Loss :0.009389473125338554\n",
      "Epoch :0.7083333333333334    Train Loss :0.0022143966052681208    Test Loss :0.011333445087075233\n",
      "Epoch :0.7166666666666667    Train Loss :0.0011482853442430496    Test Loss :0.0052801636047661304\n",
      "Epoch :0.725    Train Loss :0.0012494039256125689    Test Loss :0.005307652056217194\n",
      "Epoch :0.7333333333333333    Train Loss :0.0011572063667699695    Test Loss :0.01172148808836937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7416666666666667    Train Loss :0.0011595995165407658    Test Loss :0.00812614057213068\n",
      "Epoch :0.75    Train Loss :0.0010523179080337286    Test Loss :0.01046825386583805\n",
      "Epoch :0.7583333333333333    Train Loss :0.002431797794997692    Test Loss :0.0037820746656507254\n",
      "Epoch :0.7666666666666667    Train Loss :0.001138741266913712    Test Loss :0.0037446029018610716\n",
      "Epoch :0.775    Train Loss :0.001184598309919238    Test Loss :0.008790233172476292\n",
      "Epoch :0.7833333333333333    Train Loss :0.0011771139688789845    Test Loss :0.005796388722956181\n",
      "Epoch :0.7916666666666666    Train Loss :0.001064220443367958    Test Loss :0.004658039193600416\n",
      "Epoch :0.8    Train Loss :0.0010639012325555086    Test Loss :0.004019497893750668\n",
      "Epoch :0.8083333333333333    Train Loss :0.0010010587284341455    Test Loss :0.005815504118800163\n",
      "Epoch :0.8166666666666667    Train Loss :0.0009526495123282075    Test Loss :0.0071066757664084435\n",
      "Epoch :0.825    Train Loss :0.0009590142290107906    Test Loss :0.014796997420489788\n",
      "Epoch :0.8333333333333334    Train Loss :0.001066350843757391    Test Loss :0.008847548626363277\n",
      "Epoch :0.8416666666666667    Train Loss :0.001020491705276072    Test Loss :0.006659951526671648\n",
      "Epoch :0.85    Train Loss :0.005494288634508848    Test Loss :0.013715378940105438\n",
      "Epoch :0.8583333333333333    Train Loss :0.003480414394289255    Test Loss :0.012012537568807602\n",
      "Epoch :0.8666666666666667    Train Loss :0.00309548806399107    Test Loss :0.013370875269174576\n",
      "Epoch :0.875    Train Loss :0.003629008075222373    Test Loss :0.008089881390333176\n",
      "Epoch :0.8833333333333333    Train Loss :0.002337136073037982    Test Loss :0.00383214489556849\n",
      "Epoch :0.8916666666666667    Train Loss :0.0022617720533162355    Test Loss :0.004554726183414459\n",
      "Epoch :0.9    Train Loss :0.0016565899131819606    Test Loss :0.0049203503876924515\n",
      "Epoch :0.9083333333333333    Train Loss :0.0012649245327338576    Test Loss :0.00498040160164237\n",
      "Epoch :0.9166666666666666    Train Loss :0.0011019230587407947    Test Loss :0.004893850535154343\n",
      "Epoch :0.925    Train Loss :0.000997699098661542    Test Loss :0.004717306233942509\n",
      "Epoch :0.9333333333333333    Train Loss :0.0010317056439816952    Test Loss :0.004887070972472429\n",
      "Epoch :0.9416666666666667    Train Loss :0.0009808932663872838    Test Loss :0.004370174836367369\n",
      "Epoch :0.95    Train Loss :0.0009551667608320713    Test Loss :0.004336627200245857\n",
      "Epoch :0.9583333333333334    Train Loss :0.0009115692228078842    Test Loss :0.005059713497757912\n",
      "Epoch :0.9666666666666667    Train Loss :0.0009308998705819249    Test Loss :0.006163864862173796\n",
      "Epoch :0.975    Train Loss :0.0009247781708836555    Test Loss :0.0034053355921059847\n",
      "Epoch :0.9833333333333333    Train Loss :0.0009223559172824025    Test Loss :0.004582327790558338\n",
      "Epoch :0.9916666666666667    Train Loss :0.0009583475766703486    Test Loss :0.006615661550313234\n",
      "Epoch :1.0    Train Loss :0.0008844813564792275    Test Loss :0.006072812248021364\n",
      "RMSE: 50.92492471748128\n",
      "MAE: 47.73798811575808\n",
      "MAPE: 42.28586204190529%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 100, 'lr': 0.02, 'num_layers': 5}\n",
      "total:  100.0\n",
      "Tempo total de execução: 23012.556653261185 segundos\n",
      "RMSE: 8.183677009402915\n",
      "MAE: 6.7002931377400925\n",
      "MAPE: 5.99646917362818\n",
      "{'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 50, 'lr': 0.01, 'num_layers': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3225279933.py:82: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "params_grid = {'num_layers':[2,5],\n",
    "               'hid_size':[50,100],\n",
    "              'lr': [0.01,0.02],\n",
    "               'epochs':[400,600],\n",
    "              'dropout_rate':[0.5,0.8]}\n",
    "grid = ParameterGrid(params_grid)\n",
    "cnt = 0\n",
    "for p in grid:\n",
    "    cnt = cnt+1\n",
    "\n",
    "print('Total Possible Models',cnt)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "model_parameters = pd.DataFrame(columns = ['RMSE','Parameters'])\n",
    "best_rmse = np.inf\n",
    "best_prediction=None\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "for p in grid:\n",
    "    \n",
    "    lstm = LSTM(in_dim = x.shape[-1],\n",
    "                hid_dim = p['hid_size'],\n",
    "                out_dim = x.shape[-1],\n",
    "                num_layers = p['num_layers'], \n",
    "                dropout_rate= p['dropout_rate'])\n",
    "    \n",
    "    optimizer = torch.optim.Adam(lstm.parameters(), lr=p['lr'])\n",
    "    \n",
    "    loss_fun = nn.MSELoss()\n",
    "    \n",
    "    train_loss, test_loss = train_model(lstm,\n",
    "               loss_fun,\n",
    "               optimizer,\n",
    "               train_x,\n",
    "               test_x,\n",
    "               train_y,\n",
    "               test_y,\n",
    "               epochs=p['epochs'])\n",
    "\n",
    "    \n",
    "    # testing the predction model on multiple time series\n",
    "    last_x = train_x[-1].view(entradas)\n",
    "\n",
    "    prediction_val = []\n",
    "\n",
    "    while len(prediction_val)<len(test_y):\n",
    "        prediction = lstm(last_x.view(1,entradas,1))\n",
    "        prediction_val.append(prediction[0,0].item())\n",
    "\n",
    "\n",
    "        ## replace the predicted value in last x\n",
    "        last_x = torch.cat((last_x[1:],prediction[0]))\n",
    "\n",
    "    # plot the result\n",
    "    train_y_cp = scale.inverse_transform(train_y.detach().numpy())\n",
    "    test_y_cp = scale.inverse_transform(test_y.detach().numpy())\n",
    "    prediction_val = scale.inverse_transform(np.asarray(prediction_val).reshape(-1,1))\n",
    "\n",
    "    y_true = test_y_cp\n",
    "    y_pred = prediction_val\n",
    "\n",
    "    RMSE = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    print(f'RMSE: {RMSE}')\n",
    "\n",
    "    MAE = mean_absolute_error(y_true, y_pred)\n",
    "    print(f'MAE: {MAE}')\n",
    "\n",
    "    MAPE = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    print(f'MAPE: {MAPE}%')\n",
    "    \n",
    "    print(f'parametros: {p}')\n",
    "    \n",
    "    if RMSE < best_rmse:\n",
    "        best_rmse = RMSE\n",
    "        best_mae = MAE\n",
    "        best_mape = MAPE\n",
    "        best_prediction = y_pred\n",
    "        best_parameters = p\n",
    "    \n",
    "    model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n",
    "    count += 1\n",
    "    print(\"total: \" ,round(count/cnt,2)*100)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Tempo total de execução: {end_time - start_time} segundos\")\n",
    "parameters = model_parameters.sort_values(by=['RMSE'])\n",
    "parameters = parameters.reset_index(drop=True)\n",
    "print('RMSE:',parameters.loc[0, 'RMSE'])\n",
    "print('MAE:',parameters.loc[0, 'MAE'])\n",
    "print('MAPE:',parameters.loc[0, 'MAPE'])\n",
    "print(parameters.loc[0, 'Parameters'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b3292e",
   "metadata": {},
   "source": [
    "# Treinando o modelo com os parâmetros selecionados prevendo para os dados de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "96267d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_3 = pd.concat([treino, teste, previsao])\n",
    "df_treino3=pd.concat([treino, teste])\n",
    "df_teste=pd.concat([previsao])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a7e4a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "entradas=10\n",
    "x,y,train_x,train_y,test_x,test_y = escalonar(df_3,df_treino3, entradas,scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "04a181da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.06046665459871292    Test Loss :0.32052528858184814\n",
      "Epoch :0.016666666666666666    Train Loss :0.04072355851531029    Test Loss :0.055380597710609436\n",
      "Epoch :0.025    Train Loss :0.016649985685944557    Test Loss :0.026895958930253983\n",
      "Epoch :0.03333333333333333    Train Loss :0.013650776818394661    Test Loss :0.018873585388064384\n",
      "Epoch :0.041666666666666664    Train Loss :0.006007291376590729    Test Loss :0.026177402585744858\n",
      "Epoch :0.05    Train Loss :0.0028589642606675625    Test Loss :0.004122170619666576\n",
      "Epoch :0.058333333333333334    Train Loss :0.0035995393991470337    Test Loss :0.012801063247025013\n",
      "Epoch :0.06666666666666667    Train Loss :0.002964847255498171    Test Loss :0.015974011272192\n",
      "Epoch :0.075    Train Loss :0.0018881431315094233    Test Loss :0.007179036736488342\n",
      "Epoch :0.08333333333333333    Train Loss :0.001712470082566142    Test Loss :0.01076112873852253\n",
      "Epoch :0.09166666666666666    Train Loss :0.0017999932169914246    Test Loss :0.006789221428334713\n",
      "Epoch :0.1    Train Loss :0.0016426363727077842    Test Loss :0.005074911285191774\n",
      "Epoch :0.10833333333333334    Train Loss :0.0013981896918267012    Test Loss :0.007601866498589516\n",
      "Epoch :0.11666666666666667    Train Loss :0.0014012566534802318    Test Loss :0.00575135787948966\n",
      "Epoch :0.125    Train Loss :0.0013202168047428131    Test Loss :0.004974629729986191\n",
      "Epoch :0.13333333333333333    Train Loss :0.0013242692220956087    Test Loss :0.007760217878967524\n",
      "Epoch :0.14166666666666666    Train Loss :0.0012238931376487017    Test Loss :0.0053678336553275585\n",
      "Epoch :0.15    Train Loss :0.001195231219753623    Test Loss :0.0043793004006147385\n",
      "Epoch :0.15833333333333333    Train Loss :0.0011896818177774549    Test Loss :0.004095905460417271\n",
      "Epoch :0.16666666666666666    Train Loss :0.0011460046516731381    Test Loss :0.004297763109207153\n",
      "Epoch :0.175    Train Loss :0.0010780069278553128    Test Loss :0.004059149883687496\n",
      "Epoch :0.18333333333333332    Train Loss :0.0010765857296064496    Test Loss :0.004425975028425455\n",
      "Epoch :0.19166666666666668    Train Loss :0.001066800206899643    Test Loss :0.0034007099457085133\n",
      "Epoch :0.2    Train Loss :0.000992319663055241    Test Loss :0.003944022115319967\n",
      "Epoch :0.20833333333333334    Train Loss :0.00102181825786829    Test Loss :0.0035216372925788164\n",
      "Epoch :0.21666666666666667    Train Loss :0.0009652912267483771    Test Loss :0.0027865611482411623\n",
      "Epoch :0.225    Train Loss :0.0009398823021911085    Test Loss :0.002706053201109171\n",
      "Epoch :0.23333333333333334    Train Loss :0.0009343550773337483    Test Loss :0.002307372633367777\n",
      "Epoch :0.24166666666666667    Train Loss :0.0009063935140147805    Test Loss :0.0023510761093348265\n",
      "Epoch :0.25    Train Loss :0.0008737017051316798    Test Loss :0.002311444841325283\n",
      "Epoch :0.25833333333333336    Train Loss :0.0008621710585430264    Test Loss :0.001912900828756392\n",
      "Epoch :0.26666666666666666    Train Loss :0.0008170261280611157    Test Loss :0.0016748312627896667\n",
      "Epoch :0.275    Train Loss :0.0008124852902255952    Test Loss :0.0016341203590855002\n",
      "Epoch :0.2833333333333333    Train Loss :0.0008101275307126343    Test Loss :0.0019144846592098475\n",
      "Epoch :0.2916666666666667    Train Loss :0.000801474554464221    Test Loss :0.0012785664293915033\n",
      "Epoch :0.3    Train Loss :0.0007797495345585048    Test Loss :0.0015882343286648393\n",
      "Epoch :0.30833333333333335    Train Loss :0.0007631014450453222    Test Loss :0.001283798716031015\n",
      "Epoch :0.31666666666666665    Train Loss :0.0007408428355120122    Test Loss :0.0012788436142727733\n",
      "Epoch :0.325    Train Loss :0.0007163884001784027    Test Loss :0.0015747171128168702\n",
      "Epoch :0.3333333333333333    Train Loss :0.0007466261158697307    Test Loss :0.0016339394496753812\n",
      "Epoch :0.3416666666666667    Train Loss :0.0007117278291843832    Test Loss :0.0010570421582087874\n",
      "Epoch :0.35    Train Loss :0.0007167040021158755    Test Loss :0.0011364558013156056\n",
      "Epoch :0.35833333333333334    Train Loss :0.0007073812885209918    Test Loss :0.001018783776089549\n",
      "Epoch :0.36666666666666664    Train Loss :0.0006867592455819249    Test Loss :0.0011406572302803397\n",
      "Epoch :0.375    Train Loss :0.0006747677689418197    Test Loss :0.0008153038797900081\n",
      "Epoch :0.38333333333333336    Train Loss :0.0006898778956383467    Test Loss :0.001325471792370081\n",
      "Epoch :0.39166666666666666    Train Loss :0.0006967443041503429    Test Loss :0.0011062066769227386\n",
      "Epoch :0.4    Train Loss :0.0006523719639517367    Test Loss :0.0016291615320369601\n",
      "Epoch :0.4083333333333333    Train Loss :0.0006970884278416634    Test Loss :0.0013187697622925043\n",
      "Epoch :0.4166666666666667    Train Loss :0.0006615371676161885    Test Loss :0.0014604624593630433\n",
      "Epoch :0.425    Train Loss :0.0006808182806707919    Test Loss :0.0011821056250482798\n",
      "Epoch :0.43333333333333335    Train Loss :0.0006439487915486097    Test Loss :0.0009465726907365024\n",
      "Epoch :0.44166666666666665    Train Loss :0.0006399432313628495    Test Loss :0.0008968356414698064\n",
      "Epoch :0.45    Train Loss :0.0007552482420578599    Test Loss :0.0015072283567860723\n",
      "Epoch :0.4583333333333333    Train Loss :0.0006819270784035325    Test Loss :0.001127760042436421\n",
      "Epoch :0.4666666666666667    Train Loss :0.0006611602148041129    Test Loss :0.0013325129402801394\n",
      "Epoch :0.475    Train Loss :0.0006403363076969981    Test Loss :0.0012613061117008328\n",
      "Epoch :0.48333333333333334    Train Loss :0.0005977832479402423    Test Loss :0.0009419294656254351\n",
      "Epoch :0.49166666666666664    Train Loss :0.0005765102105215192    Test Loss :0.0008710621041245759\n",
      "Epoch :0.5    Train Loss :0.0008803222444839776    Test Loss :0.0031014049891382456\n",
      "Epoch :0.5083333333333333    Train Loss :0.000668535940349102    Test Loss :0.002086496911942959\n",
      "Epoch :0.5166666666666667    Train Loss :0.0006974406423978508    Test Loss :0.001408343785442412\n",
      "Epoch :0.525    Train Loss :0.0006282939575612545    Test Loss :0.0009746952564455569\n",
      "Epoch :0.5333333333333333    Train Loss :0.0005716945743188262    Test Loss :0.0008667759830132127\n",
      "Epoch :0.5416666666666666    Train Loss :0.0005396417109295726    Test Loss :0.0007641217671334743\n",
      "Epoch :0.55    Train Loss :0.0005658437148667872    Test Loss :0.000751046696677804\n",
      "Epoch :0.5583333333333333    Train Loss :0.0005995089886710048    Test Loss :0.0014234811533242464\n",
      "Epoch :0.5666666666666667    Train Loss :0.0005219354061409831    Test Loss :0.001065167598426342\n",
      "Epoch :0.575    Train Loss :0.0014875343767926097    Test Loss :0.0010217331582680345\n",
      "Epoch :0.5833333333333334    Train Loss :0.0006236563785932958    Test Loss :0.002093055285513401\n",
      "Epoch :0.5916666666666667    Train Loss :0.0007410564576275647    Test Loss :0.001395344384945929\n",
      "Epoch :0.6    Train Loss :0.0005785288522019982    Test Loss :0.000935651536565274\n",
      "Epoch :0.6083333333333333    Train Loss :0.0005569854401983321    Test Loss :0.0009200170752592385\n",
      "Epoch :0.6166666666666667    Train Loss :0.0005680032190866768    Test Loss :0.0009137126035057008\n",
      "Epoch :0.625    Train Loss :0.0005085287848487496    Test Loss :0.0007899561314843595\n",
      "Epoch :0.6333333333333333    Train Loss :0.0005498031969182193    Test Loss :0.000692968547809869\n",
      "Epoch :0.6416666666666667    Train Loss :0.000516481464728713    Test Loss :0.0008408752619288862\n",
      "Epoch :0.65    Train Loss :0.0005144965252839029    Test Loss :0.000707088562194258\n",
      "Epoch :0.6583333333333333    Train Loss :0.0005219147424213588    Test Loss :0.0008760293130762875\n",
      "Epoch :0.6666666666666666    Train Loss :0.0006126817897893488    Test Loss :0.0012773495400324464\n",
      "Epoch :0.675    Train Loss :0.0005669938982464373    Test Loss :0.0008979193517006934\n",
      "Epoch :0.6833333333333333    Train Loss :0.0005256127915345132    Test Loss :0.0008015228668227792\n",
      "Epoch :0.6916666666666667    Train Loss :0.0004877748724538833    Test Loss :0.0007370765670202672\n",
      "Epoch :0.7    Train Loss :0.0005360436043702066    Test Loss :0.0007876629242673516\n",
      "Epoch :0.7083333333333334    Train Loss :0.0004911245778203011    Test Loss :0.0008294585277326405\n",
      "Epoch :0.7166666666666667    Train Loss :0.0005551722133532166    Test Loss :0.0011729341931641102\n",
      "Epoch :0.725    Train Loss :0.0004586084105540067    Test Loss :0.0008103734580799937\n",
      "Epoch :0.7333333333333333    Train Loss :0.0008805447141639888    Test Loss :0.0009737466461956501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7416666666666667    Train Loss :0.0006702456739731133    Test Loss :0.0012978025479242206\n",
      "Epoch :0.75    Train Loss :0.0005782379885204136    Test Loss :0.000617472396697849\n",
      "Epoch :0.7583333333333333    Train Loss :0.0005206386558711529    Test Loss :0.0007747237104922533\n",
      "Epoch :0.7666666666666667    Train Loss :0.0004799655289389193    Test Loss :0.0007517911726608872\n",
      "Epoch :0.775    Train Loss :0.000502035953104496    Test Loss :0.0010893793078139424\n",
      "Epoch :0.7833333333333333    Train Loss :0.0004184497520327568    Test Loss :0.0007507663685828447\n",
      "Epoch :0.7916666666666666    Train Loss :0.0004659223777707666    Test Loss :0.0007696348475292325\n",
      "Epoch :0.8    Train Loss :0.00043606467079371214    Test Loss :0.0008242328185588121\n",
      "Epoch :0.8083333333333333    Train Loss :0.0004923990345560014    Test Loss :0.0011530040064826608\n",
      "Epoch :0.8166666666666667    Train Loss :0.0009321735706180334    Test Loss :0.0009695770568214357\n",
      "Epoch :0.825    Train Loss :0.0005662647890858352    Test Loss :0.0007474686717614532\n",
      "Epoch :0.8333333333333334    Train Loss :0.00042292618309147656    Test Loss :0.0010464130900800228\n",
      "Epoch :0.8416666666666667    Train Loss :0.0004428567481227219    Test Loss :0.0009109212551265955\n",
      "Epoch :0.85    Train Loss :0.000441772019257769    Test Loss :0.00085058732656762\n",
      "Epoch :0.8583333333333333    Train Loss :0.0004604365094564855    Test Loss :0.0007909769774414599\n",
      "Epoch :0.8666666666666667    Train Loss :0.00044071278534829617    Test Loss :0.0006745195714756846\n",
      "Epoch :0.875    Train Loss :0.00039661434129811823    Test Loss :0.0006174855516292155\n",
      "Epoch :0.8833333333333333    Train Loss :0.00038699398282915354    Test Loss :0.0006853356026113033\n",
      "Epoch :0.8916666666666667    Train Loss :0.0006676677730865777    Test Loss :0.002300086198374629\n",
      "Epoch :0.9    Train Loss :0.0004226943710818887    Test Loss :0.0009917541174218059\n",
      "Epoch :0.9083333333333333    Train Loss :0.0006831412320025265    Test Loss :0.0006906773196533322\n",
      "Epoch :0.9166666666666666    Train Loss :0.00040753555367700756    Test Loss :0.0010394444689154625\n",
      "Epoch :0.925    Train Loss :0.0004784963675774634    Test Loss :0.0008119156118482351\n",
      "Epoch :0.9333333333333333    Train Loss :0.0004019940097350627    Test Loss :0.0005952915525995195\n",
      "Epoch :0.9416666666666667    Train Loss :0.0004464275552891195    Test Loss :0.0007998241344466805\n",
      "Epoch :0.95    Train Loss :0.00040193801396526396    Test Loss :0.0005269519751891494\n",
      "Epoch :0.9583333333333334    Train Loss :0.00037107206298969686    Test Loss :0.0006001694709993899\n",
      "Epoch :0.9666666666666667    Train Loss :0.0003592214488890022    Test Loss :0.00046549265971407294\n",
      "Epoch :0.975    Train Loss :0.000364552135579288    Test Loss :0.0006056652055121958\n",
      "Epoch :0.9833333333333333    Train Loss :0.0003882973687723279    Test Loss :0.0005602223100140691\n",
      "Epoch :0.9916666666666667    Train Loss :0.0004963763640262187    Test Loss :0.001157077495008707\n",
      "Epoch :1.0    Train Loss :0.0003509377711452544    Test Loss :0.0005618976429104805\n",
      "RMSE: 3.263600132890495\n",
      "MAE: 2.6623689370935777\n",
      "MAPE: 2.073651335191892%\n",
      "{'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 50, 'lr': 0.01, 'num_layers': 2}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "   \n",
    "lstm = LSTM(in_dim = x.shape[-1],\n",
    "            hid_dim = best_parameters['hid_size'],\n",
    "            out_dim = x.shape[-1],\n",
    "            num_layers =best_parameters['num_layers'], \n",
    "            dropout_rate= best_parameters['dropout_rate'])\n",
    "\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=best_parameters['lr'])\n",
    "\n",
    "loss_fun = nn.MSELoss()\n",
    "\n",
    "train_loss, test_loss = train_model(lstm,\n",
    "           loss_fun,\n",
    "           optimizer,\n",
    "           train_x,\n",
    "           test_x,\n",
    "           train_y,\n",
    "           test_y,\n",
    "           epochs=best_parameters['epochs'])\n",
    "\n",
    "\n",
    "# testing the predction model on multiple time series\n",
    "last_x = train_x[-1].view(entradas)\n",
    "\n",
    "prediction_val = []\n",
    "\n",
    "while len(prediction_val)<len(test_y):\n",
    "    prediction = lstm(last_x.view(1,entradas,1))\n",
    "    prediction_val.append(prediction[0,0].item())\n",
    "\n",
    "\n",
    "    ## replace the predicted value in last x\n",
    "    last_x = torch.cat((last_x[1:],prediction[0]))\n",
    "\n",
    "# plot the result\n",
    "train_y_cp = scale.inverse_transform(train_y.detach().numpy())\n",
    "test_y_cp = scale.inverse_transform(test_y.detach().numpy())\n",
    "prediction_val = scale.inverse_transform(np.asarray(prediction_val).reshape(-1,1))\n",
    "\n",
    "y_true = test_y_cp\n",
    "y_pred = prediction_val\n",
    "\n",
    "RMSE = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "print(f'RMSE: {RMSE}')\n",
    "\n",
    "MAE = mean_absolute_error(y_true, y_pred)\n",
    "print(f'MAE: {MAE}')\n",
    "\n",
    "MAPE = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "print(f'MAPE: {MAPE}%')\n",
    "\n",
    "print(best_parameters)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0e502075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotando os resultados\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Linha da série temporal\n",
    "plt.plot(treino.index, treino['valor'],color='blue', label='Dados de treino')\n",
    "plt.plot(teste.index, teste['valor'],color='green', label='Dados de validação')\n",
    "plt.plot(previsao.index, previsao['valor'],color='red', label='Dados de teste')\n",
    "\n",
    "\n",
    "# Linha das previsões\n",
    "#plt.plot(prev_teste, label='Previsões testes', color='orange')\n",
    "plt.plot(teste.index[2:],best_prediction,label='Previsões de validação',color = 'orange')\n",
    "plt.plot(previsao.index,prediction_val,label='Previsões de teste',color = 'cyan')\n",
    "\n",
    "plt.ylabel('Valores', fontsize=12)\n",
    "plt.xlabel('Datas', fontsize=12)\n",
    "plt.title('Índice Ibovespa diário previsto com o modelo LSTM')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a7f5beaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotando os resultados\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Linha da série temporal\n",
    "#plt.plot(treino.index, treino['valor'],color='blue', label='Dados de treino')\n",
    "plt.plot(teste.index, teste['valor'],color='green', label='Dados de validação')\n",
    "plt.plot(previsao.index, previsao['valor'],color='red', label='Dados de teste')\n",
    "\n",
    "\n",
    "# Linha das previsões\n",
    "#plt.plot(prev_teste, label='Previsões testes', color='orange')\n",
    "plt.plot(teste.index[2:],best_prediction,label='Previsões de validação',color = 'orange')\n",
    "plt.plot(previsao.index,prediction_val,label='Previsões de teste',color = 'cyan')\n",
    "\n",
    "plt.ylabel('Valores', fontsize=12)\n",
    "plt.xlabel('Datas', fontsize=12)\n",
    "plt.title('Índice Ibovespa diário previsto com o modelo LSTM')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837bd1ee",
   "metadata": {},
   "source": [
    "# Eliminando dados irregulares da pandemia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc97d3c6",
   "metadata": {},
   "source": [
    "## Separando os dados "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c502a4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "treino1  = treino.loc['2016-01-01':'2020-02-01']\n",
    "treino2 = treino.loc['2020-07-02':]\n",
    "dados_pand = treino.loc['2020-02-02':'2020-07-01']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e780427",
   "metadata": {},
   "source": [
    "## Retirando os dados entre  02/02/2020 e 01/07/2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b6cb63f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defina o intervalo de datas\n",
    "data_inicio = '2020-02-02'\n",
    "data_fim = '2020-07-01'\n",
    "\n",
    "# Crie as datas dentro do intervalo\n",
    "datas = pd.date_range(start=data_inicio, end=data_fim, freq='D')\n",
    "\n",
    "# Gere valores entre 110 e 120\n",
    "valores = np.linspace(117.597180, 96.234960, num=len(datas))\n",
    "\n",
    "# Crie um DataFrame com as datas e valores\n",
    "dados_pand = pd.DataFrame({'data': datas, 'valor': valores})\n",
    "dados_pand.index = dados_pand['data']\n",
    "dados_pand.drop('data', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7ab6be25",
   "metadata": {},
   "outputs": [],
   "source": [
    "treino_sem_pand = pd.concat([treino1,dados_pand,treino2])\n",
    "df_2 = pd.concat([treino_sem_pand, teste])\n",
    "df_treino=pd.concat([treino_sem_pand])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8c2ec439",
   "metadata": {},
   "outputs": [],
   "source": [
    "entradas=10\n",
    "x,y,train_x,train_y,test_x,test_y = escalonar(df_2,df_treino, entradas,scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00f0e7f",
   "metadata": {},
   "source": [
    "# Treinando o modelo e aplicando o Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "dd4da311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Possible Models 32\n",
      "Epoch :0.0125    Train Loss :0.03594362363219261    Test Loss :0.17013584077358246\n",
      "Epoch :0.025    Train Loss :0.039585042744874954    Test Loss :0.06001850590109825\n",
      "Epoch :0.0375    Train Loss :0.030572840943932533    Test Loss :0.04794837161898613\n",
      "Epoch :0.05    Train Loss :0.017648473381996155    Test Loss :0.014801830984652042\n",
      "Epoch :0.0625    Train Loss :0.007553896401077509    Test Loss :0.014372364617884159\n",
      "Epoch :0.075    Train Loss :0.0034046247601509094    Test Loss :0.004309099167585373\n",
      "Epoch :0.0875    Train Loss :0.0025175302289426327    Test Loss :0.0037321592681109905\n",
      "Epoch :0.1    Train Loss :0.0016446186928078532    Test Loss :0.006790869403630495\n",
      "Epoch :0.1125    Train Loss :0.0020874750334769487    Test Loss :0.0035293796099722385\n",
      "Epoch :0.125    Train Loss :0.0019039951730519533    Test Loss :0.005471232812851667\n",
      "Epoch :0.1375    Train Loss :0.0012703875545412302    Test Loss :0.0026049937587231398\n",
      "Epoch :0.15    Train Loss :0.0010273748775944114    Test Loss :0.0021430635824799538\n",
      "Epoch :0.1625    Train Loss :0.0009362508426420391    Test Loss :0.0028867071960121393\n",
      "Epoch :0.175    Train Loss :0.0010442889761179686    Test Loss :0.0019182434771209955\n",
      "Epoch :0.1875    Train Loss :0.000792025588452816    Test Loss :0.0019523288356140256\n",
      "Epoch :0.2    Train Loss :0.0008191131055355072    Test Loss :0.002148524858057499\n",
      "Epoch :0.2125    Train Loss :0.0007621539989486337    Test Loss :0.0016439275350421667\n",
      "Epoch :0.225    Train Loss :0.0007677483954466879    Test Loss :0.0017702904297038913\n",
      "Epoch :0.2375    Train Loss :0.0007301853038370609    Test Loss :0.001778877223841846\n",
      "Epoch :0.25    Train Loss :0.0006773925269953907    Test Loss :0.0017921153921633959\n",
      "Epoch :0.2625    Train Loss :0.0006819076952524483    Test Loss :0.0016437703743577003\n",
      "Epoch :0.275    Train Loss :0.0006823551957495511    Test Loss :0.0016436829464510083\n",
      "Epoch :0.2875    Train Loss :0.0007039838819764555    Test Loss :0.0015460728900507092\n",
      "Epoch :0.3    Train Loss :0.0006928966613486409    Test Loss :0.001474784454330802\n",
      "Epoch :0.3125    Train Loss :0.0006497795111499727    Test Loss :0.0015084861079230905\n",
      "Epoch :0.325    Train Loss :0.0006338571547530591    Test Loss :0.0014109350740909576\n",
      "Epoch :0.3375    Train Loss :0.0006105321226641536    Test Loss :0.0013474314473569393\n",
      "Epoch :0.35    Train Loss :0.0005960569251328707    Test Loss :0.0013233446516096592\n",
      "Epoch :0.3625    Train Loss :0.0005910480977036059    Test Loss :0.0012957191793248057\n",
      "Epoch :0.375    Train Loss :0.0005691970000043511    Test Loss :0.0012989052338525653\n",
      "Epoch :0.3875    Train Loss :0.0005677106673829257    Test Loss :0.0012606403324753046\n",
      "Epoch :0.4    Train Loss :0.000547085131984204    Test Loss :0.0012884538155049086\n",
      "Epoch :0.4125    Train Loss :0.0005558771663345397    Test Loss :0.0011758391046896577\n",
      "Epoch :0.425    Train Loss :0.0005501924315467477    Test Loss :0.0011555608361959457\n",
      "Epoch :0.4375    Train Loss :0.000544812239240855    Test Loss :0.0011707593221217394\n",
      "Epoch :0.45    Train Loss :0.0005228549707680941    Test Loss :0.0012258726637810469\n",
      "Epoch :0.4625    Train Loss :0.0005414834013208747    Test Loss :0.0011727195233106613\n",
      "Epoch :0.475    Train Loss :0.0005345916142687201    Test Loss :0.0011840970255434513\n",
      "Epoch :0.4875    Train Loss :0.0004997106152586639    Test Loss :0.0010631922632455826\n",
      "Epoch :0.5    Train Loss :0.0005101423594169319    Test Loss :0.001114527927711606\n",
      "Epoch :0.5125    Train Loss :0.0004719372373074293    Test Loss :0.0009489662479609251\n",
      "Epoch :0.525    Train Loss :0.000488795805722475    Test Loss :0.001020188326947391\n",
      "Epoch :0.5375    Train Loss :0.00046311839832924306    Test Loss :0.0010675626108422875\n",
      "Epoch :0.55    Train Loss :0.0004511672887019813    Test Loss :0.001029826933518052\n",
      "Epoch :0.5625    Train Loss :0.0004517186898738146    Test Loss :0.0010187984444200993\n",
      "Epoch :0.575    Train Loss :0.0004764460027217865    Test Loss :0.0009904967155307531\n",
      "Epoch :0.5875    Train Loss :0.00047247568727470934    Test Loss :0.0009648026316426694\n",
      "Epoch :0.6    Train Loss :0.00046740355901420116    Test Loss :0.0009106685174629092\n",
      "Epoch :0.6125    Train Loss :0.00046139347250573337    Test Loss :0.0009415215463377535\n",
      "Epoch :0.625    Train Loss :0.0004530788864940405    Test Loss :0.000854038109537214\n",
      "Epoch :0.6375    Train Loss :0.00039944410673342645    Test Loss :0.0008538616821169853\n",
      "Epoch :0.65    Train Loss :0.00043433113023638725    Test Loss :0.0009393159416504204\n",
      "Epoch :0.6625    Train Loss :0.0004202309937682003    Test Loss :0.0008808299899101257\n",
      "Epoch :0.675    Train Loss :0.0004091024457011372    Test Loss :0.0008199207950383425\n",
      "Epoch :0.6875    Train Loss :0.00042076976387761533    Test Loss :0.0008966276654973626\n",
      "Epoch :0.7    Train Loss :0.0004001287743449211    Test Loss :0.0009027121705003083\n",
      "Epoch :0.7125    Train Loss :0.0003954075218643993    Test Loss :0.0008590575307607651\n",
      "Epoch :0.725    Train Loss :0.00042338576167821884    Test Loss :0.0008319023763760924\n",
      "Epoch :0.7375    Train Loss :0.00041375868022441864    Test Loss :0.0007919681374914944\n",
      "Epoch :0.75    Train Loss :0.0004071725707035512    Test Loss :0.0008187778876163065\n",
      "Epoch :0.7625    Train Loss :0.00039625560748390853    Test Loss :0.0007823173655197024\n",
      "Epoch :0.775    Train Loss :0.0004141180543228984    Test Loss :0.0007665016455575824\n",
      "Epoch :0.7875    Train Loss :0.0003889145446009934    Test Loss :0.0007335920818150043\n",
      "Epoch :0.8    Train Loss :0.00037940419861115515    Test Loss :0.0007445723749697208\n",
      "Epoch :0.8125    Train Loss :0.00038452455191873014    Test Loss :0.0007324647740460932\n",
      "Epoch :0.825    Train Loss :0.0003718994848895818    Test Loss :0.0007823977502994239\n",
      "Epoch :0.8375    Train Loss :0.0003753609489649534    Test Loss :0.0007538276840932667\n",
      "Epoch :0.85    Train Loss :0.00036688055843114853    Test Loss :0.0007710888748988509\n",
      "Epoch :0.8625    Train Loss :0.0003646074328571558    Test Loss :0.000693954061716795\n",
      "Epoch :0.875    Train Loss :0.00038781799958087504    Test Loss :0.0006430092034861445\n",
      "Epoch :0.8875    Train Loss :0.00037034510751254857    Test Loss :0.0007202159031294286\n",
      "Epoch :0.9    Train Loss :0.00038338592275977135    Test Loss :0.0007273759110830724\n",
      "Epoch :0.9125    Train Loss :0.00036594810080714524    Test Loss :0.0006932198302820325\n",
      "Epoch :0.925    Train Loss :0.00035808744723908603    Test Loss :0.0006487848586402833\n",
      "Epoch :0.9375    Train Loss :0.0003566145896911621    Test Loss :0.0007090974249877036\n",
      "Epoch :0.95    Train Loss :0.0003383863950148225    Test Loss :0.0006778060342185199\n",
      "Epoch :0.9625    Train Loss :0.00035554179339669645    Test Loss :0.0006782883428968489\n",
      "Epoch :0.975    Train Loss :0.00034831499215215445    Test Loss :0.0006467629573307931\n",
      "Epoch :0.9875    Train Loss :0.0003352865460328758    Test Loss :0.000634486903436482\n",
      "Epoch :1.0    Train Loss :0.00034674210473895073    Test Loss :0.0006158467731438577\n",
      "RMSE: 8.7485602515805\n",
      "MAE: 7.138902915892425\n",
      "MAPE: 6.431526272002151%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 50, 'lr': 0.01, 'num_layers': 2}\n",
      "total:  3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.05653408542275429    Test Loss :0.14689619839191437\n",
      "Epoch :0.025    Train Loss :0.04934369772672653    Test Loss :0.1112370491027832\n",
      "Epoch :0.0375    Train Loss :0.04814382642507553    Test Loss :0.08482695370912552\n",
      "Epoch :0.05    Train Loss :0.03609111160039902    Test Loss :0.05020870640873909\n",
      "Epoch :0.0625    Train Loss :0.016634901985526085    Test Loss :0.013800128363072872\n",
      "Epoch :0.075    Train Loss :0.022326255217194557    Test Loss :0.05417338013648987\n",
      "Epoch :0.0875    Train Loss :0.015226738527417183    Test Loss :0.008700833655893803\n",
      "Epoch :0.1    Train Loss :0.012042134068906307    Test Loss :0.013300330378115177\n",
      "Epoch :0.1125    Train Loss :0.006029200740158558    Test Loss :0.01272551529109478\n",
      "Epoch :0.125    Train Loss :0.004168832208961248    Test Loss :0.0154656320810318\n",
      "Epoch :0.1375    Train Loss :0.0032488962169736624    Test Loss :0.007783382665365934\n",
      "Epoch :0.15    Train Loss :0.0023398189805448055    Test Loss :0.00539640337228775\n",
      "Epoch :0.1625    Train Loss :0.0018438788829371333    Test Loss :0.003810435999184847\n",
      "Epoch :0.175    Train Loss :0.0017687102081254125    Test Loss :0.0045454478822648525\n",
      "Epoch :0.1875    Train Loss :0.0016217153752222657    Test Loss :0.003647764679044485\n",
      "Epoch :0.2    Train Loss :0.0014059760142117739    Test Loss :0.003069087862968445\n",
      "Epoch :0.2125    Train Loss :0.0013042953796684742    Test Loss :0.002671186113730073\n",
      "Epoch :0.225    Train Loss :0.0012764198472723365    Test Loss :0.002519001718610525\n",
      "Epoch :0.2375    Train Loss :0.0011922806734219193    Test Loss :0.002742413431406021\n",
      "Epoch :0.25    Train Loss :0.0011337387841194868    Test Loss :0.0028596362099051476\n",
      "Epoch :0.2625    Train Loss :0.0011029093293473125    Test Loss :0.002582274377346039\n",
      "Epoch :0.275    Train Loss :0.0010600194800645113    Test Loss :0.0023321835324168205\n",
      "Epoch :0.2875    Train Loss :0.0010196729563176632    Test Loss :0.0025867365766316652\n",
      "Epoch :0.3    Train Loss :0.0010082983644679189    Test Loss :0.0023934710770845413\n",
      "Epoch :0.3125    Train Loss :0.001043203053995967    Test Loss :0.0023860547225922346\n",
      "Epoch :0.325    Train Loss :0.0009735714411363006    Test Loss :0.002281330758705735\n",
      "Epoch :0.3375    Train Loss :0.0008761307108215988    Test Loss :0.0023009367287158966\n",
      "Epoch :0.35    Train Loss :0.000926262466236949    Test Loss :0.0022215612698346376\n",
      "Epoch :0.3625    Train Loss :0.0009184960508719087    Test Loss :0.0022236129734665155\n",
      "Epoch :0.375    Train Loss :0.0008763562655076385    Test Loss :0.0022763716988265514\n",
      "Epoch :0.3875    Train Loss :0.000831949058920145    Test Loss :0.002144475234672427\n",
      "Epoch :0.4    Train Loss :0.0008538608672097325    Test Loss :0.0021350299939513206\n",
      "Epoch :0.4125    Train Loss :0.000784228672273457    Test Loss :0.002141994656994939\n",
      "Epoch :0.425    Train Loss :0.0007810742245055735    Test Loss :0.0021534012630581856\n",
      "Epoch :0.4375    Train Loss :0.0008064850699156523    Test Loss :0.002180340001359582\n",
      "Epoch :0.45    Train Loss :0.000765399367082864    Test Loss :0.0022483980283141136\n",
      "Epoch :0.4625    Train Loss :0.0007457927567884326    Test Loss :0.0020548198372125626\n",
      "Epoch :0.475    Train Loss :0.0007246592431329191    Test Loss :0.002132575260475278\n",
      "Epoch :0.4875    Train Loss :0.0007099134963937104    Test Loss :0.002163538010790944\n",
      "Epoch :0.5    Train Loss :0.0006882931338623166    Test Loss :0.0020746029913425446\n",
      "Epoch :0.5125    Train Loss :0.0006856567924842238    Test Loss :0.0020329863764345646\n",
      "Epoch :0.525    Train Loss :0.0007307920604944229    Test Loss :0.00226109242066741\n",
      "Epoch :0.5375    Train Loss :0.0006631856085732579    Test Loss :0.002156982896849513\n",
      "Epoch :0.55    Train Loss :0.0007090670987963676    Test Loss :0.0021497083362191916\n",
      "Epoch :0.5625    Train Loss :0.0006627091788686812    Test Loss :0.0021238920744508505\n",
      "Epoch :0.575    Train Loss :0.0006764064892195165    Test Loss :0.0021780591923743486\n",
      "Epoch :0.5875    Train Loss :0.0009299216908402741    Test Loss :0.0024808829184621572\n",
      "Epoch :0.6    Train Loss :0.0006703079561702907    Test Loss :0.0023890475276857615\n",
      "Epoch :0.6125    Train Loss :0.0006142043275758624    Test Loss :0.001948943710885942\n",
      "Epoch :0.625    Train Loss :0.0006527159712277353    Test Loss :0.002015570644289255\n",
      "Epoch :0.6375    Train Loss :0.0006147713284008205    Test Loss :0.001887379214167595\n",
      "Epoch :0.65    Train Loss :0.0006286251009441912    Test Loss :0.0020011193118989468\n",
      "Epoch :0.6625    Train Loss :0.0005935016670264304    Test Loss :0.0018892184598371387\n",
      "Epoch :0.675    Train Loss :0.0006544756470248103    Test Loss :0.0022333518136292696\n",
      "Epoch :0.6875    Train Loss :0.0006044369656592607    Test Loss :0.002214268781244755\n",
      "Epoch :0.7    Train Loss :0.0005544006708078086    Test Loss :0.001992841949686408\n",
      "Epoch :0.7125    Train Loss :0.0005676826112903655    Test Loss :0.0019732522778213024\n",
      "Epoch :0.725    Train Loss :0.0006147801759652793    Test Loss :0.002126450650393963\n",
      "Epoch :0.7375    Train Loss :0.0005694630090147257    Test Loss :0.002208050573244691\n",
      "Epoch :0.75    Train Loss :0.0006182056968100369    Test Loss :0.002176550216972828\n",
      "Epoch :0.7625    Train Loss :0.0005257516750134528    Test Loss :0.0019402101170271635\n",
      "Epoch :0.775    Train Loss :0.0005555770476348698    Test Loss :0.0018781098769977689\n",
      "Epoch :0.7875    Train Loss :0.0005043408018536866    Test Loss :0.0018958139698952436\n",
      "Epoch :0.8    Train Loss :0.0005271537229418755    Test Loss :0.002035376615822315\n",
      "Epoch :0.8125    Train Loss :0.000580942549277097    Test Loss :0.002312066499143839\n",
      "Epoch :0.825    Train Loss :0.0006714827613905072    Test Loss :0.0021014604717493057\n",
      "Epoch :0.8375    Train Loss :0.0005615488043986261    Test Loss :0.0019572500605136156\n",
      "Epoch :0.85    Train Loss :0.0005703045753762126    Test Loss :0.0020817469339817762\n",
      "Epoch :0.8625    Train Loss :0.0005591493099927902    Test Loss :0.0020568666514009237\n",
      "Epoch :0.875    Train Loss :0.0005462827393785119    Test Loss :0.00200677034445107\n",
      "Epoch :0.8875    Train Loss :0.0005231199902482331    Test Loss :0.0020038404036313295\n",
      "Epoch :0.9    Train Loss :0.0005108124460093677    Test Loss :0.0018937261775135994\n",
      "Epoch :0.9125    Train Loss :0.0008875261410139501    Test Loss :0.002167746890336275\n",
      "Epoch :0.925    Train Loss :0.0006028268835507333    Test Loss :0.0021058102138340473\n",
      "Epoch :0.9375    Train Loss :0.000544081733096391    Test Loss :0.0018534624250605702\n",
      "Epoch :0.95    Train Loss :0.000522713060490787    Test Loss :0.0018345472635701299\n",
      "Epoch :0.9625    Train Loss :0.0005593705573119223    Test Loss :0.001845749793574214\n",
      "Epoch :0.975    Train Loss :0.00048188038635998964    Test Loss :0.0017789886333048344\n",
      "Epoch :0.9875    Train Loss :0.00047240572166629136    Test Loss :0.0018885936588048935\n",
      "Epoch :1.0    Train Loss :0.0004569381126202643    Test Loss :0.0017849380383267999\n",
      "RMSE: 8.94659467477468\n",
      "MAE: 7.352975333997815\n",
      "MAPE: 6.706147313825051%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 50, 'lr': 0.01, 'num_layers': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  6.0\n",
      "Epoch :0.0125    Train Loss :0.04939766228199005    Test Loss :0.03763565793633461\n",
      "Epoch :0.025    Train Loss :0.025614850223064423    Test Loss :0.014711039140820503\n",
      "Epoch :0.0375    Train Loss :0.0030146553181111813    Test Loss :0.018456298857927322\n",
      "Epoch :0.05    Train Loss :0.007387044373899698    Test Loss :0.008337889797985554\n",
      "Epoch :0.0625    Train Loss :0.0020154668018221855    Test Loss :0.00368223013356328\n",
      "Epoch :0.075    Train Loss :0.003395803039893508    Test Loss :0.009612425230443478\n",
      "Epoch :0.0875    Train Loss :0.0015444260789081454    Test Loss :0.0037692950572818518\n",
      "Epoch :0.1    Train Loss :0.0015339464880526066    Test Loss :0.002784941578283906\n",
      "Epoch :0.1125    Train Loss :0.0013968133134767413    Test Loss :0.0026647266931831837\n",
      "Epoch :0.125    Train Loss :0.0011176865082234144    Test Loss :0.0027703370433300734\n",
      "Epoch :0.1375    Train Loss :0.000994852394796908    Test Loss :0.0024494777899235487\n",
      "Epoch :0.15    Train Loss :0.000894848199095577    Test Loss :0.0021512298844754696\n",
      "Epoch :0.1625    Train Loss :0.000876614183653146    Test Loss :0.002182039665058255\n",
      "Epoch :0.175    Train Loss :0.0007621232070960104    Test Loss :0.0021888259798288345\n",
      "Epoch :0.1875    Train Loss :0.0006956011638976634    Test Loss :0.0017309015383943915\n",
      "Epoch :0.2    Train Loss :0.0006759396055713296    Test Loss :0.0016522601945325732\n",
      "Epoch :0.2125    Train Loss :0.0006438120035454631    Test Loss :0.00155021867249161\n",
      "Epoch :0.225    Train Loss :0.0005819392972625792    Test Loss :0.0014258009614422917\n",
      "Epoch :0.2375    Train Loss :0.0005528319743461907    Test Loss :0.00130281294696033\n",
      "Epoch :0.25    Train Loss :0.0005801186780445278    Test Loss :0.0012091147946193814\n",
      "Epoch :0.2625    Train Loss :0.0005719317705370486    Test Loss :0.0011145406169816852\n",
      "Epoch :0.275    Train Loss :0.000520762987434864    Test Loss :0.0011473518097773194\n",
      "Epoch :0.2875    Train Loss :0.0004910380812361836    Test Loss :0.0011154207168146968\n",
      "Epoch :0.3    Train Loss :0.0005259116878733039    Test Loss :0.0010504189413040876\n",
      "Epoch :0.3125    Train Loss :0.0005123013979755342    Test Loss :0.0010408801026642323\n",
      "Epoch :0.325    Train Loss :0.0004669653426390141    Test Loss :0.0009707419667392969\n",
      "Epoch :0.3375    Train Loss :0.0004133682232350111    Test Loss :0.0009461904410272837\n",
      "Epoch :0.35    Train Loss :0.0004338975122664124    Test Loss :0.0008520676055923104\n",
      "Epoch :0.3625    Train Loss :0.0004480906354729086    Test Loss :0.000895264558494091\n",
      "Epoch :0.375    Train Loss :0.00041910557774826884    Test Loss :0.0008144684252329171\n",
      "Epoch :0.3875    Train Loss :0.0004198308160994202    Test Loss :0.0008076040539890528\n",
      "Epoch :0.4    Train Loss :0.00039895158261060715    Test Loss :0.0007813192787580192\n",
      "Epoch :0.4125    Train Loss :0.0003635442990344018    Test Loss :0.0007430549594573677\n",
      "Epoch :0.425    Train Loss :0.00034191139275208116    Test Loss :0.0007004502695053816\n",
      "Epoch :0.4375    Train Loss :0.00037609043647535145    Test Loss :0.00065019162138924\n",
      "Epoch :0.45    Train Loss :0.00035382184432819486    Test Loss :0.000659024459309876\n",
      "Epoch :0.4625    Train Loss :0.00047786522191017866    Test Loss :0.0011627950007095933\n",
      "Epoch :0.475    Train Loss :0.0003908922371920198    Test Loss :0.001259682234376669\n",
      "Epoch :0.4875    Train Loss :0.0004451505665201694    Test Loss :0.0007143752882257104\n",
      "Epoch :0.5    Train Loss :0.0004223524883855134    Test Loss :0.000773160601966083\n",
      "Epoch :0.5125    Train Loss :0.0003702806425280869    Test Loss :0.0008174250833690166\n",
      "Epoch :0.525    Train Loss :0.00034456304274499416    Test Loss :0.0006765836733393371\n",
      "Epoch :0.5375    Train Loss :0.0003122910566162318    Test Loss :0.0006877919076941907\n",
      "Epoch :0.55    Train Loss :0.0003811297647189349    Test Loss :0.000698673480655998\n",
      "Epoch :0.5625    Train Loss :0.0003227937559131533    Test Loss :0.0006059552542865276\n",
      "Epoch :0.575    Train Loss :0.0003189673298038542    Test Loss :0.0006249125581234694\n",
      "Epoch :0.5875    Train Loss :0.0003188253322150558    Test Loss :0.0006275601335801184\n",
      "Epoch :0.6    Train Loss :0.00036004220601171255    Test Loss :0.0005701776244677603\n",
      "Epoch :0.6125    Train Loss :0.0003048952203243971    Test Loss :0.0005374141037464142\n",
      "Epoch :0.625    Train Loss :0.0003103321068920195    Test Loss :0.0005485053989104927\n",
      "Epoch :0.6375    Train Loss :0.00030306269763968885    Test Loss :0.0006899840664118528\n",
      "Epoch :0.65    Train Loss :0.002805519150570035    Test Loss :0.0034269236493855715\n",
      "Epoch :0.6625    Train Loss :0.0005090665654279292    Test Loss :0.0024913076777011156\n",
      "Epoch :0.675    Train Loss :0.0005727046518586576    Test Loss :0.0009835462551563978\n",
      "Epoch :0.6875    Train Loss :0.0005596928531304002    Test Loss :0.0009726145653985441\n",
      "Epoch :0.7    Train Loss :0.0004697201366070658    Test Loss :0.0011554945958778262\n",
      "Epoch :0.7125    Train Loss :0.00043579094926826656    Test Loss :0.000827525625936687\n",
      "Epoch :0.725    Train Loss :0.00034774045343510807    Test Loss :0.0007965710246935487\n",
      "Epoch :0.7375    Train Loss :0.0003428175114095211    Test Loss :0.0007626631995663047\n",
      "Epoch :0.75    Train Loss :0.00032041038502939045    Test Loss :0.0007356872665695846\n",
      "Epoch :0.7625    Train Loss :0.00032697213464416564    Test Loss :0.000751421379391104\n",
      "Epoch :0.775    Train Loss :0.0003132482524961233    Test Loss :0.0006460373406298459\n",
      "Epoch :0.7875    Train Loss :0.0003249995643272996    Test Loss :0.0006499934243038297\n",
      "Epoch :0.8    Train Loss :0.0002931726339738816    Test Loss :0.0006017599371261895\n",
      "Epoch :0.8125    Train Loss :0.00030324928229674697    Test Loss :0.0005838688812218606\n",
      "Epoch :0.825    Train Loss :0.0002988829801324755    Test Loss :0.0006291479803621769\n",
      "Epoch :0.8375    Train Loss :0.00028303966973908246    Test Loss :0.0005409760633483529\n",
      "Epoch :0.85    Train Loss :0.0002872299519367516    Test Loss :0.000535020895767957\n",
      "Epoch :0.8625    Train Loss :0.00027280545327812433    Test Loss :0.0005271935369819403\n",
      "Epoch :0.875    Train Loss :0.000273397978162393    Test Loss :0.0005286200903356075\n",
      "Epoch :0.8875    Train Loss :0.0002759367926046252    Test Loss :0.0005332388100214303\n",
      "Epoch :0.9    Train Loss :0.0002673818089533597    Test Loss :0.0004800607275683433\n",
      "Epoch :0.9125    Train Loss :0.0002744167286437005    Test Loss :0.0005107541219331324\n",
      "Epoch :0.925    Train Loss :0.0002651609538588673    Test Loss :0.00044853685540147126\n",
      "Epoch :0.9375    Train Loss :0.0002553914673626423    Test Loss :0.0004757474234793335\n",
      "Epoch :0.95    Train Loss :0.00025470665423199534    Test Loss :0.00045724029769189656\n",
      "Epoch :0.9625    Train Loss :0.0002588467614259571    Test Loss :0.0004960710648447275\n",
      "Epoch :0.975    Train Loss :0.00024884651065804064    Test Loss :0.0004443138313945383\n",
      "Epoch :0.9875    Train Loss :0.00027252337895333767    Test Loss :0.00044196267845109105\n",
      "Epoch :1.0    Train Loss :0.00027962628519162536    Test Loss :0.0004496412875596434\n",
      "RMSE: 11.056905607027568\n",
      "MAE: 9.158278161206209\n",
      "MAPE: 8.386929812103684%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 50, 'lr': 0.02, 'num_layers': 2}\n",
      "total:  9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.06721712648868561    Test Loss :0.1514224112033844\n",
      "Epoch :0.025    Train Loss :0.05159275233745575    Test Loss :0.06836704909801483\n",
      "Epoch :0.0375    Train Loss :0.04924120381474495    Test Loss :0.10098650306463242\n",
      "Epoch :0.05    Train Loss :0.04250720143318176    Test Loss :0.0713830292224884\n",
      "Epoch :0.0625    Train Loss :0.04927680641412735    Test Loss :0.046652235090732574\n",
      "Epoch :0.075    Train Loss :0.03569352626800537    Test Loss :0.1059398427605629\n",
      "Epoch :0.0875    Train Loss :0.013736261054873466    Test Loss :0.016429712995886803\n",
      "Epoch :0.1    Train Loss :0.01647774875164032    Test Loss :0.022565575316548347\n",
      "Epoch :0.1125    Train Loss :0.008424091152846813    Test Loss :0.011709615588188171\n",
      "Epoch :0.125    Train Loss :0.007693012244999409    Test Loss :0.022413011640310287\n",
      "Epoch :0.1375    Train Loss :0.005232406780123711    Test Loss :0.017316477373242378\n",
      "Epoch :0.15    Train Loss :0.004205293022096157    Test Loss :0.0079776206985116\n",
      "Epoch :0.1625    Train Loss :0.003194519318640232    Test Loss :0.008335897698998451\n",
      "Epoch :0.175    Train Loss :0.002801361959427595    Test Loss :0.008135180920362473\n",
      "Epoch :0.1875    Train Loss :0.0025946758687496185    Test Loss :0.0058428095653653145\n",
      "Epoch :0.2    Train Loss :0.002432335866615176    Test Loss :0.005280869547277689\n",
      "Epoch :0.2125    Train Loss :0.0021199332550168037    Test Loss :0.004634182434529066\n",
      "Epoch :0.225    Train Loss :0.0019931308925151825    Test Loss :0.0036761341616511345\n",
      "Epoch :0.2375    Train Loss :0.0019556251354515553    Test Loss :0.0037168473936617374\n",
      "Epoch :0.25    Train Loss :0.0018089640652760863    Test Loss :0.0036064949817955494\n",
      "Epoch :0.2625    Train Loss :0.0015824435977265239    Test Loss :0.003367694793269038\n",
      "Epoch :0.275    Train Loss :0.001590785221196711    Test Loss :0.0030537680722773075\n",
      "Epoch :0.2875    Train Loss :0.0016413297271355987    Test Loss :0.0027746292762458324\n",
      "Epoch :0.3    Train Loss :0.0014733887510374188    Test Loss :0.003255212213844061\n",
      "Epoch :0.3125    Train Loss :0.0014625267358496785    Test Loss :0.0027798237279057503\n",
      "Epoch :0.325    Train Loss :0.001392159378156066    Test Loss :0.0028430179227143526\n",
      "Epoch :0.3375    Train Loss :0.0013635331997647882    Test Loss :0.0026107903104275465\n",
      "Epoch :0.35    Train Loss :0.0012428207555785775    Test Loss :0.0025240147951990366\n",
      "Epoch :0.3625    Train Loss :0.001304890844039619    Test Loss :0.002703453181311488\n",
      "Epoch :0.375    Train Loss :0.0012702944222837687    Test Loss :0.002451764652505517\n",
      "Epoch :0.3875    Train Loss :0.0012028507189825177    Test Loss :0.0025381867308169603\n",
      "Epoch :0.4    Train Loss :0.0011507358867675066    Test Loss :0.0026204290334135294\n",
      "Epoch :0.4125    Train Loss :0.0011511117918416858    Test Loss :0.0025006181094795465\n",
      "Epoch :0.425    Train Loss :0.0011655521811917424    Test Loss :0.0024470698554068804\n",
      "Epoch :0.4375    Train Loss :0.0011476986110210419    Test Loss :0.002453149063512683\n",
      "Epoch :0.45    Train Loss :0.0010936929611489177    Test Loss :0.002600309671834111\n",
      "Epoch :0.4625    Train Loss :0.0011350936256349087    Test Loss :0.0025729043409228325\n",
      "Epoch :0.475    Train Loss :0.0009824680164456367    Test Loss :0.002454678062349558\n",
      "Epoch :0.4875    Train Loss :0.0010941874934360385    Test Loss :0.002559215994551778\n",
      "Epoch :0.5    Train Loss :0.001104858354665339    Test Loss :0.0025522897485643625\n",
      "Epoch :0.5125    Train Loss :0.001007210579700768    Test Loss :0.0024080085568130016\n",
      "Epoch :0.525    Train Loss :0.0009707411518320441    Test Loss :0.0023636608384549618\n",
      "Epoch :0.5375    Train Loss :0.0009916716953739524    Test Loss :0.002217644825577736\n",
      "Epoch :0.55    Train Loss :0.0009302953258156776    Test Loss :0.0025007931981235743\n",
      "Epoch :0.5625    Train Loss :0.0009371144697070122    Test Loss :0.0023719121236354113\n",
      "Epoch :0.575    Train Loss :0.0009974491549655795    Test Loss :0.0025461798068135977\n",
      "Epoch :0.5875    Train Loss :0.0009404636803083122    Test Loss :0.0023464192636311054\n",
      "Epoch :0.6    Train Loss :0.000897057238034904    Test Loss :0.0024979154113680124\n",
      "Epoch :0.6125    Train Loss :0.0010271263308823109    Test Loss :0.0024487641640007496\n",
      "Epoch :0.625    Train Loss :0.0008625914924778044    Test Loss :0.0022633238695561886\n",
      "Epoch :0.6375    Train Loss :0.0008153796661645174    Test Loss :0.0022628491278737783\n",
      "Epoch :0.65    Train Loss :0.0008415678748860955    Test Loss :0.002315027639269829\n",
      "Epoch :0.6625    Train Loss :0.000811264559160918    Test Loss :0.0023485501296818256\n",
      "Epoch :0.675    Train Loss :0.0008353859302587807    Test Loss :0.0024164686910808086\n",
      "Epoch :0.6875    Train Loss :0.0007822426850907505    Test Loss :0.0023626857437193394\n",
      "Epoch :0.7    Train Loss :0.0009115894208662212    Test Loss :0.0024264014791697264\n",
      "Epoch :0.7125    Train Loss :0.0008305221563205123    Test Loss :0.002471703104674816\n",
      "Epoch :0.725    Train Loss :0.0007807571673765779    Test Loss :0.0022847303189337254\n",
      "Epoch :0.7375    Train Loss :0.0007365441997535527    Test Loss :0.0024732002057135105\n",
      "Epoch :0.75    Train Loss :0.0007304486352950335    Test Loss :0.002644531661644578\n",
      "Epoch :0.7625    Train Loss :0.0008114583324640989    Test Loss :0.0026462418027222157\n",
      "Epoch :0.775    Train Loss :0.000797051121480763    Test Loss :0.0022603804245591164\n",
      "Epoch :0.7875    Train Loss :0.0007839562022127211    Test Loss :0.0023052149917930365\n",
      "Epoch :0.8    Train Loss :0.0007714088424108922    Test Loss :0.002308779861778021\n",
      "Epoch :0.8125    Train Loss :0.0007168439333327115    Test Loss :0.0023640852887183428\n",
      "Epoch :0.825    Train Loss :0.0007656302768737078    Test Loss :0.0021987701766192913\n",
      "Epoch :0.8375    Train Loss :0.0010243571596220136    Test Loss :0.002400320488959551\n",
      "Epoch :0.85    Train Loss :0.0007881077472120523    Test Loss :0.0024483574088662863\n",
      "Epoch :0.8625    Train Loss :0.0007227734313346446    Test Loss :0.0022108161356300116\n",
      "Epoch :0.875    Train Loss :0.0006963096675463021    Test Loss :0.0020697484724223614\n",
      "Epoch :0.8875    Train Loss :0.0008415657794103026    Test Loss :0.002496978035196662\n",
      "Epoch :0.9    Train Loss :0.0007160382810980082    Test Loss :0.0024157545994967222\n",
      "Epoch :0.9125    Train Loss :0.0006660109502263367    Test Loss :0.0021865866146981716\n",
      "Epoch :0.925    Train Loss :0.0006874127429910004    Test Loss :0.002201669616624713\n",
      "Epoch :0.9375    Train Loss :0.0008345796959474683    Test Loss :0.0023251506499946117\n",
      "Epoch :0.95    Train Loss :0.0007006413070484996    Test Loss :0.0023529212921857834\n",
      "Epoch :0.9625    Train Loss :0.0006520613678731024    Test Loss :0.002259056782349944\n",
      "Epoch :0.975    Train Loss :0.000703955942299217    Test Loss :0.00228047464042902\n",
      "Epoch :0.9875    Train Loss :0.0006745739374309778    Test Loss :0.0021630804985761642\n",
      "Epoch :1.0    Train Loss :0.0007892231224104762    Test Loss :0.002318905433639884\n",
      "RMSE: 9.002921642111279\n",
      "MAE: 7.386361353373252\n",
      "MAPE: 6.724333550215422%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 50, 'lr': 0.02, 'num_layers': 5}\n",
      "total:  12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.1424155980348587    Test Loss :0.470509797334671\n",
      "Epoch :0.025    Train Loss :0.15531253814697266    Test Loss :0.3292600214481354\n",
      "Epoch :0.0375    Train Loss :0.06420902907848358    Test Loss :0.07245272397994995\n",
      "Epoch :0.05    Train Loss :0.06011597439646721    Test Loss :0.1715196967124939\n",
      "Epoch :0.0625    Train Loss :0.03590736538171768    Test Loss :0.043361153453588486\n",
      "Epoch :0.075    Train Loss :0.03345995396375656    Test Loss :0.04369102418422699\n",
      "Epoch :0.0875    Train Loss :0.021401870995759964    Test Loss :0.024142589420080185\n",
      "Epoch :0.1    Train Loss :0.008721728809177876    Test Loss :0.016666840761899948\n",
      "Epoch :0.1125    Train Loss :0.004121651407331228    Test Loss :0.009327631443738937\n",
      "Epoch :0.125    Train Loss :0.0037073262501507998    Test Loss :0.006967280991375446\n",
      "Epoch :0.1375    Train Loss :0.002902667038142681    Test Loss :0.010443584993481636\n",
      "Epoch :0.15    Train Loss :0.002120767254382372    Test Loss :0.005223790183663368\n",
      "Epoch :0.1625    Train Loss :0.0014154366217553616    Test Loss :0.004169273655861616\n",
      "Epoch :0.175    Train Loss :0.0014246165519580245    Test Loss :0.0046882545575499535\n",
      "Epoch :0.1875    Train Loss :0.0011749784462153912    Test Loss :0.002724431687965989\n",
      "Epoch :0.2    Train Loss :0.0010320162400603294    Test Loss :0.0024888631887733936\n",
      "Epoch :0.2125    Train Loss :0.0009186929673887789    Test Loss :0.002489925129339099\n",
      "Epoch :0.225    Train Loss :0.0008921584812924266    Test Loss :0.0020786658860743046\n",
      "Epoch :0.2375    Train Loss :0.0008077881648205221    Test Loss :0.002083701780065894\n",
      "Epoch :0.25    Train Loss :0.0008183114696294069    Test Loss :0.002004199894145131\n",
      "Epoch :0.2625    Train Loss :0.0007239188998937607    Test Loss :0.0019071614369750023\n",
      "Epoch :0.275    Train Loss :0.0007449863478541374    Test Loss :0.0017287007067352533\n",
      "Epoch :0.2875    Train Loss :0.0006889173528179526    Test Loss :0.0016070316778495908\n",
      "Epoch :0.3    Train Loss :0.0006473042303696275    Test Loss :0.0017505416180938482\n",
      "Epoch :0.3125    Train Loss :0.0006951135001145303    Test Loss :0.0015628852415829897\n",
      "Epoch :0.325    Train Loss :0.0006557935266755521    Test Loss :0.001532779191620648\n",
      "Epoch :0.3375    Train Loss :0.0006281628157012165    Test Loss :0.001510489615611732\n",
      "Epoch :0.35    Train Loss :0.0006401167484000325    Test Loss :0.001445799134671688\n",
      "Epoch :0.3625    Train Loss :0.0005968940095044672    Test Loss :0.0013658793177455664\n",
      "Epoch :0.375    Train Loss :0.0005716653540730476    Test Loss :0.001545536913909018\n",
      "Epoch :0.3875    Train Loss :0.0005936357192695141    Test Loss :0.001420136308297515\n",
      "Epoch :0.4    Train Loss :0.0005967898177914321    Test Loss :0.001367245800793171\n",
      "Epoch :0.4125    Train Loss :0.0005580645520240068    Test Loss :0.001340799150057137\n",
      "Epoch :0.425    Train Loss :0.0005738555919378996    Test Loss :0.0012795600341632962\n",
      "Epoch :0.4375    Train Loss :0.000584107474423945    Test Loss :0.0011760885827243328\n",
      "Epoch :0.45    Train Loss :0.0005306167877279222    Test Loss :0.0012040423462167382\n",
      "Epoch :0.4625    Train Loss :0.0005462245899252594    Test Loss :0.0012514954432845116\n",
      "Epoch :0.475    Train Loss :0.0005309711559675634    Test Loss :0.0011970718624070287\n",
      "Epoch :0.4875    Train Loss :0.0005292691639624536    Test Loss :0.001215170370414853\n",
      "Epoch :0.5    Train Loss :0.0005305890808813274    Test Loss :0.0012078206054866314\n",
      "Epoch :0.5125    Train Loss :0.0005302079953253269    Test Loss :0.0011767890537157655\n",
      "Epoch :0.525    Train Loss :0.0005117111140862107    Test Loss :0.0010493457084521651\n",
      "Epoch :0.5375    Train Loss :0.0005076845409348607    Test Loss :0.0011834827018901706\n",
      "Epoch :0.55    Train Loss :0.00047216471284627914    Test Loss :0.0010924411471933126\n",
      "Epoch :0.5625    Train Loss :0.0004959732759743929    Test Loss :0.001110487850382924\n",
      "Epoch :0.575    Train Loss :0.0004902397049590945    Test Loss :0.0010772612877190113\n",
      "Epoch :0.5875    Train Loss :0.000466583704110235    Test Loss :0.0010873890714719892\n",
      "Epoch :0.6    Train Loss :0.0004976294003427029    Test Loss :0.0011166507611051202\n",
      "Epoch :0.6125    Train Loss :0.0004963429528288543    Test Loss :0.0009811361087486148\n",
      "Epoch :0.625    Train Loss :0.0004805255448445678    Test Loss :0.0010628075106069446\n",
      "Epoch :0.6375    Train Loss :0.00047703931340947747    Test Loss :0.000986179686151445\n",
      "Epoch :0.65    Train Loss :0.00047400264884345233    Test Loss :0.0010537649504840374\n",
      "Epoch :0.6625    Train Loss :0.00044544407865032554    Test Loss :0.0010294527746737003\n",
      "Epoch :0.675    Train Loss :0.000446148740593344    Test Loss :0.0009895031107589602\n",
      "Epoch :0.6875    Train Loss :0.00043905802886001766    Test Loss :0.0009688028367236257\n",
      "Epoch :0.7    Train Loss :0.00043151856516487896    Test Loss :0.001000128104351461\n",
      "Epoch :0.7125    Train Loss :0.00046469352673739195    Test Loss :0.0008965698652900755\n",
      "Epoch :0.725    Train Loss :0.0004571637837216258    Test Loss :0.000877029204275459\n",
      "Epoch :0.7375    Train Loss :0.00042089633643627167    Test Loss :0.0008643977344036102\n",
      "Epoch :0.75    Train Loss :0.0004344336630310863    Test Loss :0.0009054309921339154\n",
      "Epoch :0.7625    Train Loss :0.0004393791314214468    Test Loss :0.0009223906672559679\n",
      "Epoch :0.775    Train Loss :0.00043509522220119834    Test Loss :0.0009179706103168428\n",
      "Epoch :0.7875    Train Loss :0.0004233461513649672    Test Loss :0.000942337850574404\n",
      "Epoch :0.8    Train Loss :0.00042945315362885594    Test Loss :0.0008786622202023864\n",
      "Epoch :0.8125    Train Loss :0.0004405284416861832    Test Loss :0.000915447948500514\n",
      "Epoch :0.825    Train Loss :0.00041217118268832564    Test Loss :0.0008363522938452661\n",
      "Epoch :0.8375    Train Loss :0.00042938461410813034    Test Loss :0.0009059382136911154\n",
      "Epoch :0.85    Train Loss :0.00041684930329211056    Test Loss :0.0008439357625320554\n",
      "Epoch :0.8625    Train Loss :0.0004091162118129432    Test Loss :0.000849395408295095\n",
      "Epoch :0.875    Train Loss :0.0004133510810788721    Test Loss :0.0008028392330743372\n",
      "Epoch :0.8875    Train Loss :0.0004128715372644365    Test Loss :0.0008973311632871628\n",
      "Epoch :0.9    Train Loss :0.00040589633863419294    Test Loss :0.000787579920142889\n",
      "Epoch :0.9125    Train Loss :0.0003957879962399602    Test Loss :0.0007771068485453725\n",
      "Epoch :0.925    Train Loss :0.00040285984869115055    Test Loss :0.0007574810879305005\n",
      "Epoch :0.9375    Train Loss :0.00039925449527800083    Test Loss :0.0007748877978883684\n",
      "Epoch :0.95    Train Loss :0.0003955138090532273    Test Loss :0.0008071204647421837\n",
      "Epoch :0.9625    Train Loss :0.00037042482290416956    Test Loss :0.0007775759440846741\n",
      "Epoch :0.975    Train Loss :0.0003908018989022821    Test Loss :0.0007508511771447957\n",
      "Epoch :0.9875    Train Loss :0.00038509885780513287    Test Loss :0.0007937568007037044\n",
      "Epoch :1.0    Train Loss :0.00038656010292470455    Test Loss :0.0007434789440594614\n",
      "RMSE: 7.930554814887464\n",
      "MAE: 6.432557521841557\n",
      "MAPE: 5.564407815771348%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 100, 'lr': 0.01, 'num_layers': 2}\n",
      "total:  16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.12372037768363953    Test Loss :0.1517159640789032\n",
      "Epoch :0.025    Train Loss :0.049852073192596436    Test Loss :0.05821804702281952\n",
      "Epoch :0.0375    Train Loss :0.04864712059497833    Test Loss :0.12354275584220886\n",
      "Epoch :0.05    Train Loss :0.03923654183745384    Test Loss :0.05507338419556618\n",
      "Epoch :0.0625    Train Loss :0.017223505303263664    Test Loss :0.022168247029185295\n",
      "Epoch :0.075    Train Loss :0.018680231645703316    Test Loss :0.03538692370057106\n",
      "Epoch :0.0875    Train Loss :0.0058523258194327354    Test Loss :0.011247484013438225\n",
      "Epoch :0.1    Train Loss :0.004510358441621065    Test Loss :0.004959689918905497\n",
      "Epoch :0.1125    Train Loss :0.003211989300325513    Test Loss :0.006549111567437649\n",
      "Epoch :0.125    Train Loss :0.0021902904845774174    Test Loss :0.004789083264768124\n",
      "Epoch :0.1375    Train Loss :0.002001073444262147    Test Loss :0.004581365268677473\n",
      "Epoch :0.15    Train Loss :0.0016835008282214403    Test Loss :0.0032740875612944365\n",
      "Epoch :0.1625    Train Loss :0.001642650575377047    Test Loss :0.0034384059254080057\n",
      "Epoch :0.175    Train Loss :0.0013795241247862577    Test Loss :0.0028145192191004753\n",
      "Epoch :0.1875    Train Loss :0.0013191167963668704    Test Loss :0.0027110143564641476\n",
      "Epoch :0.2    Train Loss :0.001269437139853835    Test Loss :0.002612345851957798\n",
      "Epoch :0.2125    Train Loss :0.0011927469167858362    Test Loss :0.0023721542675048113\n",
      "Epoch :0.225    Train Loss :0.0011779580963775516    Test Loss :0.0024170740507543087\n",
      "Epoch :0.2375    Train Loss :0.0010933298617601395    Test Loss :0.0025022567715495825\n",
      "Epoch :0.25    Train Loss :0.001010183827020228    Test Loss :0.002392273396253586\n",
      "Epoch :0.2625    Train Loss :0.000958617078140378    Test Loss :0.002320540603250265\n",
      "Epoch :0.275    Train Loss :0.0009503457695245743    Test Loss :0.0021416896488517523\n",
      "Epoch :0.2875    Train Loss :0.0008746138191781938    Test Loss :0.0023077179212123156\n",
      "Epoch :0.3    Train Loss :0.0009336007060483098    Test Loss :0.00257376697845757\n",
      "Epoch :0.3125    Train Loss :0.0009444472962059081    Test Loss :0.002728804247453809\n",
      "Epoch :0.325    Train Loss :0.000852307362947613    Test Loss :0.0032600804697722197\n",
      "Epoch :0.3375    Train Loss :0.0008899591630324721    Test Loss :0.0033390589524060488\n",
      "Epoch :0.35    Train Loss :0.0007686074823141098    Test Loss :0.0031157357152551413\n",
      "Epoch :0.3625    Train Loss :0.0008148785564117134    Test Loss :0.003275769529864192\n",
      "Epoch :0.375    Train Loss :0.0008097006939351559    Test Loss :0.003098325105383992\n",
      "Epoch :0.3875    Train Loss :0.0008176460978575051    Test Loss :0.0025457062292844057\n",
      "Epoch :0.4    Train Loss :0.0007292227819561958    Test Loss :0.0026586863677948713\n",
      "Epoch :0.4125    Train Loss :0.0007995841442607343    Test Loss :0.0026807154063135386\n",
      "Epoch :0.425    Train Loss :0.0008662966429255903    Test Loss :0.0030319388024508953\n",
      "Epoch :0.4375    Train Loss :0.0008129816851578653    Test Loss :0.0029997273813933134\n",
      "Epoch :0.45    Train Loss :0.0010098363272845745    Test Loss :0.003745222231373191\n",
      "Epoch :0.4625    Train Loss :0.0009279155055992305    Test Loss :0.0033186201471835375\n",
      "Epoch :0.475    Train Loss :0.0007287362823262811    Test Loss :0.0030054242815822363\n",
      "Epoch :0.4875    Train Loss :0.0006762124830856919    Test Loss :0.003373845247551799\n",
      "Epoch :0.5    Train Loss :0.0006839108536951244    Test Loss :0.003064477350562811\n",
      "Epoch :0.5125    Train Loss :0.0007041246863082051    Test Loss :0.0030306661501526833\n",
      "Epoch :0.525    Train Loss :0.0007972160237841308    Test Loss :0.0025464959908276796\n",
      "Epoch :0.5375    Train Loss :0.0006608015391975641    Test Loss :0.002216499298810959\n",
      "Epoch :0.55    Train Loss :0.0011118141701444983    Test Loss :0.00636711809784174\n",
      "Epoch :0.5625    Train Loss :0.0008604521863162518    Test Loss :0.005469958297908306\n",
      "Epoch :0.575    Train Loss :0.0008574296371079981    Test Loss :0.004986230283975601\n",
      "Epoch :0.5875    Train Loss :0.000742393487598747    Test Loss :0.003934537060558796\n",
      "Epoch :0.6    Train Loss :0.0007112260791473091    Test Loss :0.003692185739055276\n",
      "Epoch :0.6125    Train Loss :0.0007494702003896236    Test Loss :0.0033564979676157236\n",
      "Epoch :0.625    Train Loss :0.0007987719145603478    Test Loss :0.002924805274233222\n",
      "Epoch :0.6375    Train Loss :0.0006654136814177036    Test Loss :0.0034065633080899715\n",
      "Epoch :0.65    Train Loss :0.0006835545063950121    Test Loss :0.0029044384136795998\n",
      "Epoch :0.6625    Train Loss :0.000635380856692791    Test Loss :0.002600847277790308\n",
      "Epoch :0.675    Train Loss :0.0006721009849570692    Test Loss :0.0028144163079559803\n",
      "Epoch :0.6875    Train Loss :0.0006516725406982005    Test Loss :0.002504893811419606\n",
      "Epoch :0.7    Train Loss :0.000666893320158124    Test Loss :0.00287790410220623\n",
      "Epoch :0.7125    Train Loss :0.0011954281944781542    Test Loss :0.0035573120694607496\n",
      "Epoch :0.725    Train Loss :0.0006527198711410165    Test Loss :0.003135733073577285\n",
      "Epoch :0.7375    Train Loss :0.000630934548098594    Test Loss :0.0028073603753000498\n",
      "Epoch :0.75    Train Loss :0.0005899154930375516    Test Loss :0.0032171921338886023\n",
      "Epoch :0.7625    Train Loss :0.0005972018698230386    Test Loss :0.0033742140512913465\n",
      "Epoch :0.775    Train Loss :0.0005937027162872255    Test Loss :0.0027497822884470224\n",
      "Epoch :0.7875    Train Loss :0.0006294106133282185    Test Loss :0.002333219861611724\n",
      "Epoch :0.8    Train Loss :0.0006115512805990875    Test Loss :0.003274295711889863\n",
      "Epoch :0.8125    Train Loss :0.0006430904613807797    Test Loss :0.0031077128369361162\n",
      "Epoch :0.825    Train Loss :0.000712589651811868    Test Loss :0.0038588286843150854\n",
      "Epoch :0.8375    Train Loss :0.0005835716729052365    Test Loss :0.0026557547971606255\n",
      "Epoch :0.85    Train Loss :0.0005660471506416798    Test Loss :0.00207786844111979\n",
      "Epoch :0.8625    Train Loss :0.0005847930442541838    Test Loss :0.002657458186149597\n",
      "Epoch :0.875    Train Loss :0.0005724422517232597    Test Loss :0.0028010716196149588\n",
      "Epoch :0.8875    Train Loss :0.0008497733506374061    Test Loss :0.005047611892223358\n",
      "Epoch :0.9    Train Loss :0.0014263564953580499    Test Loss :0.004630736541002989\n",
      "Epoch :0.9125    Train Loss :0.0006577549502253532    Test Loss :0.008778480812907219\n",
      "Epoch :0.925    Train Loss :0.001344075659289956    Test Loss :0.008691282011568546\n",
      "Epoch :0.9375    Train Loss :0.0009053951362147927    Test Loss :0.003077810863032937\n",
      "Epoch :0.95    Train Loss :0.000976797309704125    Test Loss :0.002802340779453516\n",
      "Epoch :0.9625    Train Loss :0.0005736469756811857    Test Loss :0.00395753001794219\n",
      "Epoch :0.975    Train Loss :0.0006572266574949026    Test Loss :0.002828814322128892\n",
      "Epoch :0.9875    Train Loss :0.0006570752011612058    Test Loss :0.002396791474893689\n",
      "Epoch :1.0    Train Loss :0.00055144471116364    Test Loss :0.003030401887372136\n",
      "RMSE: 8.637962942404444\n",
      "MAE: 7.094579223684849\n",
      "MAPE: 6.426779064951418%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 100, 'lr': 0.01, 'num_layers': 5}\n",
      "total:  19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.17459547519683838    Test Loss :0.24424581229686737\n",
      "Epoch :0.025    Train Loss :0.04586433246731758    Test Loss :0.10969323664903641\n",
      "Epoch :0.0375    Train Loss :0.04539172351360321    Test Loss :0.08612086623907089\n",
      "Epoch :0.05    Train Loss :0.03353150188922882    Test Loss :0.02311697043478489\n",
      "Epoch :0.0625    Train Loss :0.01225163135677576    Test Loss :0.021860396489501\n",
      "Epoch :0.075    Train Loss :0.004359534475952387    Test Loss :0.023817434906959534\n",
      "Epoch :0.0875    Train Loss :0.004744797013700008    Test Loss :0.013647431507706642\n",
      "Epoch :0.1    Train Loss :0.0031714916694909334    Test Loss :0.007669544778764248\n",
      "Epoch :0.1125    Train Loss :0.0019941297359764576    Test Loss :0.006012970581650734\n",
      "Epoch :0.125    Train Loss :0.0014442566316574812    Test Loss :0.0030472443904727697\n",
      "Epoch :0.1375    Train Loss :0.0014385977992787957    Test Loss :0.0027645037043839693\n",
      "Epoch :0.15    Train Loss :0.0010204082354903221    Test Loss :0.002471964806318283\n",
      "Epoch :0.1625    Train Loss :0.0008931165793910623    Test Loss :0.0019979244098067284\n",
      "Epoch :0.175    Train Loss :0.0009014695533551276    Test Loss :0.0024445580784231424\n",
      "Epoch :0.1875    Train Loss :0.0007655017543584108    Test Loss :0.0018325034761801362\n",
      "Epoch :0.2    Train Loss :0.0007477445178665221    Test Loss :0.0017318897880613804\n",
      "Epoch :0.2125    Train Loss :0.000657847966067493    Test Loss :0.0015421328134834766\n",
      "Epoch :0.225    Train Loss :0.0006595797603949904    Test Loss :0.0014015836641192436\n",
      "Epoch :0.2375    Train Loss :0.0005971543723717332    Test Loss :0.0014245867496356368\n",
      "Epoch :0.25    Train Loss :0.0006188054685480893    Test Loss :0.0013661928242072463\n",
      "Epoch :0.2625    Train Loss :0.000600238039623946    Test Loss :0.0013720027636736631\n",
      "Epoch :0.275    Train Loss :0.0005710250115953386    Test Loss :0.0013248317409306765\n",
      "Epoch :0.2875    Train Loss :0.000575270620174706    Test Loss :0.0012067395728081465\n",
      "Epoch :0.3    Train Loss :0.0005622539902105927    Test Loss :0.0012121619656682014\n",
      "Epoch :0.3125    Train Loss :0.0005303748766891658    Test Loss :0.0011577089317142963\n",
      "Epoch :0.325    Train Loss :0.0005541243008337915    Test Loss :0.0011581250000745058\n",
      "Epoch :0.3375    Train Loss :0.0005316599272191525    Test Loss :0.0011065428843721747\n",
      "Epoch :0.35    Train Loss :0.0005008758744224906    Test Loss :0.001152189215645194\n",
      "Epoch :0.3625    Train Loss :0.00047835378791205585    Test Loss :0.001026409910991788\n",
      "Epoch :0.375    Train Loss :0.0004898671177215874    Test Loss :0.0010227826423943043\n",
      "Epoch :0.3875    Train Loss :0.0004886664682999253    Test Loss :0.0010996948694810271\n",
      "Epoch :0.4    Train Loss :0.0004699761338997632    Test Loss :0.001000328455120325\n",
      "Epoch :0.4125    Train Loss :0.0005015134811401367    Test Loss :0.0009371605701744556\n",
      "Epoch :0.425    Train Loss :0.000455982459243387    Test Loss :0.0009696776978671551\n",
      "Epoch :0.4375    Train Loss :0.0004937330377288163    Test Loss :0.0009530705865472555\n",
      "Epoch :0.45    Train Loss :0.0004445026279427111    Test Loss :0.0008758106851018965\n",
      "Epoch :0.4625    Train Loss :0.000416697992477566    Test Loss :0.000920880411285907\n",
      "Epoch :0.475    Train Loss :0.0004224865115247667    Test Loss :0.0009449710487388074\n",
      "Epoch :0.4875    Train Loss :0.0004141736135352403    Test Loss :0.000874198682140559\n",
      "Epoch :0.5    Train Loss :0.00041470120777375996    Test Loss :0.0008953276555985212\n",
      "Epoch :0.5125    Train Loss :0.00041531791794113815    Test Loss :0.0008353524026460946\n",
      "Epoch :0.525    Train Loss :0.00043056064168922603    Test Loss :0.000911666254978627\n",
      "Epoch :0.5375    Train Loss :0.00039890664629638195    Test Loss :0.0008256626897491515\n",
      "Epoch :0.55    Train Loss :0.00041309220250695944    Test Loss :0.000779598718509078\n",
      "Epoch :0.5625    Train Loss :0.00037633496685884893    Test Loss :0.0008592153317295015\n",
      "Epoch :0.575    Train Loss :0.00039272932917810977    Test Loss :0.0008138312841765583\n",
      "Epoch :0.5875    Train Loss :0.00039796417695470154    Test Loss :0.0008421755628660321\n",
      "Epoch :0.6    Train Loss :0.0003601730859372765    Test Loss :0.0007574073388241231\n",
      "Epoch :0.6125    Train Loss :0.0003695078194141388    Test Loss :0.0007164475973695517\n",
      "Epoch :0.625    Train Loss :0.00036620773607864976    Test Loss :0.0007924463134258986\n",
      "Epoch :0.6375    Train Loss :0.00036379724042490125    Test Loss :0.0007250302587635815\n",
      "Epoch :0.65    Train Loss :0.0003722786786966026    Test Loss :0.0007533034076914191\n",
      "Epoch :0.6625    Train Loss :0.0003508319496177137    Test Loss :0.0007325256592594087\n",
      "Epoch :0.675    Train Loss :0.00036818895023316145    Test Loss :0.0007134090992622077\n",
      "Epoch :0.6875    Train Loss :0.00035056393244303763    Test Loss :0.00071047677192837\n",
      "Epoch :0.7    Train Loss :0.00033545453334227204    Test Loss :0.0007215923396870494\n",
      "Epoch :0.7125    Train Loss :0.0003512529656291008    Test Loss :0.0006874734535813332\n",
      "Epoch :0.725    Train Loss :0.000348620000295341    Test Loss :0.0006702993996441364\n",
      "Epoch :0.7375    Train Loss :0.0003324247372802347    Test Loss :0.0006806273013353348\n",
      "Epoch :0.75    Train Loss :0.0003267303400207311    Test Loss :0.0006655851029790938\n",
      "Epoch :0.7625    Train Loss :0.0003384630545042455    Test Loss :0.0006244677933864295\n",
      "Epoch :0.775    Train Loss :0.00034381679142825305    Test Loss :0.0006517894216813147\n",
      "Epoch :0.7875    Train Loss :0.00035553553607314825    Test Loss :0.0006475091213360429\n",
      "Epoch :0.8    Train Loss :0.0002992906665895134    Test Loss :0.000639412784948945\n",
      "Epoch :0.8125    Train Loss :0.00032768843811936677    Test Loss :0.0006937403813935816\n",
      "Epoch :0.825    Train Loss :0.00031976704485714436    Test Loss :0.0006283124093897641\n",
      "Epoch :0.8375    Train Loss :0.00030495517421513796    Test Loss :0.0005987603217363358\n",
      "Epoch :0.85    Train Loss :0.00030996502027846873    Test Loss :0.0005997217958793044\n",
      "Epoch :0.8625    Train Loss :0.0003021216543857008    Test Loss :0.0005860039964318275\n",
      "Epoch :0.875    Train Loss :0.00029754944262094796    Test Loss :0.0006280885427258909\n",
      "Epoch :0.8875    Train Loss :0.0003067964280489832    Test Loss :0.0005817980854772031\n",
      "Epoch :0.9    Train Loss :0.0003029249783139676    Test Loss :0.0005932975909672678\n",
      "Epoch :0.9125    Train Loss :0.00030848736059851944    Test Loss :0.0006083363550715148\n",
      "Epoch :0.925    Train Loss :0.00028631751774810255    Test Loss :0.0005614483379758894\n",
      "Epoch :0.9375    Train Loss :0.0002939026162493974    Test Loss :0.0005117074470035732\n",
      "Epoch :0.95    Train Loss :0.00042853516060858965    Test Loss :0.0010277434485033154\n",
      "Epoch :0.9625    Train Loss :0.0005080190603621304    Test Loss :0.0005708123790100217\n",
      "Epoch :0.975    Train Loss :0.0003502189356368035    Test Loss :0.0005309395492076874\n",
      "Epoch :0.9875    Train Loss :0.0003173912991769612    Test Loss :0.0005368667189031839\n",
      "Epoch :1.0    Train Loss :0.00030874600633978844    Test Loss :0.0005302514182403684\n",
      "RMSE: 12.794075158912683\n",
      "MAE: 10.904929188131222\n",
      "MAPE: 10.023541842172087%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 100, 'lr': 0.02, 'num_layers': 2}\n",
      "total:  22.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.1527142971754074    Test Loss :1.7124836444854736\n",
      "Epoch :0.025    Train Loss :0.11542455106973648    Test Loss :0.04922293499112129\n",
      "Epoch :0.0375    Train Loss :0.058438289910554886    Test Loss :0.08668797463178635\n",
      "Epoch :0.05    Train Loss :0.04871578887104988    Test Loss :0.14993877708911896\n",
      "Epoch :0.0625    Train Loss :0.04896951839327812    Test Loss :0.06355055421590805\n",
      "Epoch :0.075    Train Loss :0.04938620701432228    Test Loss :0.14158689975738525\n",
      "Epoch :0.0875    Train Loss :0.04959569498896599    Test Loss :0.07207255065441132\n",
      "Epoch :0.1    Train Loss :0.04976428300142288    Test Loss :0.12781189382076263\n",
      "Epoch :0.1125    Train Loss :0.04956597462296486    Test Loss :0.08362838625907898\n",
      "Epoch :0.125    Train Loss :0.04936891049146652    Test Loss :0.110979825258255\n",
      "Epoch :0.1375    Train Loss :0.04884493350982666    Test Loss :0.09917423129081726\n",
      "Epoch :0.15    Train Loss :0.04844564199447632    Test Loss :0.09651363641023636\n",
      "Epoch :0.1625    Train Loss :0.048553887754678726    Test Loss :0.10771086812019348\n",
      "Epoch :0.175    Train Loss :0.04863988980650902    Test Loss :0.09555945545434952\n",
      "Epoch :0.1875    Train Loss :0.04860016331076622    Test Loss :0.1024429127573967\n",
      "Epoch :0.2    Train Loss :0.048461224883794785    Test Loss :0.10295682400465012\n",
      "Epoch :0.2125    Train Loss :0.04853292927145958    Test Loss :0.09827841073274612\n",
      "Epoch :0.225    Train Loss :0.04850123077630997    Test Loss :0.10187335312366486\n",
      "Epoch :0.2375    Train Loss :0.048488467931747437    Test Loss :0.10184421390295029\n",
      "Epoch :0.25    Train Loss :0.04848373681306839    Test Loss :0.0999917984008789\n",
      "Epoch :0.2625    Train Loss :0.04855293035507202    Test Loss :0.10093723237514496\n",
      "Epoch :0.275    Train Loss :0.04853886365890503    Test Loss :0.10149744153022766\n",
      "Epoch :0.2875    Train Loss :0.04855247959494591    Test Loss :0.10079068690538406\n",
      "Epoch :0.3    Train Loss :0.0484764389693737    Test Loss :0.10078277438879013\n",
      "Epoch :0.3125    Train Loss :0.048549264669418335    Test Loss :0.10103610157966614\n",
      "Epoch :0.325    Train Loss :0.04845849797129631    Test Loss :0.10109926760196686\n",
      "Epoch :0.3375    Train Loss :0.048494208604097366    Test Loss :0.10058632493019104\n",
      "Epoch :0.35    Train Loss :0.048493705689907074    Test Loss :0.10063223540782928\n",
      "Epoch :0.3625    Train Loss :0.04848119243979454    Test Loss :0.10120748728513718\n",
      "Epoch :0.375    Train Loss :0.04850108176469803    Test Loss :0.10095786303281784\n",
      "Epoch :0.3875    Train Loss :0.04858853667974472    Test Loss :0.10084321349859238\n",
      "Epoch :0.4    Train Loss :0.04850788414478302    Test Loss :0.10073861479759216\n",
      "Epoch :0.4125    Train Loss :0.04852240905165672    Test Loss :0.10095246881246567\n",
      "Epoch :0.425    Train Loss :0.04851279780268669    Test Loss :0.101112499833107\n",
      "Epoch :0.4375    Train Loss :0.04849115386605263    Test Loss :0.1006486564874649\n",
      "Epoch :0.45    Train Loss :0.04850396141409874    Test Loss :0.1006847396492958\n",
      "Epoch :0.4625    Train Loss :0.04854113981127739    Test Loss :0.10087317228317261\n",
      "Epoch :0.475    Train Loss :0.04850871115922928    Test Loss :0.1010827124118805\n",
      "Epoch :0.4875    Train Loss :0.048476509749889374    Test Loss :0.10080911964178085\n",
      "Epoch :0.5    Train Loss :0.04854770004749298    Test Loss :0.10058628767728806\n",
      "Epoch :0.5125    Train Loss :0.048461079597473145    Test Loss :0.10080891102552414\n",
      "Epoch :0.525    Train Loss :0.04852506145834923    Test Loss :0.1011132299900055\n",
      "Epoch :0.5375    Train Loss :0.0484987273812294    Test Loss :0.10100896656513214\n",
      "Epoch :0.55    Train Loss :0.048523981124162674    Test Loss :0.10099726170301437\n",
      "Epoch :0.5625    Train Loss :0.04849505424499512    Test Loss :0.10092873871326447\n",
      "Epoch :0.575    Train Loss :0.04849766939878464    Test Loss :0.10102751851081848\n",
      "Epoch :0.5875    Train Loss :0.04853004589676857    Test Loss :0.10087085515260696\n",
      "Epoch :0.6    Train Loss :0.04849947988986969    Test Loss :0.10096587240695953\n",
      "Epoch :0.6125    Train Loss :0.04848911240696907    Test Loss :0.1006886437535286\n",
      "Epoch :0.625    Train Loss :0.04849155619740486    Test Loss :0.1006278321146965\n",
      "Epoch :0.6375    Train Loss :0.04852311313152313    Test Loss :0.10092431306838989\n",
      "Epoch :0.65    Train Loss :0.048502516001462936    Test Loss :0.10087037086486816\n",
      "Epoch :0.6625    Train Loss :0.04848034679889679    Test Loss :0.10097174346446991\n",
      "Epoch :0.675    Train Loss :0.048492833971977234    Test Loss :0.1009860560297966\n",
      "Epoch :0.6875    Train Loss :0.04849507659673691    Test Loss :0.10083965212106705\n",
      "Epoch :0.7    Train Loss :0.04850461333990097    Test Loss :0.1009114682674408\n",
      "Epoch :0.7125    Train Loss :0.04850428178906441    Test Loss :0.10098976641893387\n",
      "Epoch :0.725    Train Loss :0.04849489778280258    Test Loss :0.10093823075294495\n",
      "Epoch :0.7375    Train Loss :0.048536211252212524    Test Loss :0.10087098181247711\n",
      "Epoch :0.75    Train Loss :0.04851548746228218    Test Loss :0.10084710270166397\n",
      "Epoch :0.7625    Train Loss :0.048506006598472595    Test Loss :0.10081221163272858\n",
      "Epoch :0.775    Train Loss :0.04852262884378433    Test Loss :0.10080558806657791\n",
      "Epoch :0.7875    Train Loss :0.04849560186266899    Test Loss :0.10092924535274506\n",
      "Epoch :0.8    Train Loss :0.04853734001517296    Test Loss :0.10095393657684326\n",
      "Epoch :0.8125    Train Loss :0.0485026054084301    Test Loss :0.10096339136362076\n",
      "Epoch :0.825    Train Loss :0.04852340742945671    Test Loss :0.1009564995765686\n",
      "Epoch :0.8375    Train Loss :0.048505742102861404    Test Loss :0.10092179477214813\n",
      "Epoch :0.85    Train Loss :0.04850005358457565    Test Loss :0.10096317529678345\n",
      "Epoch :0.8625    Train Loss :0.04850393161177635    Test Loss :0.10096020251512527\n",
      "Epoch :0.875    Train Loss :0.04848836362361908    Test Loss :0.10094039887189865\n",
      "Epoch :0.8875    Train Loss :0.048499591648578644    Test Loss :0.1009405180811882\n",
      "Epoch :0.9    Train Loss :0.04849764704704285    Test Loss :0.10087886452674866\n",
      "Epoch :0.9125    Train Loss :0.04849030077457428    Test Loss :0.10086450725793839\n",
      "Epoch :0.925    Train Loss :0.04852060601115227    Test Loss :0.10089124739170074\n",
      "Epoch :0.9375    Train Loss :0.048494577407836914    Test Loss :0.10101724416017532\n",
      "Epoch :0.95    Train Loss :0.04851618409156799    Test Loss :0.10102789103984833\n",
      "Epoch :0.9625    Train Loss :0.048488762229681015    Test Loss :0.10097149759531021\n",
      "Epoch :0.975    Train Loss :0.048503972589969635    Test Loss :0.10099669545888901\n",
      "Epoch :0.9875    Train Loss :0.04850877448916435    Test Loss :0.10098214447498322\n",
      "Epoch :1.0    Train Loss :0.048493944108486176    Test Loss :0.10103265941143036\n",
      "RMSE: 30.728056897147063\n",
      "MAE: 29.709032358045835\n",
      "MAPE: 25.871667142941746%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 100, 'lr': 0.02, 'num_layers': 5}\n",
      "total:  25.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.04826786369085312    Test Loss :0.16309690475463867\n",
      "Epoch :0.016666666666666666    Train Loss :0.039234958589076996    Test Loss :0.058202773332595825\n",
      "Epoch :0.025    Train Loss :0.023135269060730934    Test Loss :0.03798668086528778\n",
      "Epoch :0.03333333333333333    Train Loss :0.009053915739059448    Test Loss :0.005892887711524963\n",
      "Epoch :0.041666666666666664    Train Loss :0.004244833253324032    Test Loss :0.0048362212255597115\n",
      "Epoch :0.05    Train Loss :0.0038917039055377245    Test Loss :0.0052529978565871716\n",
      "Epoch :0.058333333333333334    Train Loss :0.0028757494874298573    Test Loss :0.0034505585208535194\n",
      "Epoch :0.06666666666666667    Train Loss :0.0014873795444145799    Test Loss :0.004912617150694132\n",
      "Epoch :0.075    Train Loss :0.0014757892349734902    Test Loss :0.0028794496320188046\n",
      "Epoch :0.08333333333333333    Train Loss :0.0014465490821748972    Test Loss :0.0026541948318481445\n",
      "Epoch :0.09166666666666666    Train Loss :0.0009619466727599502    Test Loss :0.00327286496758461\n",
      "Epoch :0.1    Train Loss :0.0009653751621954143    Test Loss :0.0020526067819446325\n",
      "Epoch :0.10833333333333334    Train Loss :0.0008762729703448713    Test Loss :0.002082824008539319\n",
      "Epoch :0.11666666666666667    Train Loss :0.0008828761056065559    Test Loss :0.002483443822711706\n",
      "Epoch :0.125    Train Loss :0.000806940661277622    Test Loss :0.0019720513373613358\n",
      "Epoch :0.13333333333333333    Train Loss :0.0007137142238207161    Test Loss :0.002088598906993866\n",
      "Epoch :0.14166666666666666    Train Loss :0.0007813107804395258    Test Loss :0.001853015972301364\n",
      "Epoch :0.15    Train Loss :0.00074152578599751    Test Loss :0.0015501329908147454\n",
      "Epoch :0.15833333333333333    Train Loss :0.0006796569214202464    Test Loss :0.0015654276357963681\n",
      "Epoch :0.16666666666666666    Train Loss :0.0006163163343444467    Test Loss :0.0015387146268039942\n",
      "Epoch :0.175    Train Loss :0.0006376057281158864    Test Loss :0.0016714700032025576\n",
      "Epoch :0.18333333333333332    Train Loss :0.000639208301436156    Test Loss :0.001445528818294406\n",
      "Epoch :0.19166666666666668    Train Loss :0.000613440468441695    Test Loss :0.0013455399312078953\n",
      "Epoch :0.2    Train Loss :0.0006035586702637374    Test Loss :0.0012601730413734913\n",
      "Epoch :0.20833333333333334    Train Loss :0.000558128987904638    Test Loss :0.0012295794440433383\n",
      "Epoch :0.21666666666666667    Train Loss :0.0005940122064203024    Test Loss :0.0011730353580787778\n",
      "Epoch :0.225    Train Loss :0.0005255157593637705    Test Loss :0.0012082544853910804\n",
      "Epoch :0.23333333333333334    Train Loss :0.0005429216544143856    Test Loss :0.0010631417389959097\n",
      "Epoch :0.24166666666666667    Train Loss :0.0005716606974601746    Test Loss :0.0011689345119521022\n",
      "Epoch :0.25    Train Loss :0.0005329815321601927    Test Loss :0.0010053529404103756\n",
      "Epoch :0.25833333333333336    Train Loss :0.0005273276474326849    Test Loss :0.0010132788447663188\n",
      "Epoch :0.26666666666666666    Train Loss :0.0005404478870332241    Test Loss :0.0010592153994366527\n",
      "Epoch :0.275    Train Loss :0.0004590786702465266    Test Loss :0.0009264574036933482\n",
      "Epoch :0.2833333333333333    Train Loss :0.0004857845779042691    Test Loss :0.0010303735034540296\n",
      "Epoch :0.2916666666666667    Train Loss :0.00045875695650465786    Test Loss :0.0009620508644729853\n",
      "Epoch :0.3    Train Loss :0.0004731974913738668    Test Loss :0.0010190649190917611\n",
      "Epoch :0.30833333333333335    Train Loss :0.00045876350486651063    Test Loss :0.0008470485918223858\n",
      "Epoch :0.31666666666666665    Train Loss :0.00046188748092390597    Test Loss :0.000944918196182698\n",
      "Epoch :0.325    Train Loss :0.00045122523442842066    Test Loss :0.0009952177060768008\n",
      "Epoch :0.3333333333333333    Train Loss :0.0004273986560292542    Test Loss :0.0008469294989481568\n",
      "Epoch :0.3416666666666667    Train Loss :0.0004378729499876499    Test Loss :0.000935215677600354\n",
      "Epoch :0.35    Train Loss :0.00041588605381548405    Test Loss :0.0008533683721907437\n",
      "Epoch :0.35833333333333334    Train Loss :0.00042676005978137255    Test Loss :0.0008390866569243371\n",
      "Epoch :0.36666666666666664    Train Loss :0.00041415129089728    Test Loss :0.0007871061097830534\n",
      "Epoch :0.375    Train Loss :0.00040408753557130694    Test Loss :0.00083306769374758\n",
      "Epoch :0.38333333333333336    Train Loss :0.0003990015247836709    Test Loss :0.0008296437445096672\n",
      "Epoch :0.39166666666666666    Train Loss :0.00039331617881543934    Test Loss :0.0007943062228150666\n",
      "Epoch :0.4    Train Loss :0.0003916219575330615    Test Loss :0.000837475061416626\n",
      "Epoch :0.4083333333333333    Train Loss :0.00040225606062449515    Test Loss :0.0007598314550705254\n",
      "Epoch :0.4166666666666667    Train Loss :0.0003900374285876751    Test Loss :0.0007252919021993876\n",
      "Epoch :0.425    Train Loss :0.0003736373037099838    Test Loss :0.0007293698145076632\n",
      "Epoch :0.43333333333333335    Train Loss :0.00037360930582508445    Test Loss :0.0007262413273565471\n",
      "Epoch :0.44166666666666665    Train Loss :0.0003591474087443203    Test Loss :0.000700542179401964\n",
      "Epoch :0.45    Train Loss :0.0003626880352385342    Test Loss :0.0007113700266927481\n",
      "Epoch :0.4583333333333333    Train Loss :0.0003707933647092432    Test Loss :0.0006962358602322638\n",
      "Epoch :0.4666666666666667    Train Loss :0.0003548451350070536    Test Loss :0.0007363507174886763\n",
      "Epoch :0.475    Train Loss :0.00035344151547178626    Test Loss :0.0006778094684705138\n",
      "Epoch :0.48333333333333334    Train Loss :0.00035788631066679955    Test Loss :0.0006937397411093116\n",
      "Epoch :0.49166666666666664    Train Loss :0.0003592421126086265    Test Loss :0.0006730239256285131\n",
      "Epoch :0.5    Train Loss :0.0003686693380586803    Test Loss :0.0006527140503749251\n",
      "Epoch :0.5083333333333333    Train Loss :0.00035388980177231133    Test Loss :0.0007155878120101988\n",
      "Epoch :0.5166666666666667    Train Loss :0.0003922781615983695    Test Loss :0.0008962408755905926\n",
      "Epoch :0.525    Train Loss :0.0004139886877965182    Test Loss :0.0006621451466344297\n",
      "Epoch :0.5333333333333333    Train Loss :0.0005106835742481053    Test Loss :0.000681061705108732\n",
      "Epoch :0.5416666666666666    Train Loss :0.0003986747469753027    Test Loss :0.0006653182208538055\n",
      "Epoch :0.55    Train Loss :0.00036187743535265326    Test Loss :0.0005943431169725955\n",
      "Epoch :0.5583333333333333    Train Loss :0.0003813851799350232    Test Loss :0.000597423582803458\n",
      "Epoch :0.5666666666666667    Train Loss :0.0004064700333401561    Test Loss :0.000624799751676619\n",
      "Epoch :0.575    Train Loss :0.0003630774444900453    Test Loss :0.0006824457086622715\n",
      "Epoch :0.5833333333333334    Train Loss :0.0003708198491949588    Test Loss :0.0007625003345310688\n",
      "Epoch :0.5916666666666667    Train Loss :0.00032432322041131556    Test Loss :0.0007038986659608781\n",
      "Epoch :0.6    Train Loss :0.0003951303951907903    Test Loss :0.0008061181870289147\n",
      "Epoch :0.6083333333333333    Train Loss :0.0003910057130269706    Test Loss :0.000664569204673171\n",
      "Epoch :0.6166666666666667    Train Loss :0.0004092424060218036    Test Loss :0.0006331565091386437\n",
      "Epoch :0.625    Train Loss :0.0003362201096024364    Test Loss :0.0006508567021228373\n",
      "Epoch :0.6333333333333333    Train Loss :0.0003294810594525188    Test Loss :0.0006932072574272752\n",
      "Epoch :0.6416666666666667    Train Loss :0.0003429877106100321    Test Loss :0.000709567335434258\n",
      "Epoch :0.65    Train Loss :0.00030407385202124715    Test Loss :0.0005609282525256276\n",
      "Epoch :0.6583333333333333    Train Loss :0.00032420677598565817    Test Loss :0.0006221686489880085\n",
      "Epoch :0.6666666666666666    Train Loss :0.0003220486396458    Test Loss :0.000714189896825701\n",
      "Epoch :0.675    Train Loss :0.0003049153310712427    Test Loss :0.0005756615428254008\n",
      "Epoch :0.6833333333333333    Train Loss :0.0003038874710910022    Test Loss :0.0005469048046506941\n",
      "Epoch :0.6916666666666667    Train Loss :0.0003141482302453369    Test Loss :0.0006734157796017826\n",
      "Epoch :0.7    Train Loss :0.0008691487601026893    Test Loss :0.0016281723510473967\n",
      "Epoch :0.7083333333333334    Train Loss :0.0003532753325998783    Test Loss :0.0010643715504556894\n",
      "Epoch :0.7166666666666667    Train Loss :0.0006118793971836567    Test Loss :0.000625697139184922\n",
      "Epoch :0.725    Train Loss :0.0005215646815486252    Test Loss :0.0008059965912252665\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7333333333333333    Train Loss :0.0003857252304442227    Test Loss :0.0006261262460611761\n",
      "Epoch :0.7416666666666667    Train Loss :0.0003593479632399976    Test Loss :0.0006454624817706645\n",
      "Epoch :0.75    Train Loss :0.00034780066926032305    Test Loss :0.0005972718354314566\n",
      "Epoch :0.7583333333333333    Train Loss :0.00033818191150203347    Test Loss :0.0007104174001142383\n",
      "Epoch :0.7666666666666667    Train Loss :0.0003363713331054896    Test Loss :0.0005866987048648298\n",
      "Epoch :0.775    Train Loss :0.00029842049116268754    Test Loss :0.0006040877779014409\n",
      "Epoch :0.7833333333333333    Train Loss :0.0003072689287364483    Test Loss :0.0005645587807521224\n",
      "Epoch :0.7916666666666666    Train Loss :0.00029163830913603306    Test Loss :0.000555345497559756\n",
      "Epoch :0.8    Train Loss :0.00028582909726537764    Test Loss :0.0005360233481042087\n",
      "Epoch :0.8083333333333333    Train Loss :0.0002705168444663286    Test Loss :0.0005063446005806327\n",
      "Epoch :0.8166666666666667    Train Loss :0.00028984929667785764    Test Loss :0.0005247347289696336\n",
      "Epoch :0.825    Train Loss :0.0002805010008160025    Test Loss :0.0005613595712929964\n",
      "Epoch :0.8333333333333334    Train Loss :0.00028282517450861633    Test Loss :0.0004778819566126913\n",
      "Epoch :0.8416666666666667    Train Loss :0.00027482424047775567    Test Loss :0.0004701994184870273\n",
      "Epoch :0.85    Train Loss :0.00027417601086199284    Test Loss :0.000516214466188103\n",
      "Epoch :0.8583333333333333    Train Loss :0.0002840037632267922    Test Loss :0.00047026461106725037\n",
      "Epoch :0.8666666666666667    Train Loss :0.0003077613073401153    Test Loss :0.0005552376969717443\n",
      "Epoch :0.875    Train Loss :0.00028957778704352677    Test Loss :0.0004697752301581204\n",
      "Epoch :0.8833333333333333    Train Loss :0.0003367521276231855    Test Loss :0.000631081813480705\n",
      "Epoch :0.8916666666666667    Train Loss :0.00027382734697312117    Test Loss :0.0004886400420218706\n",
      "Epoch :0.9    Train Loss :0.00026173272635787725    Test Loss :0.0005273911519907415\n",
      "Epoch :0.9083333333333333    Train Loss :0.0003894447290804237    Test Loss :0.0006076847203075886\n",
      "Epoch :0.9166666666666666    Train Loss :0.0002616994606796652    Test Loss :0.00047735977568663657\n",
      "Epoch :0.925    Train Loss :0.00037389068165794015    Test Loss :0.0005786809488199651\n",
      "Epoch :0.9333333333333333    Train Loss :0.0003296414506621659    Test Loss :0.0005766220856457949\n",
      "Epoch :0.9416666666666667    Train Loss :0.00030611082911491394    Test Loss :0.0006584502407349646\n",
      "Epoch :0.95    Train Loss :0.00027396323275752366    Test Loss :0.0005483238492161036\n",
      "Epoch :0.9583333333333334    Train Loss :0.00024225155357271433    Test Loss :0.00044006522512063384\n",
      "Epoch :0.9666666666666667    Train Loss :0.0003328181628603488    Test Loss :0.00047795154387131333\n",
      "Epoch :0.975    Train Loss :0.00030237610917538404    Test Loss :0.000551641162019223\n",
      "Epoch :0.9833333333333333    Train Loss :0.00025310926139354706    Test Loss :0.0004152734181843698\n",
      "Epoch :0.9916666666666667    Train Loss :0.0003161783388350159    Test Loss :0.0005805642576888204\n",
      "Epoch :1.0    Train Loss :0.00045645455247722566    Test Loss :0.001070427126251161\n",
      "RMSE: 66.93639335887009\n",
      "MAE: 65.78263773012867\n",
      "MAPE: 58.146403019828064%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 600, 'hid_size': 50, 'lr': 0.01, 'num_layers': 2}\n",
      "total:  28.000000000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.0753418356180191    Test Loss :0.2759815752506256\n",
      "Epoch :0.016666666666666666    Train Loss :0.07134107500314713    Test Loss :0.17573794722557068\n",
      "Epoch :0.025    Train Loss :0.05760788545012474    Test Loss :0.06179202347993851\n",
      "Epoch :0.03333333333333333    Train Loss :0.04891331493854523    Test Loss :0.11997675895690918\n",
      "Epoch :0.041666666666666664    Train Loss :0.04950212314724922    Test Loss :0.11599227786064148\n",
      "Epoch :0.05    Train Loss :0.048437152057886124    Test Loss :0.09241537749767303\n",
      "Epoch :0.058333333333333334    Train Loss :0.04844777658581734    Test Loss :0.09014486521482468\n",
      "Epoch :0.06666666666666667    Train Loss :0.043757520616054535    Test Loss :0.08684570342302322\n",
      "Epoch :0.075    Train Loss :0.0114854471758008    Test Loss :0.011283301748335361\n",
      "Epoch :0.08333333333333333    Train Loss :0.012504404410719872    Test Loss :0.037175800651311874\n",
      "Epoch :0.09166666666666666    Train Loss :0.01147629227489233    Test Loss :0.020170606672763824\n",
      "Epoch :0.1    Train Loss :0.008624931797385216    Test Loss :0.010215076617896557\n",
      "Epoch :0.10833333333333334    Train Loss :0.005673553328961134    Test Loss :0.014466628432273865\n",
      "Epoch :0.11666666666666667    Train Loss :0.003784647211432457    Test Loss :0.005085107870399952\n",
      "Epoch :0.125    Train Loss :0.002538579748943448    Test Loss :0.005088100675493479\n",
      "Epoch :0.13333333333333333    Train Loss :0.002327652182430029    Test Loss :0.005312909372150898\n",
      "Epoch :0.14166666666666666    Train Loss :0.0019119982607662678    Test Loss :0.005216541700065136\n",
      "Epoch :0.15    Train Loss :0.0018112694378942251    Test Loss :0.0031723789870738983\n",
      "Epoch :0.15833333333333333    Train Loss :0.0015818532556295395    Test Loss :0.003097027773037553\n",
      "Epoch :0.16666666666666666    Train Loss :0.0015481527661904693    Test Loss :0.003250549780204892\n",
      "Epoch :0.175    Train Loss :0.0014312383718788624    Test Loss :0.0027979789301753044\n",
      "Epoch :0.18333333333333332    Train Loss :0.0013925408711656928    Test Loss :0.002522303257137537\n",
      "Epoch :0.19166666666666668    Train Loss :0.0013315615942701697    Test Loss :0.0030567364301532507\n",
      "Epoch :0.2    Train Loss :0.00125475216191262    Test Loss :0.0026680140290409327\n",
      "Epoch :0.20833333333333334    Train Loss :0.001275037880986929    Test Loss :0.0025257840752601624\n",
      "Epoch :0.21666666666666667    Train Loss :0.0012111146934330463    Test Loss :0.002544379560276866\n",
      "Epoch :0.225    Train Loss :0.0012234081514179707    Test Loss :0.002356531098484993\n",
      "Epoch :0.23333333333333334    Train Loss :0.0011880096280947328    Test Loss :0.0021789753809571266\n",
      "Epoch :0.24166666666666667    Train Loss :0.0011040960671380162    Test Loss :0.0020756428129971027\n",
      "Epoch :0.25    Train Loss :0.001078612171113491    Test Loss :0.002324777189642191\n",
      "Epoch :0.25833333333333336    Train Loss :0.0010339341824874282    Test Loss :0.002085909014567733\n",
      "Epoch :0.26666666666666666    Train Loss :0.0009779719403013587    Test Loss :0.00229724682867527\n",
      "Epoch :0.275    Train Loss :0.0009627810795791447    Test Loss :0.0021450351923704147\n",
      "Epoch :0.2833333333333333    Train Loss :0.0009368209284730256    Test Loss :0.002116838237270713\n",
      "Epoch :0.2916666666666667    Train Loss :0.00092279352247715    Test Loss :0.002153115114197135\n",
      "Epoch :0.3    Train Loss :0.0008979779086075723    Test Loss :0.002106078900396824\n",
      "Epoch :0.30833333333333335    Train Loss :0.0008938249084167182    Test Loss :0.0019532269798219204\n",
      "Epoch :0.31666666666666665    Train Loss :0.0009533567354083061    Test Loss :0.0019589026924222708\n",
      "Epoch :0.325    Train Loss :0.0008498234092257917    Test Loss :0.0021591996774077415\n",
      "Epoch :0.3333333333333333    Train Loss :0.0008638165309093893    Test Loss :0.0019383514299988747\n",
      "Epoch :0.3416666666666667    Train Loss :0.0008314512670040131    Test Loss :0.0017807618714869022\n",
      "Epoch :0.35    Train Loss :0.0008107095491141081    Test Loss :0.0019130555447191\n",
      "Epoch :0.35833333333333334    Train Loss :0.0007774895639158785    Test Loss :0.0017377701587975025\n",
      "Epoch :0.36666666666666664    Train Loss :0.0007585941348224878    Test Loss :0.0017792214639484882\n",
      "Epoch :0.375    Train Loss :0.0007563499384559691    Test Loss :0.0017799342749640346\n",
      "Epoch :0.38333333333333336    Train Loss :0.0007654922665096819    Test Loss :0.0016587096033617854\n",
      "Epoch :0.39166666666666666    Train Loss :0.0007604554411955178    Test Loss :0.0017429968575015664\n",
      "Epoch :0.4    Train Loss :0.0007471388089470565    Test Loss :0.0017135048983618617\n",
      "Epoch :0.4083333333333333    Train Loss :0.0007249137270264328    Test Loss :0.0017867141868919134\n",
      "Epoch :0.4166666666666667    Train Loss :0.0007561332895420492    Test Loss :0.00167216663248837\n",
      "Epoch :0.425    Train Loss :0.0007083363016135991    Test Loss :0.0016703882720321417\n",
      "Epoch :0.43333333333333335    Train Loss :0.0007006831583566964    Test Loss :0.001709339558146894\n",
      "Epoch :0.44166666666666665    Train Loss :0.0006744625861756504    Test Loss :0.0020176207181066275\n",
      "Epoch :0.45    Train Loss :0.0006851408979855478    Test Loss :0.0017710169777274132\n",
      "Epoch :0.4583333333333333    Train Loss :0.0006234283791854978    Test Loss :0.0018483295571058989\n",
      "Epoch :0.4666666666666667    Train Loss :0.0006546322838403285    Test Loss :0.001755155622959137\n",
      "Epoch :0.475    Train Loss :0.000759422080591321    Test Loss :0.0018101673340424895\n",
      "Epoch :0.48333333333333334    Train Loss :0.0006457510753534734    Test Loss :0.0018533028196543455\n",
      "Epoch :0.49166666666666664    Train Loss :0.000644914573058486    Test Loss :0.001796786324121058\n",
      "Epoch :0.5    Train Loss :0.0006389737827703357    Test Loss :0.0018242791993543506\n",
      "Epoch :0.5083333333333333    Train Loss :0.000612329866271466    Test Loss :0.0018074264517053962\n",
      "Epoch :0.5166666666666667    Train Loss :0.0005684992065653205    Test Loss :0.001856186194345355\n",
      "Epoch :0.525    Train Loss :0.0005795703618787229    Test Loss :0.001710165524855256\n",
      "Epoch :0.5333333333333333    Train Loss :0.0005696624284610152    Test Loss :0.0018436042591929436\n",
      "Epoch :0.5416666666666666    Train Loss :0.0005945086013525724    Test Loss :0.0017018432263284922\n",
      "Epoch :0.55    Train Loss :0.0005780591745860875    Test Loss :0.001793665811419487\n",
      "Epoch :0.5583333333333333    Train Loss :0.0005968307377770543    Test Loss :0.0017215735279023647\n",
      "Epoch :0.5666666666666667    Train Loss :0.0006356491358019412    Test Loss :0.0017228999640792608\n",
      "Epoch :0.575    Train Loss :0.00051572808297351    Test Loss :0.0015812736237421632\n",
      "Epoch :0.5833333333333334    Train Loss :0.0005117076798342168    Test Loss :0.0015381565317511559\n",
      "Epoch :0.5916666666666667    Train Loss :0.0005458208033815026    Test Loss :0.001567087136209011\n",
      "Epoch :0.6    Train Loss :0.0005910487961955369    Test Loss :0.00151479069609195\n",
      "Epoch :0.6083333333333333    Train Loss :0.0006137916934676468    Test Loss :0.0016360725276172161\n",
      "Epoch :0.6166666666666667    Train Loss :0.0005109533085487783    Test Loss :0.0015908299246802926\n",
      "Epoch :0.625    Train Loss :0.0005486871814355254    Test Loss :0.001591560896486044\n",
      "Epoch :0.6333333333333333    Train Loss :0.0005350522696971893    Test Loss :0.0015949057415127754\n",
      "Epoch :0.6416666666666667    Train Loss :0.0005683053168468177    Test Loss :0.0018201555358245969\n",
      "Epoch :0.65    Train Loss :0.0005208570510149002    Test Loss :0.0015832438366487622\n",
      "Epoch :0.6583333333333333    Train Loss :0.0005808340501971543    Test Loss :0.0016773099778220057\n",
      "Epoch :0.6666666666666666    Train Loss :0.0006241758819669485    Test Loss :0.0015444520395249128\n",
      "Epoch :0.675    Train Loss :0.0005597190465778112    Test Loss :0.0015464173629879951\n",
      "Epoch :0.6833333333333333    Train Loss :0.0005954024381935596    Test Loss :0.0016267751343548298\n",
      "Epoch :0.6916666666666667    Train Loss :0.0006032396340742707    Test Loss :0.0015694770263507962\n",
      "Epoch :0.7    Train Loss :0.0005084723816253245    Test Loss :0.001746615394949913\n",
      "Epoch :0.7083333333333334    Train Loss :0.0005357860936783254    Test Loss :0.0020233462564647198\n",
      "Epoch :0.7166666666666667    Train Loss :0.0005342012736946344    Test Loss :0.0016098226187750697\n",
      "Epoch :0.725    Train Loss :0.0004696176329161972    Test Loss :0.001631339779123664\n",
      "Epoch :0.7333333333333333    Train Loss :0.0004963459214195609    Test Loss :0.0016069966368377209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7416666666666667    Train Loss :0.0005141181172803044    Test Loss :0.0015404458390548825\n",
      "Epoch :0.75    Train Loss :0.0005031148320995271    Test Loss :0.001668046461418271\n",
      "Epoch :0.7583333333333333    Train Loss :0.0004531509766820818    Test Loss :0.001629820093512535\n",
      "Epoch :0.7666666666666667    Train Loss :0.0004709105414804071    Test Loss :0.0016399003798142076\n",
      "Epoch :0.775    Train Loss :0.00045539397979155183    Test Loss :0.0014773898292332888\n",
      "Epoch :0.7833333333333333    Train Loss :0.0004730080545414239    Test Loss :0.0014681839384138584\n",
      "Epoch :0.7916666666666666    Train Loss :0.0004743205208797008    Test Loss :0.0014745399821549654\n",
      "Epoch :0.8    Train Loss :0.00044478668132796884    Test Loss :0.0014958078972995281\n",
      "Epoch :0.8083333333333333    Train Loss :0.0004419574688654393    Test Loss :0.0014632870443165302\n",
      "Epoch :0.8166666666666667    Train Loss :0.0004420742334332317    Test Loss :0.0014602687442675233\n",
      "Epoch :0.825    Train Loss :0.000432628090493381    Test Loss :0.0014534402871504426\n",
      "Epoch :0.8333333333333334    Train Loss :0.0004702630976680666    Test Loss :0.0015411729691550136\n",
      "Epoch :0.8416666666666667    Train Loss :0.0004413745191413909    Test Loss :0.0015626339009031653\n",
      "Epoch :0.85    Train Loss :0.0005078660324215889    Test Loss :0.0014196208212524652\n",
      "Epoch :0.8583333333333333    Train Loss :0.0004434404836501926    Test Loss :0.0014420590596273541\n",
      "Epoch :0.8666666666666667    Train Loss :0.000473531661555171    Test Loss :0.0014654649421572685\n",
      "Epoch :0.875    Train Loss :0.0004458434123080224    Test Loss :0.0015162639319896698\n",
      "Epoch :0.8833333333333333    Train Loss :0.0004297201812732965    Test Loss :0.0015985878417268395\n",
      "Epoch :0.8916666666666667    Train Loss :0.0004063523083459586    Test Loss :0.0013902276987209916\n",
      "Epoch :0.9    Train Loss :0.00044505370897240937    Test Loss :0.0014172677183523774\n",
      "Epoch :0.9083333333333333    Train Loss :0.00055628054542467    Test Loss :0.0017125523881986737\n",
      "Epoch :0.9166666666666666    Train Loss :0.00047253037337213755    Test Loss :0.0016360962763428688\n",
      "Epoch :0.925    Train Loss :0.00042175641283392906    Test Loss :0.0014730087714269757\n",
      "Epoch :0.9333333333333333    Train Loss :0.0004395950527396053    Test Loss :0.0014689472736790776\n",
      "Epoch :0.9416666666666667    Train Loss :0.0004536185588221997    Test Loss :0.0014767983229830861\n",
      "Epoch :0.95    Train Loss :0.00045754402526654303    Test Loss :0.001461197156459093\n",
      "Epoch :0.9583333333333334    Train Loss :0.000432621716754511    Test Loss :0.001415338716469705\n",
      "Epoch :0.9666666666666667    Train Loss :0.0003970499674323946    Test Loss :0.0014510469045490026\n",
      "Epoch :0.975    Train Loss :0.000428162282332778    Test Loss :0.0014910409227013588\n",
      "Epoch :0.9833333333333333    Train Loss :0.0003908503276761621    Test Loss :0.0014297848101705313\n",
      "Epoch :0.9916666666666667    Train Loss :0.00040426774648949504    Test Loss :0.0014012170722708106\n",
      "Epoch :1.0    Train Loss :0.0003936977591365576    Test Loss :0.001347492914646864\n",
      "RMSE: 11.604898758958717\n",
      "MAE: 9.36057291031679\n",
      "MAPE: 8.165440992121626%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 600, 'hid_size': 50, 'lr': 0.01, 'num_layers': 5}\n",
      "total:  31.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.06927632540464401    Test Loss :0.1615508496761322\n",
      "Epoch :0.016666666666666666    Train Loss :0.04372264817357063    Test Loss :0.06858190149068832\n",
      "Epoch :0.025    Train Loss :0.015603571198880672    Test Loss :0.09551806002855301\n",
      "Epoch :0.03333333333333333    Train Loss :0.03144856542348862    Test Loss :0.060765720903873444\n",
      "Epoch :0.041666666666666664    Train Loss :0.018117601051926613    Test Loss :0.007001154590398073\n",
      "Epoch :0.05    Train Loss :0.006079721264541149    Test Loss :0.02047611027956009\n",
      "Epoch :0.058333333333333334    Train Loss :0.0024713375605642796    Test Loss :0.0038411812856793404\n",
      "Epoch :0.06666666666666667    Train Loss :0.003867280902341008    Test Loss :0.0034499524626880884\n",
      "Epoch :0.075    Train Loss :0.002468497958034277    Test Loss :0.0034729784820228815\n",
      "Epoch :0.08333333333333333    Train Loss :0.0015684844693168998    Test Loss :0.0027583783958107233\n",
      "Epoch :0.09166666666666666    Train Loss :0.0016286273021250963    Test Loss :0.002418172312900424\n",
      "Epoch :0.1    Train Loss :0.0012968481751158834    Test Loss :0.003826634958386421\n",
      "Epoch :0.10833333333333334    Train Loss :0.0010387955699115992    Test Loss :0.002455225447192788\n",
      "Epoch :0.11666666666666667    Train Loss :0.0009931178065016866    Test Loss :0.002421638695523143\n",
      "Epoch :0.125    Train Loss :0.0008724659564904869    Test Loss :0.002172235632315278\n",
      "Epoch :0.13333333333333333    Train Loss :0.0008573102531954646    Test Loss :0.0019101380603387952\n",
      "Epoch :0.14166666666666666    Train Loss :0.0007948401616886258    Test Loss :0.0019317290280014277\n",
      "Epoch :0.15    Train Loss :0.0007414981373585761    Test Loss :0.0016651625046506524\n",
      "Epoch :0.15833333333333333    Train Loss :0.000663953716866672    Test Loss :0.00182812858838588\n",
      "Epoch :0.16666666666666666    Train Loss :0.0006614126032218337    Test Loss :0.0014459361555054784\n",
      "Epoch :0.175    Train Loss :0.0006295674247667193    Test Loss :0.0015326801221817732\n",
      "Epoch :0.18333333333333332    Train Loss :0.000663858256302774    Test Loss :0.0012450101785361767\n",
      "Epoch :0.19166666666666668    Train Loss :0.0006173772271722555    Test Loss :0.0012064158217981458\n",
      "Epoch :0.2    Train Loss :0.0005717860767617822    Test Loss :0.001175469602458179\n",
      "Epoch :0.20833333333333334    Train Loss :0.0005263808416202664    Test Loss :0.0010958191705867648\n",
      "Epoch :0.21666666666666667    Train Loss :0.0005610553780570626    Test Loss :0.0010565996635705233\n",
      "Epoch :0.225    Train Loss :0.0004955870099365711    Test Loss :0.0010071891592815518\n",
      "Epoch :0.23333333333333334    Train Loss :0.0005342380609363317    Test Loss :0.0009610457345843315\n",
      "Epoch :0.24166666666666667    Train Loss :0.0004981810343451798    Test Loss :0.0010166654828935862\n",
      "Epoch :0.25    Train Loss :0.0005094112711958587    Test Loss :0.0010235789231956005\n",
      "Epoch :0.25833333333333336    Train Loss :0.0004730545624624938    Test Loss :0.0009043480386026204\n",
      "Epoch :0.26666666666666666    Train Loss :0.00043390068458393216    Test Loss :0.000901176652405411\n",
      "Epoch :0.275    Train Loss :0.0004633396165445447    Test Loss :0.000917591736651957\n",
      "Epoch :0.2833333333333333    Train Loss :0.00044567446457222104    Test Loss :0.0008120259153656662\n",
      "Epoch :0.2916666666666667    Train Loss :0.000414763402659446    Test Loss :0.0008410917362198234\n",
      "Epoch :0.3    Train Loss :0.0004074083990417421    Test Loss :0.0008244058699347079\n",
      "Epoch :0.30833333333333335    Train Loss :0.000419970863731578    Test Loss :0.0008481165277771652\n",
      "Epoch :0.31666666666666665    Train Loss :0.0004086080298293382    Test Loss :0.0008528012549504638\n",
      "Epoch :0.325    Train Loss :0.00040764803998172283    Test Loss :0.000742932315915823\n",
      "Epoch :0.3333333333333333    Train Loss :0.0004117610806133598    Test Loss :0.0007197103113867342\n",
      "Epoch :0.3416666666666667    Train Loss :0.0003809515619650483    Test Loss :0.0007618720992468297\n",
      "Epoch :0.35    Train Loss :0.00039587821811437607    Test Loss :0.0007027905667200685\n",
      "Epoch :0.35833333333333334    Train Loss :0.00039367645513266325    Test Loss :0.000725645397324115\n",
      "Epoch :0.36666666666666664    Train Loss :0.0003656201879493892    Test Loss :0.0006766063161194324\n",
      "Epoch :0.375    Train Loss :0.0003730471362359822    Test Loss :0.0006458049756474793\n",
      "Epoch :0.38333333333333336    Train Loss :0.00035980073153041303    Test Loss :0.0007187563460320234\n",
      "Epoch :0.39166666666666666    Train Loss :0.0003522569313645363    Test Loss :0.0006959413294680417\n",
      "Epoch :0.4    Train Loss :0.0003561759367585182    Test Loss :0.0006771830376237631\n",
      "Epoch :0.4083333333333333    Train Loss :0.00036625252687372267    Test Loss :0.0006967880763113499\n",
      "Epoch :0.4166666666666667    Train Loss :0.0003573727735783905    Test Loss :0.0007057361654005945\n",
      "Epoch :0.425    Train Loss :0.00036085923784412444    Test Loss :0.0008021986577659845\n",
      "Epoch :0.43333333333333335    Train Loss :0.0004238923138473183    Test Loss :0.0006506103090941906\n",
      "Epoch :0.44166666666666665    Train Loss :0.00039634795393794775    Test Loss :0.000668577675241977\n",
      "Epoch :0.45    Train Loss :0.000331802642904222    Test Loss :0.0006031825905665755\n",
      "Epoch :0.4583333333333333    Train Loss :0.00035631214268505573    Test Loss :0.0006409853231161833\n",
      "Epoch :0.4666666666666667    Train Loss :0.00033744992106221616    Test Loss :0.0006374988006427884\n",
      "Epoch :0.475    Train Loss :0.00032128955353982747    Test Loss :0.000608387344982475\n",
      "Epoch :0.48333333333333334    Train Loss :0.00032348252716474235    Test Loss :0.0005919329705648124\n",
      "Epoch :0.49166666666666664    Train Loss :0.0003251760790590197    Test Loss :0.0006517449510283768\n",
      "Epoch :0.5    Train Loss :0.0003170088166370988    Test Loss :0.0005598327261395752\n",
      "Epoch :0.5083333333333333    Train Loss :0.0003345449804328382    Test Loss :0.000603281136136502\n",
      "Epoch :0.5166666666666667    Train Loss :0.001951487734913826    Test Loss :0.006643770262598991\n",
      "Epoch :0.525    Train Loss :0.0006160310003906488    Test Loss :0.0008410760783590376\n",
      "Epoch :0.5333333333333333    Train Loss :0.0004544664698187262    Test Loss :0.002089619869366288\n",
      "Epoch :0.5416666666666666    Train Loss :0.0006394368247129023    Test Loss :0.0011822128435596824\n",
      "Epoch :0.55    Train Loss :0.0004897143808193505    Test Loss :0.0008762028301134706\n",
      "Epoch :0.5583333333333333    Train Loss :0.0003831006761174649    Test Loss :0.0008385570254176855\n",
      "Epoch :0.5666666666666667    Train Loss :0.00041596469236537814    Test Loss :0.000786604592576623\n",
      "Epoch :0.575    Train Loss :0.0003818785771727562    Test Loss :0.0008344504167325795\n",
      "Epoch :0.5833333333333334    Train Loss :0.000350345071638003    Test Loss :0.0007548829889856279\n",
      "Epoch :0.5916666666666667    Train Loss :0.00039374190964736044    Test Loss :0.0007207324379123747\n",
      "Epoch :0.6    Train Loss :0.00036077536060474813    Test Loss :0.0007056351169012487\n",
      "Epoch :0.6083333333333333    Train Loss :0.0003666726406663656    Test Loss :0.0007468965486623347\n",
      "Epoch :0.6166666666666667    Train Loss :0.0003502035979181528    Test Loss :0.0006608417024835944\n",
      "Epoch :0.625    Train Loss :0.0003466431808192283    Test Loss :0.0007103066891431808\n",
      "Epoch :0.6333333333333333    Train Loss :0.0003247398417443037    Test Loss :0.000645561667624861\n",
      "Epoch :0.6416666666666667    Train Loss :0.0003431176592130214    Test Loss :0.0006465684855356812\n",
      "Epoch :0.65    Train Loss :0.0003189235576428473    Test Loss :0.0006656419718638062\n",
      "Epoch :0.6583333333333333    Train Loss :0.00031166308326646686    Test Loss :0.0005899443058297038\n",
      "Epoch :0.6666666666666666    Train Loss :0.000321956496918574    Test Loss :0.0006611085846088827\n",
      "Epoch :0.675    Train Loss :0.00032931112218648195    Test Loss :0.0006300023524090648\n",
      "Epoch :0.6833333333333333    Train Loss :0.00030991167295724154    Test Loss :0.0005903073470108211\n",
      "Epoch :0.6916666666666667    Train Loss :0.00031257333466783166    Test Loss :0.0005671062390320003\n",
      "Epoch :0.7    Train Loss :0.00031060646870173514    Test Loss :0.0006125190993770957\n",
      "Epoch :0.7083333333333334    Train Loss :0.0003121084300801158    Test Loss :0.0005941640702076256\n",
      "Epoch :0.7166666666666667    Train Loss :0.0003099546884186566    Test Loss :0.0005877637304365635\n",
      "Epoch :0.725    Train Loss :0.00031691277399659157    Test Loss :0.0005424623377621174\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7333333333333333    Train Loss :0.0003063191252294928    Test Loss :0.0005881398101337254\n",
      "Epoch :0.7416666666666667    Train Loss :0.0002926346205640584    Test Loss :0.0005475063226185739\n",
      "Epoch :0.75    Train Loss :0.0003020667645614594    Test Loss :0.000574711593799293\n",
      "Epoch :0.7583333333333333    Train Loss :0.00028869646484963596    Test Loss :0.0005926846642978489\n",
      "Epoch :0.7666666666666667    Train Loss :0.00029148635803721845    Test Loss :0.0005756705650128424\n",
      "Epoch :0.775    Train Loss :0.0002935853262897581    Test Loss :0.0005758186453022063\n",
      "Epoch :0.7833333333333333    Train Loss :0.00029399539926089346    Test Loss :0.0005894038476981223\n",
      "Epoch :0.7916666666666666    Train Loss :0.0002806503907777369    Test Loss :0.0005240440368652344\n",
      "Epoch :0.8    Train Loss :0.00029918653308413923    Test Loss :0.0005546245956793427\n",
      "Epoch :0.8083333333333333    Train Loss :0.00040087883826345205    Test Loss :0.0010773532558232546\n",
      "Epoch :0.8166666666666667    Train Loss :0.0007174458587542176    Test Loss :0.0007066162070259452\n",
      "Epoch :0.825    Train Loss :0.0004492873267736286    Test Loss :0.0005518075195141137\n",
      "Epoch :0.8333333333333334    Train Loss :0.00029929698212072253    Test Loss :0.0006950307288207114\n",
      "Epoch :0.8416666666666667    Train Loss :0.0003071714600082487    Test Loss :0.0007778270519338548\n",
      "Epoch :0.85    Train Loss :0.00031917807064019144    Test Loss :0.0005535174277611077\n",
      "Epoch :0.8583333333333333    Train Loss :0.0002907629241235554    Test Loss :0.0005166035261936486\n",
      "Epoch :0.8666666666666667    Train Loss :0.00031158680212683976    Test Loss :0.0005912865162827075\n",
      "Epoch :0.875    Train Loss :0.00035575329093262553    Test Loss :0.000507646647747606\n",
      "Epoch :0.8833333333333333    Train Loss :0.0003086482756771147    Test Loss :0.0005126218311488628\n",
      "Epoch :0.8916666666666667    Train Loss :0.00028386758640408516    Test Loss :0.0005472973571158946\n",
      "Epoch :0.9    Train Loss :0.00027368543669581413    Test Loss :0.0005945847369730473\n",
      "Epoch :0.9083333333333333    Train Loss :0.0002707733365241438    Test Loss :0.0005235790158621967\n",
      "Epoch :0.9166666666666666    Train Loss :0.0002812503953464329    Test Loss :0.0004944200627505779\n",
      "Epoch :0.925    Train Loss :0.00032078553340397775    Test Loss :0.0006631652940995991\n",
      "Epoch :0.9333333333333333    Train Loss :0.00028246943838894367    Test Loss :0.00047011423157528043\n",
      "Epoch :0.9416666666666667    Train Loss :0.0004002090136054903    Test Loss :0.0007313221576623619\n",
      "Epoch :0.95    Train Loss :0.00043222401291131973    Test Loss :0.0007625169237144291\n",
      "Epoch :0.9583333333333334    Train Loss :0.00036593113327398896    Test Loss :0.0008132060756906867\n",
      "Epoch :0.9666666666666667    Train Loss :0.0002888830495066941    Test Loss :0.0006060712621547282\n",
      "Epoch :0.975    Train Loss :0.00026153336511924863    Test Loss :0.0005274552386254072\n",
      "Epoch :0.9833333333333333    Train Loss :0.00026548432651907206    Test Loss :0.00047788492520339787\n",
      "Epoch :0.9916666666666667    Train Loss :0.00026074890047311783    Test Loss :0.00045814670738764107\n",
      "Epoch :1.0    Train Loss :0.00028325815219432116    Test Loss :0.0006067943177185953\n",
      "RMSE: 56.15818377575653\n",
      "MAE: 53.05586345619532\n",
      "MAPE: 47.00956953305872%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 600, 'hid_size': 50, 'lr': 0.02, 'num_layers': 2}\n",
      "total:  34.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.12233076244592667    Test Loss :0.43549516797065735\n",
      "Epoch :0.016666666666666666    Train Loss :0.09514053165912628    Test Loss :0.16178658604621887\n",
      "Epoch :0.025    Train Loss :0.052263498306274414    Test Loss :0.1424800604581833\n",
      "Epoch :0.03333333333333333    Train Loss :0.04996975511312485    Test Loss :0.10021162033081055\n",
      "Epoch :0.041666666666666664    Train Loss :0.05009342357516289    Test Loss :0.0951334610581398\n",
      "Epoch :0.05    Train Loss :0.04933986812829971    Test Loss :0.11809273064136505\n",
      "Epoch :0.058333333333333334    Train Loss :0.048791006207466125    Test Loss :0.09221149981021881\n",
      "Epoch :0.06666666666666667    Train Loss :0.04869537055492401    Test Loss :0.10028659552335739\n",
      "Epoch :0.075    Train Loss :0.048874493688344955    Test Loss :0.10841525346040726\n",
      "Epoch :0.08333333333333333    Train Loss :0.048753466457128525    Test Loss :0.0956716313958168\n",
      "Epoch :0.09166666666666666    Train Loss :0.048607341945171356    Test Loss :0.10222215205430984\n",
      "Epoch :0.1    Train Loss :0.04867548868060112    Test Loss :0.10339236259460449\n",
      "Epoch :0.10833333333333334    Train Loss :0.048707734793424606    Test Loss :0.09849942475557327\n",
      "Epoch :0.11666666666666667    Train Loss :0.04848630353808403    Test Loss :0.10257583856582642\n",
      "Epoch :0.125    Train Loss :0.04856250435113907    Test Loss :0.10093269497156143\n",
      "Epoch :0.13333333333333333    Train Loss :0.04856805503368378    Test Loss :0.1002090722322464\n",
      "Epoch :0.14166666666666666    Train Loss :0.048615697771310806    Test Loss :0.10170681774616241\n",
      "Epoch :0.15    Train Loss :0.048464085906744    Test Loss :0.10038892924785614\n",
      "Epoch :0.15833333333333333    Train Loss :0.04848821088671684    Test Loss :0.10149165242910385\n",
      "Epoch :0.16666666666666666    Train Loss :0.04848485440015793    Test Loss :0.10069764405488968\n",
      "Epoch :0.175    Train Loss :0.04851102456450462    Test Loss :0.10109391063451767\n",
      "Epoch :0.18333333333333332    Train Loss :0.04853488877415657    Test Loss :0.10131622850894928\n",
      "Epoch :0.19166666666666668    Train Loss :0.04852939397096634    Test Loss :0.10074011981487274\n",
      "Epoch :0.2    Train Loss :0.048507317900657654    Test Loss :0.1008356511592865\n",
      "Epoch :0.20833333333333334    Train Loss :0.048588428646326065    Test Loss :0.1007578894495964\n",
      "Epoch :0.21666666666666667    Train Loss :0.04855240881443024    Test Loss :0.10132790356874466\n",
      "Epoch :0.225    Train Loss :0.04852161929011345    Test Loss :0.10078582167625427\n",
      "Epoch :0.23333333333333334    Train Loss :0.04847417771816254    Test Loss :0.1011517196893692\n",
      "Epoch :0.24166666666666667    Train Loss :0.048515934497117996    Test Loss :0.10108541697263718\n",
      "Epoch :0.25    Train Loss :0.04853251576423645    Test Loss :0.10090162605047226\n",
      "Epoch :0.25833333333333336    Train Loss :0.048513367772102356    Test Loss :0.1006920263171196\n",
      "Epoch :0.26666666666666666    Train Loss :0.04854084923863411    Test Loss :0.10115495324134827\n",
      "Epoch :0.275    Train Loss :0.04857116937637329    Test Loss :0.10075166076421738\n",
      "Epoch :0.2833333333333333    Train Loss :0.04857417568564415    Test Loss :0.10110673308372498\n",
      "Epoch :0.2916666666666667    Train Loss :0.04860445484519005    Test Loss :0.1010010838508606\n",
      "Epoch :0.3    Train Loss :0.04853513836860657    Test Loss :0.10071034729480743\n",
      "Epoch :0.30833333333333335    Train Loss :0.048530928790569305    Test Loss :0.10120809078216553\n",
      "Epoch :0.31666666666666665    Train Loss :0.04855603724718094    Test Loss :0.10077814012765884\n",
      "Epoch :0.325    Train Loss :0.04848410189151764    Test Loss :0.10113489627838135\n",
      "Epoch :0.3333333333333333    Train Loss :0.04849663004279137    Test Loss :0.10046432912349701\n",
      "Epoch :0.3416666666666667    Train Loss :0.04855002462863922    Test Loss :0.10100999474525452\n",
      "Epoch :0.35    Train Loss :0.04853351414203644    Test Loss :0.10123452544212341\n",
      "Epoch :0.35833333333333334    Train Loss :0.04846781864762306    Test Loss :0.10065485537052155\n",
      "Epoch :0.36666666666666664    Train Loss :0.048502251505851746    Test Loss :0.10123365372419357\n",
      "Epoch :0.375    Train Loss :0.048471394926309586    Test Loss :0.10091187059879303\n",
      "Epoch :0.38333333333333336    Train Loss :0.04848289489746094    Test Loss :0.10110117495059967\n",
      "Epoch :0.39166666666666666    Train Loss :0.048549555242061615    Test Loss :0.101015605032444\n",
      "Epoch :0.4    Train Loss :0.048503078520298004    Test Loss :0.10112176090478897\n",
      "Epoch :0.4083333333333333    Train Loss :0.04851832985877991    Test Loss :0.10095127671957016\n",
      "Epoch :0.4166666666666667    Train Loss :0.0485735721886158    Test Loss :0.10073477029800415\n",
      "Epoch :0.425    Train Loss :0.048645980656147    Test Loss :0.10088300704956055\n",
      "Epoch :0.43333333333333335    Train Loss :0.04855046793818474    Test Loss :0.10073450207710266\n",
      "Epoch :0.44166666666666665    Train Loss :0.04849565774202347    Test Loss :0.10113362967967987\n",
      "Epoch :0.45    Train Loss :0.04847406968474388    Test Loss :0.10076484829187393\n",
      "Epoch :0.4583333333333333    Train Loss :0.048497524112463    Test Loss :0.1005961000919342\n",
      "Epoch :0.4666666666666667    Train Loss :0.04856624826788902    Test Loss :0.1010710597038269\n",
      "Epoch :0.475    Train Loss :0.04853024706244469    Test Loss :0.10062193870544434\n",
      "Epoch :0.48333333333333334    Train Loss :0.04853031784296036    Test Loss :0.10125003755092621\n",
      "Epoch :0.49166666666666664    Train Loss :0.04851356893777847    Test Loss :0.10074552893638611\n",
      "Epoch :0.5    Train Loss :0.048482704907655716    Test Loss :0.10091382265090942\n",
      "Epoch :0.5083333333333333    Train Loss :0.048456091433763504    Test Loss :0.10102257877588272\n",
      "Epoch :0.5166666666666667    Train Loss :0.04849952086806297    Test Loss :0.10088257491588593\n",
      "Epoch :0.525    Train Loss :0.048460692167282104    Test Loss :0.10122387111186981\n",
      "Epoch :0.5333333333333333    Train Loss :0.04853226989507675    Test Loss :0.10092098265886307\n",
      "Epoch :0.5416666666666666    Train Loss :0.04851698875427246    Test Loss :0.10075396299362183\n",
      "Epoch :0.55    Train Loss :0.04852205887436867    Test Loss :0.10114394873380661\n",
      "Epoch :0.5583333333333333    Train Loss :0.04851970449090004    Test Loss :0.10093743354082108\n",
      "Epoch :0.5666666666666667    Train Loss :0.04857567697763443    Test Loss :0.10074352473020554\n",
      "Epoch :0.575    Train Loss :0.04838170111179352    Test Loss :0.10113348066806793\n",
      "Epoch :0.5833333333333334    Train Loss :0.0484992079436779    Test Loss :0.10067325830459595\n",
      "Epoch :0.5916666666666667    Train Loss :0.04853633791208267    Test Loss :0.10106907784938812\n",
      "Epoch :0.6    Train Loss :0.04853721335530281    Test Loss :0.10109693557024002\n",
      "Epoch :0.6083333333333333    Train Loss :0.04851885512471199    Test Loss :0.10073790699243546\n",
      "Epoch :0.6166666666666667    Train Loss :0.04846314713358879    Test Loss :0.10102143883705139\n",
      "Epoch :0.625    Train Loss :0.048508696258068085    Test Loss :0.10087210685014725\n",
      "Epoch :0.6333333333333333    Train Loss :0.04852117970585823    Test Loss :0.10088799893856049\n",
      "Epoch :0.6416666666666667    Train Loss :0.048449430614709854    Test Loss :0.10100933164358139\n",
      "Epoch :0.65    Train Loss :0.0484972707927227    Test Loss :0.10112153738737106\n",
      "Epoch :0.6583333333333333    Train Loss :0.048519283533096313    Test Loss :0.10076069086790085\n",
      "Epoch :0.6666666666666666    Train Loss :0.048523977398872375    Test Loss :0.10116707533597946\n",
      "Epoch :0.675    Train Loss :0.04851604625582695    Test Loss :0.10091748833656311\n",
      "Epoch :0.6833333333333333    Train Loss :0.04849271848797798    Test Loss :0.10085189342498779\n",
      "Epoch :0.6916666666666667    Train Loss :0.04850782826542854    Test Loss :0.10095391422510147\n",
      "Epoch :0.7    Train Loss :0.04857882112264633    Test Loss :0.10089917480945587\n",
      "Epoch :0.7083333333333334    Train Loss :0.04853154718875885    Test Loss :0.10111802816390991\n",
      "Epoch :0.7166666666666667    Train Loss :0.04853201285004616    Test Loss :0.10080500692129135\n",
      "Epoch :0.725    Train Loss :0.048447780311107635    Test Loss :0.10095753520727158\n",
      "Epoch :0.7333333333333333    Train Loss :0.048488225787878036    Test Loss :0.101015105843544\n",
      "Epoch :0.7416666666666667    Train Loss :0.04852133244276047    Test Loss :0.100833959877491\n",
      "Epoch :0.75    Train Loss :0.04851263016462326    Test Loss :0.10077691823244095\n",
      "Epoch :0.7583333333333333    Train Loss :0.04856457933783531    Test Loss :0.10101670026779175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7666666666666667    Train Loss :0.04854963347315788    Test Loss :0.10104712098836899\n",
      "Epoch :0.775    Train Loss :0.0484950914978981    Test Loss :0.10086585581302643\n",
      "Epoch :0.7833333333333333    Train Loss :0.04856560006737709    Test Loss :0.1009913980960846\n",
      "Epoch :0.7916666666666666    Train Loss :0.04852714762091637    Test Loss :0.10084337741136551\n",
      "Epoch :0.8    Train Loss :0.04856051504611969    Test Loss :0.10111691057682037\n",
      "Epoch :0.8083333333333333    Train Loss :0.04854441061615944    Test Loss :0.10100998729467392\n",
      "Epoch :0.8166666666666667    Train Loss :0.04848146066069603    Test Loss :0.10091943293809891\n",
      "Epoch :0.825    Train Loss :0.048484306782484055    Test Loss :0.10094580799341202\n",
      "Epoch :0.8333333333333334    Train Loss :0.048488326370716095    Test Loss :0.10088355839252472\n",
      "Epoch :0.8416666666666667    Train Loss :0.04850868135690689    Test Loss :0.10078172385692596\n",
      "Epoch :0.85    Train Loss :0.048486851155757904    Test Loss :0.10102537274360657\n",
      "Epoch :0.8583333333333333    Train Loss :0.048501577228307724    Test Loss :0.10103875398635864\n",
      "Epoch :0.8666666666666667    Train Loss :0.048562753945589066    Test Loss :0.10071153938770294\n",
      "Epoch :0.875    Train Loss :0.04856197535991669    Test Loss :0.10098013281822205\n",
      "Epoch :0.8833333333333333    Train Loss :0.0485273152589798    Test Loss :0.10091131925582886\n",
      "Epoch :0.8916666666666667    Train Loss :0.048539578914642334    Test Loss :0.10073048621416092\n",
      "Epoch :0.9    Train Loss :0.04848090931773186    Test Loss :0.10108103603124619\n",
      "Epoch :0.9083333333333333    Train Loss :0.04848097264766693    Test Loss :0.10092391073703766\n",
      "Epoch :0.9166666666666666    Train Loss :0.04849076643586159    Test Loss :0.10077305883169174\n",
      "Epoch :0.925    Train Loss :0.048482201993465424    Test Loss :0.10113151371479034\n",
      "Epoch :0.9333333333333333    Train Loss :0.048497650772333145    Test Loss :0.10094618052244186\n",
      "Epoch :0.9416666666666667    Train Loss :0.04851390793919563    Test Loss :0.10096532106399536\n",
      "Epoch :0.95    Train Loss :0.04852863773703575    Test Loss :0.10084936767816544\n",
      "Epoch :0.9583333333333334    Train Loss :0.04853786528110504    Test Loss :0.10088521987199783\n",
      "Epoch :0.9666666666666667    Train Loss :0.048493947833776474    Test Loss :0.10105522722005844\n",
      "Epoch :0.975    Train Loss :0.04848872125148773    Test Loss :0.10103120654821396\n",
      "Epoch :0.9833333333333333    Train Loss :0.048382144421339035    Test Loss :0.10001679509878159\n",
      "Epoch :0.9916666666666667    Train Loss :0.048079025000333786    Test Loss :0.10191822052001953\n",
      "Epoch :1.0    Train Loss :0.04823867604136467    Test Loss :0.09540661424398422\n",
      "RMSE: 29.978885541903537\n",
      "MAE: 28.913880929032533\n",
      "MAPE: 25.16719788701841%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 600, 'hid_size': 50, 'lr': 0.02, 'num_layers': 5}\n",
      "total:  38.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.1255561113357544    Test Loss :0.4261479675769806\n",
      "Epoch :0.016666666666666666    Train Loss :0.15248478949069977    Test Loss :0.36487263441085815\n",
      "Epoch :0.025    Train Loss :0.05843242257833481    Test Loss :0.014345401898026466\n",
      "Epoch :0.03333333333333333    Train Loss :0.04604825749993324    Test Loss :0.14340396225452423\n",
      "Epoch :0.041666666666666664    Train Loss :0.032545872032642365    Test Loss :0.04395163059234619\n",
      "Epoch :0.05    Train Loss :0.01843174360692501    Test Loss :0.035461798310279846\n",
      "Epoch :0.058333333333333334    Train Loss :0.00959861185401678    Test Loss :0.022626783698797226\n",
      "Epoch :0.06666666666666667    Train Loss :0.005331296473741531    Test Loss :0.023750092834234238\n",
      "Epoch :0.075    Train Loss :0.002819514600560069    Test Loss :0.0041191792115569115\n",
      "Epoch :0.08333333333333333    Train Loss :0.0018690726719796658    Test Loss :0.006859881337732077\n",
      "Epoch :0.09166666666666666    Train Loss :0.0013881660997867584    Test Loss :0.0033608335070312023\n",
      "Epoch :0.1    Train Loss :0.0015569145325571299    Test Loss :0.005525292828679085\n",
      "Epoch :0.10833333333333334    Train Loss :0.0014780565397813916    Test Loss :0.0037629359867423773\n",
      "Epoch :0.11666666666666667    Train Loss :0.0010370573727414012    Test Loss :0.002769115613773465\n",
      "Epoch :0.125    Train Loss :0.0008744536899030209    Test Loss :0.0021409085020422935\n",
      "Epoch :0.13333333333333333    Train Loss :0.0008192973327822983    Test Loss :0.001910078921355307\n",
      "Epoch :0.14166666666666666    Train Loss :0.0007865884690545499    Test Loss :0.001957559958100319\n",
      "Epoch :0.15    Train Loss :0.0007667768513783813    Test Loss :0.0017779403133317828\n",
      "Epoch :0.15833333333333333    Train Loss :0.000710430322214961    Test Loss :0.0018691112054511905\n",
      "Epoch :0.16666666666666666    Train Loss :0.00067162822233513    Test Loss :0.001592902117408812\n",
      "Epoch :0.175    Train Loss :0.0006639384664595127    Test Loss :0.0017353696748614311\n",
      "Epoch :0.18333333333333332    Train Loss :0.0006339255487546325    Test Loss :0.0017948637250810862\n",
      "Epoch :0.19166666666666668    Train Loss :0.0006239000358618796    Test Loss :0.0016133551253005862\n",
      "Epoch :0.2    Train Loss :0.0006066070636734366    Test Loss :0.0013518176274374127\n",
      "Epoch :0.20833333333333334    Train Loss :0.0005973712541162968    Test Loss :0.0013299151323735714\n",
      "Epoch :0.21666666666666667    Train Loss :0.0005532054929062724    Test Loss :0.0012035358231514692\n",
      "Epoch :0.225    Train Loss :0.0005635136622004211    Test Loss :0.0012349012540653348\n",
      "Epoch :0.23333333333333334    Train Loss :0.000553996826056391    Test Loss :0.0012946093920618296\n",
      "Epoch :0.24166666666666667    Train Loss :0.0005242833867669106    Test Loss :0.0011983157601207495\n",
      "Epoch :0.25    Train Loss :0.0005273565766401589    Test Loss :0.001169239403679967\n",
      "Epoch :0.25833333333333336    Train Loss :0.000507867953274399    Test Loss :0.001146635040640831\n",
      "Epoch :0.26666666666666666    Train Loss :0.0005301552591845393    Test Loss :0.0012264358811080456\n",
      "Epoch :0.275    Train Loss :0.0005238872836343944    Test Loss :0.0010381016181781888\n",
      "Epoch :0.2833333333333333    Train Loss :0.0005280448822304606    Test Loss :0.00100751465652138\n",
      "Epoch :0.2916666666666667    Train Loss :0.0004821958136744797    Test Loss :0.0011293429415673018\n",
      "Epoch :0.3    Train Loss :0.000496060005389154    Test Loss :0.0009820503182709217\n",
      "Epoch :0.30833333333333335    Train Loss :0.00046396569814532995    Test Loss :0.0010513952001929283\n",
      "Epoch :0.31666666666666665    Train Loss :0.0004760974261444062    Test Loss :0.0009745443821884692\n",
      "Epoch :0.325    Train Loss :0.0004567256255540997    Test Loss :0.001003179349936545\n",
      "Epoch :0.3333333333333333    Train Loss :0.0004618726670742035    Test Loss :0.0009822876891121268\n",
      "Epoch :0.3416666666666667    Train Loss :0.0004779727023560554    Test Loss :0.0009502346510998905\n",
      "Epoch :0.35    Train Loss :0.00044136171345598996    Test Loss :0.0009029401117004454\n",
      "Epoch :0.35833333333333334    Train Loss :0.00044488866114988923    Test Loss :0.0009364919969812036\n",
      "Epoch :0.36666666666666664    Train Loss :0.00045575652620755136    Test Loss :0.0009352412889711559\n",
      "Epoch :0.375    Train Loss :0.00042197108268737793    Test Loss :0.0008376890327781439\n",
      "Epoch :0.38333333333333336    Train Loss :0.00041418964974582195    Test Loss :0.0008778319461271167\n",
      "Epoch :0.39166666666666666    Train Loss :0.00042938970727846026    Test Loss :0.0008077183738350868\n",
      "Epoch :0.4    Train Loss :0.0004441077762749046    Test Loss :0.0008767886902205646\n",
      "Epoch :0.4083333333333333    Train Loss :0.00043239258229732513    Test Loss :0.0009202416404150426\n",
      "Epoch :0.4166666666666667    Train Loss :0.0004334757977630943    Test Loss :0.0007845425861887634\n",
      "Epoch :0.425    Train Loss :0.0003992820857092738    Test Loss :0.0008227420621551573\n",
      "Epoch :0.43333333333333335    Train Loss :0.00042282763752155006    Test Loss :0.00083106046076864\n",
      "Epoch :0.44166666666666665    Train Loss :0.00040361867286264896    Test Loss :0.0007841464830562472\n",
      "Epoch :0.45    Train Loss :0.0004119411751162261    Test Loss :0.000795737374573946\n",
      "Epoch :0.4583333333333333    Train Loss :0.0004269921046216041    Test Loss :0.000816848361864686\n",
      "Epoch :0.4666666666666667    Train Loss :0.00038803054485470057    Test Loss :0.0007729143253527582\n",
      "Epoch :0.475    Train Loss :0.00040388613706454635    Test Loss :0.0008111187489703298\n",
      "Epoch :0.48333333333333334    Train Loss :0.00038615093217231333    Test Loss :0.0008637905120849609\n",
      "Epoch :0.49166666666666664    Train Loss :0.0004000373592134565    Test Loss :0.0008506143931299448\n",
      "Epoch :0.5    Train Loss :0.00041754814446903765    Test Loss :0.0007755441474728286\n",
      "Epoch :0.5083333333333333    Train Loss :0.00039371964521706104    Test Loss :0.0007473562145605683\n",
      "Epoch :0.5166666666666667    Train Loss :0.00044095999328419566    Test Loss :0.000721264339517802\n",
      "Epoch :0.525    Train Loss :0.0004027987306471914    Test Loss :0.0008397191413678229\n",
      "Epoch :0.5333333333333333    Train Loss :0.00038454684545286    Test Loss :0.0007887115934863687\n",
      "Epoch :0.5416666666666666    Train Loss :0.0003991044359281659    Test Loss :0.0007519343635067344\n",
      "Epoch :0.55    Train Loss :0.00041224207961931825    Test Loss :0.0006802439456805587\n",
      "Epoch :0.5583333333333333    Train Loss :0.00042089688940905035    Test Loss :0.0007298607379198074\n",
      "Epoch :0.5666666666666667    Train Loss :0.0003769707400351763    Test Loss :0.0008312320569530129\n",
      "Epoch :0.575    Train Loss :0.00038952118484303355    Test Loss :0.0007749953656457365\n",
      "Epoch :0.5833333333333334    Train Loss :0.00038827143725939095    Test Loss :0.0006956057623028755\n",
      "Epoch :0.5916666666666667    Train Loss :0.00038147452869452536    Test Loss :0.0007349550141952932\n",
      "Epoch :0.6    Train Loss :0.00037008998333476484    Test Loss :0.0007203273708000779\n",
      "Epoch :0.6083333333333333    Train Loss :0.00036459852708503604    Test Loss :0.0007162057445384562\n",
      "Epoch :0.6166666666666667    Train Loss :0.0003713809128385037    Test Loss :0.0006768153980374336\n",
      "Epoch :0.625    Train Loss :0.0003753125201910734    Test Loss :0.0007104360265657306\n",
      "Epoch :0.6333333333333333    Train Loss :0.00038762210169807076    Test Loss :0.000672076886985451\n",
      "Epoch :0.6416666666666667    Train Loss :0.00037354641244746745    Test Loss :0.0007094319444149733\n",
      "Epoch :0.65    Train Loss :0.0003748478484340012    Test Loss :0.0006838382105343044\n",
      "Epoch :0.6583333333333333    Train Loss :0.0003717852523550391    Test Loss :0.000682723824866116\n",
      "Epoch :0.6666666666666666    Train Loss :0.00037279893876984715    Test Loss :0.0007469364791177213\n",
      "Epoch :0.675    Train Loss :0.00034962271456606686    Test Loss :0.0008036436629481614\n",
      "Epoch :0.6833333333333333    Train Loss :0.0003737967344932258    Test Loss :0.0007861618651077151\n",
      "Epoch :0.6916666666666667    Train Loss :0.0003730722819454968    Test Loss :0.0008039415115490556\n",
      "Epoch :0.7    Train Loss :0.0003709250595420599    Test Loss :0.0007324931211769581\n",
      "Epoch :0.7083333333333334    Train Loss :0.0003688105789478868    Test Loss :0.0006911252858117223\n",
      "Epoch :0.7166666666666667    Train Loss :0.00036007812013849616    Test Loss :0.0007062961813062429\n",
      "Epoch :0.725    Train Loss :0.0003638604248408228    Test Loss :0.0006847881595604122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7333333333333333    Train Loss :0.00035143576678819954    Test Loss :0.0007030964479781687\n",
      "Epoch :0.7416666666666667    Train Loss :0.00035851338179782033    Test Loss :0.0006789201288484037\n",
      "Epoch :0.75    Train Loss :0.0003473816905170679    Test Loss :0.0006595978047698736\n",
      "Epoch :0.7583333333333333    Train Loss :0.00034768422483466566    Test Loss :0.0006167232641018927\n",
      "Epoch :0.7666666666666667    Train Loss :0.0003393458027858287    Test Loss :0.0006885419134050608\n",
      "Epoch :0.775    Train Loss :0.0003474568366073072    Test Loss :0.0006084796623326838\n",
      "Epoch :0.7833333333333333    Train Loss :0.0003319202223792672    Test Loss :0.000632879207842052\n",
      "Epoch :0.7916666666666666    Train Loss :0.0003382029535714537    Test Loss :0.000613563577644527\n",
      "Epoch :0.8    Train Loss :0.00033129469375126064    Test Loss :0.0006498302100226283\n",
      "Epoch :0.8083333333333333    Train Loss :0.0003383425937499851    Test Loss :0.0006104406784288585\n",
      "Epoch :0.8166666666666667    Train Loss :0.0004160308453720063    Test Loss :0.0006382197607308626\n",
      "Epoch :0.825    Train Loss :0.0003874664253089577    Test Loss :0.000710394000634551\n",
      "Epoch :0.8333333333333334    Train Loss :0.00032654739334248006    Test Loss :0.0005938561516813934\n",
      "Epoch :0.8416666666666667    Train Loss :0.0003267749270889908    Test Loss :0.0005987429176457226\n",
      "Epoch :0.85    Train Loss :0.0003268379077780992    Test Loss :0.000588812050409615\n",
      "Epoch :0.8583333333333333    Train Loss :0.0003153947473037988    Test Loss :0.0006406106986105442\n",
      "Epoch :0.8666666666666667    Train Loss :0.0003219303034711629    Test Loss :0.0005802836967632174\n",
      "Epoch :0.875    Train Loss :0.00030198684544302523    Test Loss :0.0005958713009022176\n",
      "Epoch :0.8833333333333333    Train Loss :0.0003698859363794327    Test Loss :0.0008045911672525108\n",
      "Epoch :0.8916666666666667    Train Loss :0.000329174188664183    Test Loss :0.0005850385059602559\n",
      "Epoch :0.9    Train Loss :0.0003387297037988901    Test Loss :0.000576309219468385\n",
      "Epoch :0.9083333333333333    Train Loss :0.00030417487141676247    Test Loss :0.0005489217583090067\n",
      "Epoch :0.9166666666666666    Train Loss :0.0003394869854673743    Test Loss :0.0007908397237770259\n",
      "Epoch :0.925    Train Loss :0.00032077819923870265    Test Loss :0.0005403598770499229\n",
      "Epoch :0.9333333333333333    Train Loss :0.00030626103398390114    Test Loss :0.0005448284209705889\n",
      "Epoch :0.9416666666666667    Train Loss :0.00029554779757745564    Test Loss :0.0005763681256212294\n",
      "Epoch :0.95    Train Loss :0.0004164155980106443    Test Loss :0.0014116719830781221\n",
      "Epoch :0.9583333333333334    Train Loss :0.0004203608550596982    Test Loss :0.0005388610297814012\n",
      "Epoch :0.9666666666666667    Train Loss :0.00033537379931658506    Test Loss :0.0006033274112269282\n",
      "Epoch :0.975    Train Loss :0.00029733829433098435    Test Loss :0.0006993430433794856\n",
      "Epoch :0.9833333333333333    Train Loss :0.00031302502611652017    Test Loss :0.0007154206396080554\n",
      "Epoch :0.9916666666666667    Train Loss :0.0003194524033460766    Test Loss :0.0006100790924392641\n",
      "Epoch :1.0    Train Loss :0.0002913873177021742    Test Loss :0.000547054864000529\n",
      "RMSE: 8.150303122308145\n",
      "MAE: 6.612522815625509\n",
      "MAPE: 5.886957926500499%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 600, 'hid_size': 100, 'lr': 0.01, 'num_layers': 2}\n",
      "total:  41.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.14938028156757355    Test Loss :0.48877930641174316\n",
      "Epoch :0.016666666666666666    Train Loss :0.10545220226049423    Test Loss :0.10509846359491348\n",
      "Epoch :0.025    Train Loss :0.06436758488416672    Test Loss :0.21595296263694763\n",
      "Epoch :0.03333333333333333    Train Loss :0.05324991047382355    Test Loss :0.10452128946781158\n",
      "Epoch :0.041666666666666664    Train Loss :0.051861535757780075    Test Loss :0.08902931958436966\n",
      "Epoch :0.05    Train Loss :0.050831638276576996    Test Loss :0.13123352825641632\n",
      "Epoch :0.058333333333333334    Train Loss :0.046028196811676025    Test Loss :0.08762248605489731\n",
      "Epoch :0.06666666666666667    Train Loss :0.02625286392867565    Test Loss :0.024763718247413635\n",
      "Epoch :0.075    Train Loss :0.01947993040084839    Test Loss :0.0815492570400238\n",
      "Epoch :0.08333333333333333    Train Loss :0.010961201973259449    Test Loss :0.01600361056625843\n",
      "Epoch :0.09166666666666666    Train Loss :0.009269936941564083    Test Loss :0.03451908007264137\n",
      "Epoch :0.1    Train Loss :0.0069454871118068695    Test Loss :0.010946397669613361\n",
      "Epoch :0.10833333333333334    Train Loss :0.0065008532255887985    Test Loss :0.011046670377254486\n",
      "Epoch :0.11666666666666667    Train Loss :0.005075750406831503    Test Loss :0.006850991398096085\n",
      "Epoch :0.125    Train Loss :0.0027044001035392284    Test Loss :0.004366803914308548\n",
      "Epoch :0.13333333333333333    Train Loss :0.0028506333474069834    Test Loss :0.003900487208738923\n",
      "Epoch :0.14166666666666666    Train Loss :0.0024345158599317074    Test Loss :0.005072969011962414\n",
      "Epoch :0.15    Train Loss :0.0022230662871152163    Test Loss :0.003773153992369771\n",
      "Epoch :0.15833333333333333    Train Loss :0.001649760757572949    Test Loss :0.0037201608065515757\n",
      "Epoch :0.16666666666666666    Train Loss :0.0017316598678007722    Test Loss :0.0036493379157036543\n",
      "Epoch :0.175    Train Loss :0.001453500590287149    Test Loss :0.0033973895478993654\n",
      "Epoch :0.18333333333333332    Train Loss :0.0013484727824106812    Test Loss :0.0031149163842201233\n",
      "Epoch :0.19166666666666668    Train Loss :0.001342577626928687    Test Loss :0.0034004126209765673\n",
      "Epoch :0.2    Train Loss :0.0012894717510789633    Test Loss :0.0026909930165857077\n",
      "Epoch :0.20833333333333334    Train Loss :0.0011850495357066393    Test Loss :0.0029797935858368874\n",
      "Epoch :0.21666666666666667    Train Loss :0.0011835290351882577    Test Loss :0.0027200356125831604\n",
      "Epoch :0.225    Train Loss :0.0011782285291701555    Test Loss :0.002671873429790139\n",
      "Epoch :0.23333333333333334    Train Loss :0.0010722657898440957    Test Loss :0.0025831409730017185\n",
      "Epoch :0.24166666666666667    Train Loss :0.0010745119070634246    Test Loss :0.002422170015051961\n",
      "Epoch :0.25    Train Loss :0.001068056095391512    Test Loss :0.0024235271848738194\n",
      "Epoch :0.25833333333333336    Train Loss :0.0010741373989731073    Test Loss :0.0024021046701818705\n",
      "Epoch :0.26666666666666666    Train Loss :0.001030594576150179    Test Loss :0.0022709721233695745\n",
      "Epoch :0.275    Train Loss :0.0009878656128421426    Test Loss :0.0022297189570963383\n",
      "Epoch :0.2833333333333333    Train Loss :0.0008917151717469096    Test Loss :0.0022328635677695274\n",
      "Epoch :0.2916666666666667    Train Loss :0.0008611382218077779    Test Loss :0.002208265010267496\n",
      "Epoch :0.3    Train Loss :0.0009339380194433033    Test Loss :0.002166618127375841\n",
      "Epoch :0.30833333333333335    Train Loss :0.0008244779310189188    Test Loss :0.002148554427549243\n",
      "Epoch :0.31666666666666665    Train Loss :0.0008556987741030753    Test Loss :0.0019736981485038996\n",
      "Epoch :0.325    Train Loss :0.0008676861179992557    Test Loss :0.0019968273118138313\n",
      "Epoch :0.3333333333333333    Train Loss :0.0008201606688089669    Test Loss :0.0020575718954205513\n",
      "Epoch :0.3416666666666667    Train Loss :0.0008063392015174031    Test Loss :0.0020600350107997656\n",
      "Epoch :0.35    Train Loss :0.0007739114225842059    Test Loss :0.0019488161196932197\n",
      "Epoch :0.35833333333333334    Train Loss :0.000775895721744746    Test Loss :0.0020360478665679693\n",
      "Epoch :0.36666666666666664    Train Loss :0.0007938016206026077    Test Loss :0.00218337494879961\n",
      "Epoch :0.375    Train Loss :0.0007813224219717085    Test Loss :0.0020570058841258287\n",
      "Epoch :0.38333333333333336    Train Loss :0.0007140100351534784    Test Loss :0.0020997230894863605\n",
      "Epoch :0.39166666666666666    Train Loss :0.0007274288218468428    Test Loss :0.0020161757711321115\n",
      "Epoch :0.4    Train Loss :0.0007017003954388201    Test Loss :0.0021980558522045612\n",
      "Epoch :0.4083333333333333    Train Loss :0.0007326978375203907    Test Loss :0.001885392819531262\n",
      "Epoch :0.4166666666666667    Train Loss :0.0006962062907405198    Test Loss :0.001808152999728918\n",
      "Epoch :0.425    Train Loss :0.0006696656928397715    Test Loss :0.002014029072597623\n",
      "Epoch :0.43333333333333335    Train Loss :0.0006799965049140155    Test Loss :0.0019232092890888453\n",
      "Epoch :0.44166666666666665    Train Loss :0.0006509615341201425    Test Loss :0.002036166610196233\n",
      "Epoch :0.45    Train Loss :0.000648067332804203    Test Loss :0.0019567208364605904\n",
      "Epoch :0.4583333333333333    Train Loss :0.0006294293561950326    Test Loss :0.0019753361120820045\n",
      "Epoch :0.4666666666666667    Train Loss :0.0006579569890163839    Test Loss :0.0020282843615859747\n",
      "Epoch :0.475    Train Loss :0.000645724474452436    Test Loss :0.0019267275929450989\n",
      "Epoch :0.48333333333333334    Train Loss :0.000627723231445998    Test Loss :0.0019234326900914311\n",
      "Epoch :0.49166666666666664    Train Loss :0.0006222197553142905    Test Loss :0.002156144706532359\n",
      "Epoch :0.5    Train Loss :0.0005995508399792016    Test Loss :0.0019504014635458589\n",
      "Epoch :0.5083333333333333    Train Loss :0.000661446072626859    Test Loss :0.0018050017533823848\n",
      "Epoch :0.5166666666666667    Train Loss :0.0005898671224713326    Test Loss :0.00209771073423326\n",
      "Epoch :0.525    Train Loss :0.000649292953312397    Test Loss :0.0019576610065996647\n",
      "Epoch :0.5333333333333333    Train Loss :0.0005414907936938107    Test Loss :0.0019074800657108426\n",
      "Epoch :0.5416666666666666    Train Loss :0.0005699727917090058    Test Loss :0.0017960254335775971\n",
      "Epoch :0.55    Train Loss :0.0005846215644851327    Test Loss :0.0018292446620762348\n",
      "Epoch :0.5583333333333333    Train Loss :0.000604113913141191    Test Loss :0.0017670764354988933\n",
      "Epoch :0.5666666666666667    Train Loss :0.0005886658327654004    Test Loss :0.001713114557787776\n",
      "Epoch :0.575    Train Loss :0.0005646045319736004    Test Loss :0.0018216314492747188\n",
      "Epoch :0.5833333333333334    Train Loss :0.000563585665076971    Test Loss :0.0017679150914773345\n",
      "Epoch :0.5916666666666667    Train Loss :0.0005871681496500969    Test Loss :0.0020177371334284544\n",
      "Epoch :0.6    Train Loss :0.0005949467304162681    Test Loss :0.0017516365041956306\n",
      "Epoch :0.6083333333333333    Train Loss :0.0005217228317633271    Test Loss :0.0018533773254603148\n",
      "Epoch :0.6166666666666667    Train Loss :0.00051492900820449    Test Loss :0.001810734742321074\n",
      "Epoch :0.625    Train Loss :0.0005230366950854659    Test Loss :0.00168275844771415\n",
      "Epoch :0.6333333333333333    Train Loss :0.0005485450965352356    Test Loss :0.0016219643875956535\n",
      "Epoch :0.6416666666666667    Train Loss :0.0005293391295708716    Test Loss :0.0017053793417289853\n",
      "Epoch :0.65    Train Loss :0.0004889071569778025    Test Loss :0.001699228072538972\n",
      "Epoch :0.6583333333333333    Train Loss :0.0005486364243552089    Test Loss :0.0017083834391087294\n",
      "Epoch :0.6666666666666666    Train Loss :0.0005289971013553441    Test Loss :0.0016629161546006799\n",
      "Epoch :0.675    Train Loss :0.0005371780134737492    Test Loss :0.0016828940715640783\n",
      "Epoch :0.6833333333333333    Train Loss :0.0005071357591077685    Test Loss :0.0015351797919720411\n",
      "Epoch :0.6916666666666667    Train Loss :0.0005365703837014735    Test Loss :0.0015812920173630118\n",
      "Epoch :0.7    Train Loss :0.0004660868726205081    Test Loss :0.0014545699814334512\n",
      "Epoch :0.7083333333333334    Train Loss :0.0011074378853663802    Test Loss :0.0027839059475809336\n",
      "Epoch :0.7166666666666667    Train Loss :0.0007522586383856833    Test Loss :0.0026300365570932627\n",
      "Epoch :0.725    Train Loss :0.0006692399038001895    Test Loss :0.0026311392430216074\n",
      "Epoch :0.7333333333333333    Train Loss :0.000586366280913353    Test Loss :0.0021272755693644285\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7416666666666667    Train Loss :0.0005685915239155293    Test Loss :0.0019007098162546754\n",
      "Epoch :0.75    Train Loss :0.0004828298115171492    Test Loss :0.0015349335735663772\n",
      "Epoch :0.7583333333333333    Train Loss :0.0004535646876320243    Test Loss :0.001562940189614892\n",
      "Epoch :0.7666666666666667    Train Loss :0.0004842269408982247    Test Loss :0.0013850340619683266\n",
      "Epoch :0.775    Train Loss :0.00044406813685782254    Test Loss :0.0013208704767748713\n",
      "Epoch :0.7833333333333333    Train Loss :0.0004673366784118116    Test Loss :0.0013354802504181862\n",
      "Epoch :0.7916666666666666    Train Loss :0.00047198738320730627    Test Loss :0.0013670516200363636\n",
      "Epoch :0.8    Train Loss :0.0004313152749091387    Test Loss :0.0013022110797464848\n",
      "Epoch :0.8083333333333333    Train Loss :0.0003931335231754929    Test Loss :0.0013714416418224573\n",
      "Epoch :0.8166666666666667    Train Loss :0.0004347130306996405    Test Loss :0.001284645521081984\n",
      "Epoch :0.825    Train Loss :0.00044431176502257586    Test Loss :0.0014147077454254031\n",
      "Epoch :0.8333333333333334    Train Loss :0.0004365901695564389    Test Loss :0.0013736747205257416\n",
      "Epoch :0.8416666666666667    Train Loss :0.0004394516290631145    Test Loss :0.0013892099959775805\n",
      "Epoch :0.85    Train Loss :0.0004086797998752445    Test Loss :0.0013956030597910285\n",
      "Epoch :0.8583333333333333    Train Loss :0.00041978713124990463    Test Loss :0.0013392016990110278\n",
      "Epoch :0.8666666666666667    Train Loss :0.00041867440450005233    Test Loss :0.0013178149238228798\n",
      "Epoch :0.875    Train Loss :0.0004146573191974312    Test Loss :0.0012358853127807379\n",
      "Epoch :0.8833333333333333    Train Loss :0.00040200649527832866    Test Loss :0.0012167863314971328\n",
      "Epoch :0.8916666666666667    Train Loss :0.0005549844354391098    Test Loss :0.001454471843317151\n",
      "Epoch :0.9    Train Loss :0.0005634778644889593    Test Loss :0.0015461491420865059\n",
      "Epoch :0.9083333333333333    Train Loss :0.00043484530760906637    Test Loss :0.001620599301531911\n",
      "Epoch :0.9166666666666666    Train Loss :0.0004659008118323982    Test Loss :0.0017275229329243302\n",
      "Epoch :0.925    Train Loss :0.0004543876275420189    Test Loss :0.001650480437092483\n",
      "Epoch :0.9333333333333333    Train Loss :0.0004150599124841392    Test Loss :0.0015060940058901906\n",
      "Epoch :0.9416666666666667    Train Loss :0.0003926766512449831    Test Loss :0.0013251891359686852\n",
      "Epoch :0.95    Train Loss :0.00037540614721365273    Test Loss :0.0012270809384062886\n",
      "Epoch :0.9583333333333334    Train Loss :0.0003819938865490258    Test Loss :0.0010817040456458926\n",
      "Epoch :0.9666666666666667    Train Loss :0.00038261449662968516    Test Loss :0.0011647294741123915\n",
      "Epoch :0.975    Train Loss :0.0003938425797969103    Test Loss :0.001149054616689682\n",
      "Epoch :0.9833333333333333    Train Loss :0.00038339945604093373    Test Loss :0.001197262667119503\n",
      "Epoch :0.9916666666666667    Train Loss :0.0005297462339513004    Test Loss :0.001545096398331225\n",
      "Epoch :1.0    Train Loss :0.0003946651122532785    Test Loss :0.0015368752647191286\n",
      "RMSE: 9.379780879884372\n",
      "MAE: 7.760976501605415\n",
      "MAPE: 7.091902597653664%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 600, 'hid_size': 100, 'lr': 0.01, 'num_layers': 5}\n",
      "total:  44.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.1552944928407669    Test Loss :0.1136569008231163\n",
      "Epoch :0.016666666666666666    Train Loss :0.15420472621917725    Test Loss :0.3383358120918274\n",
      "Epoch :0.025    Train Loss :0.04695456847548485    Test Loss :0.06859320402145386\n",
      "Epoch :0.03333333333333333    Train Loss :0.04919782280921936    Test Loss :0.08732616901397705\n",
      "Epoch :0.041666666666666664    Train Loss :0.05069824680685997    Test Loss :0.13924269378185272\n",
      "Epoch :0.05    Train Loss :0.045496463775634766    Test Loss :0.09393816441297531\n",
      "Epoch :0.058333333333333334    Train Loss :0.04449237883090973    Test Loss :0.05986162647604942\n",
      "Epoch :0.06666666666666667    Train Loss :0.03763900324702263    Test Loss :0.06049247086048126\n",
      "Epoch :0.075    Train Loss :0.018449628725647926    Test Loss :0.018265925347805023\n",
      "Epoch :0.08333333333333333    Train Loss :0.009575783275067806    Test Loss :0.00953667238354683\n",
      "Epoch :0.09166666666666666    Train Loss :0.0058575887233018875    Test Loss :0.02484278194606304\n",
      "Epoch :0.1    Train Loss :0.003556670155376196    Test Loss :0.010760633274912834\n",
      "Epoch :0.10833333333333334    Train Loss :0.002240600297227502    Test Loss :0.00456532696262002\n",
      "Epoch :0.11666666666666667    Train Loss :0.0015126861399039626    Test Loss :0.002542475238442421\n",
      "Epoch :0.125    Train Loss :0.0014552066568285227    Test Loss :0.0032610492780804634\n",
      "Epoch :0.13333333333333333    Train Loss :0.0013840014580637217    Test Loss :0.0018776421202346683\n",
      "Epoch :0.14166666666666666    Train Loss :0.0011572352377697825    Test Loss :0.0018370330799371004\n",
      "Epoch :0.15    Train Loss :0.0010217062663286924    Test Loss :0.0019188753794878721\n",
      "Epoch :0.15833333333333333    Train Loss :0.0008930854965001345    Test Loss :0.0020690145902335644\n",
      "Epoch :0.16666666666666666    Train Loss :0.000888279580976814    Test Loss :0.002531512873247266\n",
      "Epoch :0.175    Train Loss :0.0008368168491870165    Test Loss :0.0020874531473964453\n",
      "Epoch :0.18333333333333332    Train Loss :0.0008074587094597518    Test Loss :0.0018965572817251086\n",
      "Epoch :0.19166666666666668    Train Loss :0.0007621847908012569    Test Loss :0.0018673223676159978\n",
      "Epoch :0.2    Train Loss :0.0007527481648139656    Test Loss :0.0018500740407034755\n",
      "Epoch :0.20833333333333334    Train Loss :0.0007058338378556073    Test Loss :0.0016219901153817773\n",
      "Epoch :0.21666666666666667    Train Loss :0.0006543007912114263    Test Loss :0.0017395407194271684\n",
      "Epoch :0.225    Train Loss :0.0006632698932662606    Test Loss :0.0017543333815410733\n",
      "Epoch :0.23333333333333334    Train Loss :0.000665330037008971    Test Loss :0.0017175603425130248\n",
      "Epoch :0.24166666666666667    Train Loss :0.00063166837207973    Test Loss :0.0017246215138584375\n",
      "Epoch :0.25    Train Loss :0.0005960945272818208    Test Loss :0.0017049298621714115\n",
      "Epoch :0.25833333333333336    Train Loss :0.0005713952705264091    Test Loss :0.0015871379291638732\n",
      "Epoch :0.26666666666666666    Train Loss :0.0006114876014180481    Test Loss :0.0016222115373238921\n",
      "Epoch :0.275    Train Loss :0.000570533680729568    Test Loss :0.0015880492283031344\n",
      "Epoch :0.2833333333333333    Train Loss :0.0005704343202523887    Test Loss :0.0015169911785051227\n",
      "Epoch :0.2916666666666667    Train Loss :0.0005278976750560105    Test Loss :0.0015256534097716212\n",
      "Epoch :0.3    Train Loss :0.0005419030203483999    Test Loss :0.0016460028709843755\n",
      "Epoch :0.30833333333333335    Train Loss :0.0005240312893874943    Test Loss :0.001550234854221344\n",
      "Epoch :0.31666666666666665    Train Loss :0.0005345738609321415    Test Loss :0.0015844055451452732\n",
      "Epoch :0.325    Train Loss :0.0005095364758744836    Test Loss :0.0015617174794897437\n",
      "Epoch :0.3333333333333333    Train Loss :0.0004861963680014014    Test Loss :0.0014734305441379547\n",
      "Epoch :0.3416666666666667    Train Loss :0.0004914353485219181    Test Loss :0.001441463129594922\n",
      "Epoch :0.35    Train Loss :0.0004907343536615372    Test Loss :0.0015636550961062312\n",
      "Epoch :0.35833333333333334    Train Loss :0.00048514740774407983    Test Loss :0.001481805695220828\n",
      "Epoch :0.36666666666666664    Train Loss :0.0004661930724978447    Test Loss :0.0015314760385081172\n",
      "Epoch :0.375    Train Loss :0.0004409282701089978    Test Loss :0.0014534059446305037\n",
      "Epoch :0.38333333333333336    Train Loss :0.0004896107129752636    Test Loss :0.0014730560360476375\n",
      "Epoch :0.39166666666666666    Train Loss :0.0004622454580385238    Test Loss :0.001492256298661232\n",
      "Epoch :0.4    Train Loss :0.000452208099886775    Test Loss :0.001474910182878375\n",
      "Epoch :0.4083333333333333    Train Loss :0.0004544838157016784    Test Loss :0.0015209598932415247\n",
      "Epoch :0.4166666666666667    Train Loss :0.00044274734682403505    Test Loss :0.0014555457746610045\n",
      "Epoch :0.425    Train Loss :0.0004421528137754649    Test Loss :0.001577944029122591\n",
      "Epoch :0.43333333333333335    Train Loss :0.00045542774023488164    Test Loss :0.0014971480704843998\n",
      "Epoch :0.44166666666666665    Train Loss :0.0004139220400247723    Test Loss :0.0015017734840512276\n",
      "Epoch :0.45    Train Loss :0.00042754257447086275    Test Loss :0.0014782792422920465\n",
      "Epoch :0.4583333333333333    Train Loss :0.0004276200197637081    Test Loss :0.0014587007462978363\n",
      "Epoch :0.4666666666666667    Train Loss :0.00040715947397984564    Test Loss :0.00140658940654248\n",
      "Epoch :0.475    Train Loss :0.00043061195174232125    Test Loss :0.0014356684405356646\n",
      "Epoch :0.48333333333333334    Train Loss :0.00041119419620372355    Test Loss :0.0014546321472153068\n",
      "Epoch :0.49166666666666664    Train Loss :0.00038262869929894805    Test Loss :0.0014046402648091316\n",
      "Epoch :0.5    Train Loss :0.00040705251740291715    Test Loss :0.0013722330331802368\n",
      "Epoch :0.5083333333333333    Train Loss :0.00041377890738658607    Test Loss :0.0013837243895977736\n",
      "Epoch :0.5166666666666667    Train Loss :0.00040068369708023965    Test Loss :0.0013617378426715732\n",
      "Epoch :0.525    Train Loss :0.000390336150303483    Test Loss :0.0013787084026262164\n",
      "Epoch :0.5333333333333333    Train Loss :0.0003857730480376631    Test Loss :0.001363586401566863\n",
      "Epoch :0.5416666666666666    Train Loss :0.0003974463324993849    Test Loss :0.001390406978316605\n",
      "Epoch :0.55    Train Loss :0.00039375407504849136    Test Loss :0.0013382135657593608\n",
      "Epoch :0.5583333333333333    Train Loss :0.000386775063816458    Test Loss :0.0013552852906286716\n",
      "Epoch :0.5666666666666667    Train Loss :0.00038638964178971946    Test Loss :0.0013526309048756957\n",
      "Epoch :0.575    Train Loss :0.0003690804878715426    Test Loss :0.001337289111688733\n",
      "Epoch :0.5833333333333334    Train Loss :0.00036113575333729386    Test Loss :0.0013238739920780063\n",
      "Epoch :0.5916666666666667    Train Loss :0.0004789623781107366    Test Loss :0.0013269379269331694\n",
      "Epoch :0.6    Train Loss :0.00042217227746732533    Test Loss :0.0013499936321750283\n",
      "Epoch :0.6083333333333333    Train Loss :0.00048632349353283644    Test Loss :0.0014521932462230325\n",
      "Epoch :0.6166666666666667    Train Loss :0.00047215327504090965    Test Loss :0.0014246613718569279\n",
      "Epoch :0.625    Train Loss :0.00039413318154402077    Test Loss :0.0013748555211350322\n",
      "Epoch :0.6333333333333333    Train Loss :0.0003578819159884006    Test Loss :0.0013305022148415446\n",
      "Epoch :0.6416666666666667    Train Loss :0.00037118259933777153    Test Loss :0.0012637015897780657\n",
      "Epoch :0.65    Train Loss :0.0003586243255995214    Test Loss :0.001297311158850789\n",
      "Epoch :0.6583333333333333    Train Loss :0.0003792699717450887    Test Loss :0.0012391762575134635\n",
      "Epoch :0.6666666666666666    Train Loss :0.0003873835375998169    Test Loss :0.0012848242186009884\n",
      "Epoch :0.675    Train Loss :0.0003884419857058674    Test Loss :0.0014124928275123239\n",
      "Epoch :0.6833333333333333    Train Loss :0.00032889808062464    Test Loss :0.0012822207063436508\n",
      "Epoch :0.6916666666666667    Train Loss :0.00045340575161390007    Test Loss :0.001290509826503694\n",
      "Epoch :0.7    Train Loss :0.000338166079018265    Test Loss :0.0012362721608951688\n",
      "Epoch :0.7083333333333334    Train Loss :0.0006139981560409069    Test Loss :0.0015433724038302898\n",
      "Epoch :0.7166666666666667    Train Loss :0.0003311792097520083    Test Loss :0.001404163078404963\n",
      "Epoch :0.725    Train Loss :0.00036088417982682586    Test Loss :0.001298335730098188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7333333333333333    Train Loss :0.0003369928163010627    Test Loss :0.0013187911827117205\n",
      "Epoch :0.7416666666666667    Train Loss :0.0003774570650421083    Test Loss :0.0012017784174531698\n",
      "Epoch :0.75    Train Loss :0.00046612878213636577    Test Loss :0.0012234391178935766\n",
      "Epoch :0.7583333333333333    Train Loss :0.0004178682283964008    Test Loss :0.0012109715025871992\n",
      "Epoch :0.7666666666666667    Train Loss :0.00032098242081701756    Test Loss :0.00122963753528893\n",
      "Epoch :0.775    Train Loss :0.00033422381966374815    Test Loss :0.0012310919119045138\n",
      "Epoch :0.7833333333333333    Train Loss :0.0003656285407487303    Test Loss :0.001338957459665835\n",
      "Epoch :0.7916666666666666    Train Loss :0.0003224615065846592    Test Loss :0.001164324814453721\n",
      "Epoch :0.8    Train Loss :0.0004778303555212915    Test Loss :0.0012382931308820844\n",
      "Epoch :0.8083333333333333    Train Loss :0.0003372477658558637    Test Loss :0.0011796907056123018\n",
      "Epoch :0.8166666666666667    Train Loss :0.0004835619474761188    Test Loss :0.0014313574647530913\n",
      "Epoch :0.825    Train Loss :0.0002963078150060028    Test Loss :0.0011895936913788319\n",
      "Epoch :0.8333333333333334    Train Loss :0.00034094465081579983    Test Loss :0.0011771592544391751\n",
      "Epoch :0.8416666666666667    Train Loss :0.0003036237903870642    Test Loss :0.0011433634208515286\n",
      "Epoch :0.85    Train Loss :0.00036667881067842245    Test Loss :0.0011622494785115123\n",
      "Epoch :0.8583333333333333    Train Loss :0.0003793108626268804    Test Loss :0.0012862251605838537\n",
      "Epoch :0.8666666666666667    Train Loss :0.0003004190803039819    Test Loss :0.001209557638503611\n",
      "Epoch :0.875    Train Loss :0.0002903618151322007    Test Loss :0.001173715339973569\n",
      "Epoch :0.8833333333333333    Train Loss :0.0002880225074477494    Test Loss :0.0011214654659852386\n",
      "Epoch :0.8916666666666667    Train Loss :0.00029847995028831065    Test Loss :0.0011464079143479466\n",
      "Epoch :0.9    Train Loss :0.0006618529441766441    Test Loss :0.0022782664746046066\n",
      "Epoch :0.9083333333333333    Train Loss :0.0008592645172029734    Test Loss :0.001408901414833963\n",
      "Epoch :0.9166666666666666    Train Loss :0.0004373217234387994    Test Loss :0.002111107110977173\n",
      "Epoch :0.925    Train Loss :0.0003307594743091613    Test Loss :0.0014820066280663013\n",
      "Epoch :0.9333333333333333    Train Loss :0.0004395031719468534    Test Loss :0.001426471397280693\n",
      "Epoch :0.9416666666666667    Train Loss :0.0003985927323810756    Test Loss :0.0016190591268241405\n",
      "Epoch :0.95    Train Loss :0.0003640168870333582    Test Loss :0.00145284878090024\n",
      "Epoch :0.9583333333333334    Train Loss :0.00029862375231459737    Test Loss :0.0015133981360122561\n",
      "Epoch :0.9666666666666667    Train Loss :0.00031406781636178493    Test Loss :0.0012682877713814378\n",
      "Epoch :0.975    Train Loss :0.0003231461450923234    Test Loss :0.0012521366588771343\n",
      "Epoch :0.9833333333333333    Train Loss :0.0003000760043505579    Test Loss :0.0013167235301807523\n",
      "Epoch :0.9916666666666667    Train Loss :0.0003013093664776534    Test Loss :0.0012833861401304603\n",
      "Epoch :1.0    Train Loss :0.0003013735986314714    Test Loss :0.001232779584825039\n",
      "RMSE: 8.975538930017832\n",
      "MAE: 7.336139851768556\n",
      "MAPE: 6.691663453816121%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 600, 'hid_size': 100, 'lr': 0.02, 'num_layers': 2}\n",
      "total:  47.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.16560722887516022    Test Loss :1.8478879928588867\n",
      "Epoch :0.016666666666666666    Train Loss :0.2082974910736084    Test Loss :0.014240533113479614\n",
      "Epoch :0.025    Train Loss :0.10533575713634491    Test Loss :0.18509873747825623\n",
      "Epoch :0.03333333333333333    Train Loss :0.07458935678005219    Test Loss :0.058036137372255325\n",
      "Epoch :0.041666666666666664    Train Loss :0.06340685486793518    Test Loss :0.1435677707195282\n",
      "Epoch :0.05    Train Loss :0.05727061629295349    Test Loss :0.07176879048347473\n",
      "Epoch :0.058333333333333334    Train Loss :0.05394493415951729    Test Loss :0.1275380551815033\n",
      "Epoch :0.06666666666666667    Train Loss :0.05163414403796196    Test Loss :0.0849420502781868\n",
      "Epoch :0.075    Train Loss :0.05008493363857269    Test Loss :0.10955539345741272\n",
      "Epoch :0.08333333333333333    Train Loss :0.04912411421537399    Test Loss :0.10002180188894272\n",
      "Epoch :0.09166666666666666    Train Loss :0.04863967373967171    Test Loss :0.09579923748970032\n",
      "Epoch :0.1    Train Loss :0.04850475862622261    Test Loss :0.11007474362850189\n",
      "Epoch :0.10833333333333334    Train Loss :0.04857718572020531    Test Loss :0.09138213843107224\n",
      "Epoch :0.11666666666666667    Train Loss :0.04865086078643799    Test Loss :0.10881752520799637\n",
      "Epoch :0.125    Train Loss :0.04861246794462204    Test Loss :0.09716930240392685\n",
      "Epoch :0.13333333333333333    Train Loss :0.048531994223594666    Test Loss :0.10054173320531845\n",
      "Epoch :0.14166666666666666    Train Loss :0.04851016402244568    Test Loss :0.10378753393888474\n",
      "Epoch :0.15    Train Loss :0.04852285981178284    Test Loss :0.0980733260512352\n",
      "Epoch :0.15833333333333333    Train Loss :0.04851726070046425    Test Loss :0.10199384391307831\n",
      "Epoch :0.16666666666666666    Train Loss :0.04849742725491524    Test Loss :0.10171990096569061\n",
      "Epoch :0.175    Train Loss :0.04850433021783829    Test Loss :0.09956714510917664\n",
      "Epoch :0.18333333333333332    Train Loss :0.0485108308494091    Test Loss :0.10145197808742523\n",
      "Epoch :0.19166666666666668    Train Loss :0.04850137233734131    Test Loss :0.1013881266117096\n",
      "Epoch :0.2    Train Loss :0.04850155860185623    Test Loss :0.10031095892190933\n",
      "Epoch :0.20833333333333334    Train Loss :0.04850506782531738    Test Loss :0.10095560550689697\n",
      "Epoch :0.21666666666666667    Train Loss :0.04850050434470177    Test Loss :0.10125557333230972\n",
      "Epoch :0.225    Train Loss :0.04850587993860245    Test Loss :0.10073701292276382\n",
      "Epoch :0.23333333333333334    Train Loss :0.048511069267988205    Test Loss :0.10077472031116486\n",
      "Epoch :0.24166666666666667    Train Loss :0.04849999025464058    Test Loss :0.10107283294200897\n",
      "Epoch :0.25    Train Loss :0.048495613038539886    Test Loss :0.10100078582763672\n",
      "Epoch :0.25833333333333336    Train Loss :0.04850563406944275    Test Loss :0.10083182901144028\n",
      "Epoch :0.26666666666666666    Train Loss :0.048507142812013626    Test Loss :0.10086013376712799\n",
      "Epoch :0.275    Train Loss :0.04850360378623009    Test Loss :0.10097925364971161\n",
      "Epoch :0.2833333333333333    Train Loss :0.04850082844495773    Test Loss :0.10091539472341537\n",
      "Epoch :0.2916666666666667    Train Loss :0.04850165918469429    Test Loss :0.10089409351348877\n",
      "Epoch :0.3    Train Loss :0.04850129410624504    Test Loss :0.10090043395757675\n",
      "Epoch :0.30833333333333335    Train Loss :0.04850117862224579    Test Loss :0.10091383755207062\n",
      "Epoch :0.31666666666666665    Train Loss :0.04849611222743988    Test Loss :0.10092318058013916\n",
      "Epoch :0.325    Train Loss :0.04850251227617264    Test Loss :0.10087254643440247\n",
      "Epoch :0.3333333333333333    Train Loss :0.04850271716713905    Test Loss :0.10092465579509735\n",
      "Epoch :0.3416666666666667    Train Loss :0.04850323498249054    Test Loss :0.10093052685260773\n",
      "Epoch :0.35    Train Loss :0.048503924161195755    Test Loss :0.10090376436710358\n",
      "Epoch :0.35833333333333334    Train Loss :0.048498522490262985    Test Loss :0.10092473030090332\n",
      "Epoch :0.36666666666666664    Train Loss :0.048494841903448105    Test Loss :0.10092059522867203\n",
      "Epoch :0.375    Train Loss :0.048497166484594345    Test Loss :0.1008998304605484\n",
      "Epoch :0.38333333333333336    Train Loss :0.048507142812013626    Test Loss :0.10091952979564667\n",
      "Epoch :0.39166666666666666    Train Loss :0.04851372539997101    Test Loss :0.10092075914144516\n",
      "Epoch :0.4    Train Loss :0.04850352928042412    Test Loss :0.10093176364898682\n",
      "Epoch :0.4083333333333333    Train Loss :0.048505108803510666    Test Loss :0.10090448707342148\n",
      "Epoch :0.4166666666666667    Train Loss :0.048505377024412155    Test Loss :0.10090987384319305\n",
      "Epoch :0.425    Train Loss :0.048500463366508484    Test Loss :0.10090892761945724\n",
      "Epoch :0.43333333333333335    Train Loss :0.048499561846256256    Test Loss :0.1009090319275856\n",
      "Epoch :0.44166666666666665    Train Loss :0.04850279912352562    Test Loss :0.10091044753789902\n",
      "Epoch :0.45    Train Loss :0.0485013872385025    Test Loss :0.1009138822555542\n",
      "Epoch :0.4583333333333333    Train Loss :0.048504799604415894    Test Loss :0.10093868523836136\n",
      "Epoch :0.4666666666666667    Train Loss :0.0485028401017189    Test Loss :0.10094223916530609\n",
      "Epoch :0.475    Train Loss :0.048505570739507675    Test Loss :0.10091928392648697\n",
      "Epoch :0.48333333333333334    Train Loss :0.04850552976131439    Test Loss :0.10090897232294083\n",
      "Epoch :0.49166666666666664    Train Loss :0.04849893972277641    Test Loss :0.10088958591222763\n",
      "Epoch :0.5    Train Loss :0.04850703477859497    Test Loss :0.10091125965118408\n",
      "Epoch :0.5083333333333333    Train Loss :0.0485045425593853    Test Loss :0.10091996192932129\n",
      "Epoch :0.5166666666666667    Train Loss :0.04850779101252556    Test Loss :0.1009223535656929\n",
      "Epoch :0.525    Train Loss :0.04850054904818535    Test Loss :0.10091567039489746\n",
      "Epoch :0.5333333333333333    Train Loss :0.04850049316883087    Test Loss :0.10094592720270157\n",
      "Epoch :0.5416666666666666    Train Loss :0.04850132390856743    Test Loss :0.10092181712388992\n",
      "Epoch :0.55    Train Loss :0.048501428216695786    Test Loss :0.10092732310295105\n",
      "Epoch :0.5583333333333333    Train Loss :0.04850225895643234    Test Loss :0.10091225057840347\n",
      "Epoch :0.5666666666666667    Train Loss :0.0485030934214592    Test Loss :0.10091206431388855\n",
      "Epoch :0.575    Train Loss :0.048506271094083786    Test Loss :0.10091230273246765\n",
      "Epoch :0.5833333333333334    Train Loss :0.04849718138575554    Test Loss :0.10090137273073196\n",
      "Epoch :0.5916666666666667    Train Loss :0.048503972589969635    Test Loss :0.10090653598308563\n",
      "Epoch :0.6    Train Loss :0.04849999025464058    Test Loss :0.1009177640080452\n",
      "Epoch :0.6083333333333333    Train Loss :0.04850669577717781    Test Loss :0.1009133905172348\n",
      "Epoch :0.6166666666666667    Train Loss :0.048501964658498764    Test Loss :0.10091210156679153\n",
      "Epoch :0.625    Train Loss :0.04850558191537857    Test Loss :0.10090979188680649\n",
      "Epoch :0.6333333333333333    Train Loss :0.04850178584456444    Test Loss :0.10089926421642303\n",
      "Epoch :0.6416666666666667    Train Loss :0.04849797859787941    Test Loss :0.10089859366416931\n",
      "Epoch :0.65    Train Loss :0.04850167781114578    Test Loss :0.10088274627923965\n",
      "Epoch :0.6583333333333333    Train Loss :0.048501621931791306    Test Loss :0.10087931901216507\n",
      "Epoch :0.6666666666666666    Train Loss :0.04849863052368164    Test Loss :0.10088697820901871\n",
      "Epoch :0.675    Train Loss :0.04849408566951752    Test Loss :0.10088072717189789\n",
      "Epoch :0.6833333333333333    Train Loss :0.04851103201508522    Test Loss :0.10086064040660858\n",
      "Epoch :0.6916666666666667    Train Loss :0.04851090535521507    Test Loss :0.10085869580507278\n",
      "Epoch :0.7    Train Loss :0.048502739518880844    Test Loss :0.10088000446557999\n",
      "Epoch :0.7083333333333334    Train Loss :0.048501644283533096    Test Loss :0.10086752474308014\n",
      "Epoch :0.7166666666666667    Train Loss :0.04850507155060768    Test Loss :0.10085397958755493\n",
      "Epoch :0.725    Train Loss :0.04849943146109581    Test Loss :0.10088901966810226\n",
      "Epoch :0.7333333333333333    Train Loss :0.0485006682574749    Test Loss :0.10092169046401978\n",
      "Epoch :0.7416666666666667    Train Loss :0.04850471392273903    Test Loss :0.10091102868318558\n",
      "Epoch :0.75    Train Loss :0.04849999025464058    Test Loss :0.10092326253652573\n",
      "Epoch :0.7583333333333333    Train Loss :0.04850459471344948    Test Loss :0.1009349673986435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7666666666666667    Train Loss :0.04850402846932411    Test Loss :0.10091082751750946\n",
      "Epoch :0.775    Train Loss :0.048503197729587555    Test Loss :0.1009168028831482\n",
      "Epoch :0.7833333333333333    Train Loss :0.04850401356816292    Test Loss :0.10090291500091553\n",
      "Epoch :0.7916666666666666    Train Loss :0.04850190877914429    Test Loss :0.10088477283716202\n",
      "Epoch :0.8    Train Loss :0.04850428178906441    Test Loss :0.10090810060501099\n",
      "Epoch :0.8083333333333333    Train Loss :0.048508819192647934    Test Loss :0.10090053826570511\n",
      "Epoch :0.8166666666666667    Train Loss :0.04849807173013687    Test Loss :0.10090408474206924\n",
      "Epoch :0.825    Train Loss :0.04849311709403992    Test Loss :0.10087898373603821\n",
      "Epoch :0.8333333333333334    Train Loss :0.04850827902555466    Test Loss :0.10089239478111267\n",
      "Epoch :0.8416666666666667    Train Loss :0.04850185662508011    Test Loss :0.10092388093471527\n",
      "Epoch :0.85    Train Loss :0.04851037263870239    Test Loss :0.1009114682674408\n",
      "Epoch :0.8583333333333333    Train Loss :0.048505451530218124    Test Loss :0.10094381123781204\n",
      "Epoch :0.8666666666666667    Train Loss :0.04850257560610771    Test Loss :0.10093358159065247\n",
      "Epoch :0.875    Train Loss :0.04850344359874725    Test Loss :0.10091733932495117\n",
      "Epoch :0.8833333333333333    Train Loss :0.04850116744637489    Test Loss :0.10088810324668884\n",
      "Epoch :0.8916666666666667    Train Loss :0.04850642383098602    Test Loss :0.10092122107744217\n",
      "Epoch :0.9    Train Loss :0.048507265746593475    Test Loss :0.10091337561607361\n",
      "Epoch :0.9083333333333333    Train Loss :0.04850402846932411    Test Loss :0.10088188946247101\n",
      "Epoch :0.9166666666666666    Train Loss :0.04850105941295624    Test Loss :0.10087785124778748\n",
      "Epoch :0.925    Train Loss :0.048503514379262924    Test Loss :0.10090640932321548\n",
      "Epoch :0.9333333333333333    Train Loss :0.04850265011191368    Test Loss :0.10093970596790314\n",
      "Epoch :0.9416666666666667    Train Loss :0.048499248921871185    Test Loss :0.10091608762741089\n",
      "Epoch :0.95    Train Loss :0.04849734157323837    Test Loss :0.10087397694587708\n",
      "Epoch :0.9583333333333334    Train Loss :0.048505932092666626    Test Loss :0.10087470710277557\n",
      "Epoch :0.9666666666666667    Train Loss :0.04850092902779579    Test Loss :0.10087690502405167\n",
      "Epoch :0.975    Train Loss :0.0484984926879406    Test Loss :0.10091906785964966\n",
      "Epoch :0.9833333333333333    Train Loss :0.04850391298532486    Test Loss :0.10093756020069122\n",
      "Epoch :0.9916666666666667    Train Loss :0.048502251505851746    Test Loss :0.10090445727109909\n",
      "Epoch :1.0    Train Loss :0.048504408448934555    Test Loss :0.1009022444486618\n",
      "RMSE: 30.717048208795642\n",
      "MAE: 29.6967266021414\n",
      "MAPE: 25.860492884616406%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 600, 'hid_size': 100, 'lr': 0.02, 'num_layers': 5}\n",
      "total:  50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.04601731896400452    Test Loss :0.1536559760570526\n",
      "Epoch :0.025    Train Loss :0.038293205201625824    Test Loss :0.0355142205953598\n",
      "Epoch :0.0375    Train Loss :0.024800149723887444    Test Loss :0.06003868579864502\n",
      "Epoch :0.05    Train Loss :0.008731221780180931    Test Loss :0.013248136267066002\n",
      "Epoch :0.0625    Train Loss :0.00486991461366415    Test Loss :0.015227721072733402\n",
      "Epoch :0.075    Train Loss :0.004468778148293495    Test Loss :0.0061133848503232\n",
      "Epoch :0.0875    Train Loss :0.0044194236397743225    Test Loss :0.004582616966217756\n",
      "Epoch :0.1    Train Loss :0.0024145718198269606    Test Loss :0.009059268049895763\n",
      "Epoch :0.1125    Train Loss :0.0022314705420285463    Test Loss :0.003364488249644637\n",
      "Epoch :0.125    Train Loss :0.001909422455355525    Test Loss :0.0031331409700214863\n",
      "Epoch :0.1375    Train Loss :0.001788390101864934    Test Loss :0.004661727696657181\n",
      "Epoch :0.15    Train Loss :0.0014953708741813898    Test Loss :0.002877933206036687\n",
      "Epoch :0.1625    Train Loss :0.0014112719800323248    Test Loss :0.0025550464633852243\n",
      "Epoch :0.175    Train Loss :0.001266416278667748    Test Loss :0.0024045410100370646\n",
      "Epoch :0.1875    Train Loss :0.0011807288974523544    Test Loss :0.002389107830822468\n",
      "Epoch :0.2    Train Loss :0.0011968015460297465    Test Loss :0.0026456359773874283\n",
      "Epoch :0.2125    Train Loss :0.0011452523758634925    Test Loss :0.0024085247423499823\n",
      "Epoch :0.225    Train Loss :0.0011384878307580948    Test Loss :0.0025308229960501194\n",
      "Epoch :0.2375    Train Loss :0.0011071008630096912    Test Loss :0.0023288987576961517\n",
      "Epoch :0.25    Train Loss :0.0010855398140847683    Test Loss :0.002090068766847253\n",
      "Epoch :0.2625    Train Loss :0.0009732169564813375    Test Loss :0.0019484796794131398\n",
      "Epoch :0.275    Train Loss :0.0010086075635626912    Test Loss :0.0019818150904029608\n",
      "Epoch :0.2875    Train Loss :0.0009304715204052627    Test Loss :0.002030891366302967\n",
      "Epoch :0.3    Train Loss :0.0009387707686983049    Test Loss :0.0019293269142508507\n",
      "Epoch :0.3125    Train Loss :0.0008819852373562753    Test Loss :0.0019813007675111294\n",
      "Epoch :0.325    Train Loss :0.0008379915961995721    Test Loss :0.001824433682486415\n",
      "Epoch :0.3375    Train Loss :0.0008427143329754472    Test Loss :0.0016957431798800826\n",
      "Epoch :0.35    Train Loss :0.0008417461067438126    Test Loss :0.001755434088408947\n",
      "Epoch :0.3625    Train Loss :0.0007923534139990807    Test Loss :0.0016205641441047192\n",
      "Epoch :0.375    Train Loss :0.0007635881192982197    Test Loss :0.0017008882714435458\n",
      "Epoch :0.3875    Train Loss :0.0008103111758828163    Test Loss :0.0015084116021171212\n",
      "Epoch :0.4    Train Loss :0.0007840946782380342    Test Loss :0.0014682705514132977\n",
      "Epoch :0.4125    Train Loss :0.0007178958039730787    Test Loss :0.0013974960893392563\n",
      "Epoch :0.425    Train Loss :0.0006873806123621762    Test Loss :0.0014263895573094487\n",
      "Epoch :0.4375    Train Loss :0.0007296797703020275    Test Loss :0.0013823816552758217\n",
      "Epoch :0.45    Train Loss :0.0007246956811286509    Test Loss :0.0014520311960950494\n",
      "Epoch :0.4625    Train Loss :0.0006919229635968804    Test Loss :0.0013511601136997342\n",
      "Epoch :0.475    Train Loss :0.0006761013646610081    Test Loss :0.0014798648189753294\n",
      "Epoch :0.4875    Train Loss :0.0006480161100625992    Test Loss :0.0012481589801609516\n",
      "Epoch :0.5    Train Loss :0.0006972255068831146    Test Loss :0.001206871122121811\n",
      "Epoch :0.5125    Train Loss :0.0006319342064671218    Test Loss :0.001508329645730555\n",
      "Epoch :0.525    Train Loss :0.0006757774972356856    Test Loss :0.0014159854035824537\n",
      "Epoch :0.5375    Train Loss :0.0006460498552769423    Test Loss :0.0012820834526792169\n",
      "Epoch :0.55    Train Loss :0.0006435620016418397    Test Loss :0.001305241254158318\n",
      "Epoch :0.5625    Train Loss :0.0006289876764640212    Test Loss :0.0012956480495631695\n",
      "Epoch :0.575    Train Loss :0.0006258618086576462    Test Loss :0.0012499154545366764\n",
      "Epoch :0.5875    Train Loss :0.0006055543199181557    Test Loss :0.0011581960134208202\n",
      "Epoch :0.6    Train Loss :0.0006108846282586455    Test Loss :0.0011824237881228328\n",
      "Epoch :0.6125    Train Loss :0.0006184785161167383    Test Loss :0.001105261966586113\n",
      "Epoch :0.625    Train Loss :0.0006059615989215672    Test Loss :0.0011379000497981906\n",
      "Epoch :0.6375    Train Loss :0.000614597403910011    Test Loss :0.0011105762096121907\n",
      "Epoch :0.65    Train Loss :0.0006265186821110547    Test Loss :0.0011471566976979375\n",
      "Epoch :0.6625    Train Loss :0.0006000542780384421    Test Loss :0.0011082193814218044\n",
      "Epoch :0.675    Train Loss :0.0006148368702270091    Test Loss :0.0013033030554652214\n",
      "Epoch :0.6875    Train Loss :0.0005830876179970801    Test Loss :0.0010940681677311659\n",
      "Epoch :0.7    Train Loss :0.0007771563832648098    Test Loss :0.001760258455760777\n",
      "Epoch :0.7125    Train Loss :0.0006129234679974616    Test Loss :0.0017054512863978744\n",
      "Epoch :0.725    Train Loss :0.0005919681279920042    Test Loss :0.0015175421722233295\n",
      "Epoch :0.7375    Train Loss :0.0005924196448177099    Test Loss :0.0012609570985659957\n",
      "Epoch :0.75    Train Loss :0.000571671174839139    Test Loss :0.0011360312346369028\n",
      "Epoch :0.7625    Train Loss :0.0005442810943350196    Test Loss :0.001036376808770001\n",
      "Epoch :0.775    Train Loss :0.0005826571141369641    Test Loss :0.0010592128382995725\n",
      "Epoch :0.7875    Train Loss :0.0005952257197350264    Test Loss :0.0012031685328111053\n",
      "Epoch :0.8    Train Loss :0.0005711734411306679    Test Loss :0.0010607005096971989\n",
      "Epoch :0.8125    Train Loss :0.0005297946045175195    Test Loss :0.0012626140378415585\n",
      "Epoch :0.825    Train Loss :0.0006398230907507241    Test Loss :0.0010538408532738686\n",
      "Epoch :0.8375    Train Loss :0.0006077915895730257    Test Loss :0.0011831384617835283\n",
      "Epoch :0.85    Train Loss :0.0005291164270602167    Test Loss :0.0010849905665963888\n",
      "Epoch :0.8625    Train Loss :0.0005093644140288234    Test Loss :0.0010208988096565008\n",
      "Epoch :0.875    Train Loss :0.0004930808790959418    Test Loss :0.0011515148216858506\n",
      "Epoch :0.8875    Train Loss :0.0006546933436766267    Test Loss :0.0010067783296108246\n",
      "Epoch :0.9    Train Loss :0.0005908117746002972    Test Loss :0.0010470745619386435\n",
      "Epoch :0.9125    Train Loss :0.0005446121213026345    Test Loss :0.0010226661106571555\n",
      "Epoch :0.925    Train Loss :0.0004684026644099504    Test Loss :0.0013681963318958879\n",
      "Epoch :0.9375    Train Loss :0.00048695923760533333    Test Loss :0.0010125193512067199\n",
      "Epoch :0.95    Train Loss :0.0005363936652429402    Test Loss :0.0009216446196660399\n",
      "Epoch :0.9625    Train Loss :0.0005086493911221623    Test Loss :0.0009929512161761522\n",
      "Epoch :0.975    Train Loss :0.0007352243992500007    Test Loss :0.002014678670093417\n",
      "Epoch :0.9875    Train Loss :0.0004570638411678374    Test Loss :0.0014590088976547122\n",
      "Epoch :1.0    Train Loss :0.0006700213416479528    Test Loss :0.0018000219715759158\n",
      "RMSE: 68.06078966831265\n",
      "MAE: 66.52289069122261\n",
      "MAPE: 58.90978429970741%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 400, 'hid_size': 50, 'lr': 0.01, 'num_layers': 2}\n",
      "total:  53.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.0674380213022232    Test Loss :0.2217082381248474\n",
      "Epoch :0.025    Train Loss :0.04860149323940277    Test Loss :0.07744738459587097\n",
      "Epoch :0.0375    Train Loss :0.049860432744026184    Test Loss :0.09466604143381119\n",
      "Epoch :0.05    Train Loss :0.050502922385931015    Test Loss :0.13052792847156525\n",
      "Epoch :0.0625    Train Loss :0.04654305800795555    Test Loss :0.0865623950958252\n",
      "Epoch :0.075    Train Loss :0.02965172380208969    Test Loss :0.028999632224440575\n",
      "Epoch :0.0875    Train Loss :0.02183893695473671    Test Loss :0.025227893143892288\n",
      "Epoch :0.1    Train Loss :0.015122842974960804    Test Loss :0.04012278467416763\n",
      "Epoch :0.1125    Train Loss :0.01389085128903389    Test Loss :0.014081625267863274\n",
      "Epoch :0.125    Train Loss :0.010828389786183834    Test Loss :0.023557448759675026\n",
      "Epoch :0.1375    Train Loss :0.008665092289447784    Test Loss :0.023900074884295464\n",
      "Epoch :0.15    Train Loss :0.005692138336598873    Test Loss :0.009691823273897171\n",
      "Epoch :0.1625    Train Loss :0.0045142099261283875    Test Loss :0.009653141722083092\n",
      "Epoch :0.175    Train Loss :0.0035849132109433413    Test Loss :0.006199167110025883\n",
      "Epoch :0.1875    Train Loss :0.00322026782669127    Test Loss :0.006437122821807861\n",
      "Epoch :0.2    Train Loss :0.002765757730230689    Test Loss :0.0056115020997822285\n",
      "Epoch :0.2125    Train Loss :0.0022540302015841007    Test Loss :0.004344985354691744\n",
      "Epoch :0.225    Train Loss :0.002274651313200593    Test Loss :0.004202269017696381\n",
      "Epoch :0.2375    Train Loss :0.0020903311669826508    Test Loss :0.004191158805042505\n",
      "Epoch :0.25    Train Loss :0.0019146923441439867    Test Loss :0.003984266426414251\n",
      "Epoch :0.2625    Train Loss :0.0018278136849403381    Test Loss :0.004104782361537218\n",
      "Epoch :0.275    Train Loss :0.0018624542281031609    Test Loss :0.003402137663215399\n",
      "Epoch :0.2875    Train Loss :0.001660604844801128    Test Loss :0.0035212039947509766\n",
      "Epoch :0.3    Train Loss :0.0016220271354541183    Test Loss :0.003344572614878416\n",
      "Epoch :0.3125    Train Loss :0.0015158198075369    Test Loss :0.003643926465883851\n",
      "Epoch :0.325    Train Loss :0.0014486374566331506    Test Loss :0.0033981092274188995\n",
      "Epoch :0.3375    Train Loss :0.0013640320394188166    Test Loss :0.0033328707795590162\n",
      "Epoch :0.35    Train Loss :0.0013809503288939595    Test Loss :0.0033576281275600195\n",
      "Epoch :0.3625    Train Loss :0.0013000142062082887    Test Loss :0.0033524089958518744\n",
      "Epoch :0.375    Train Loss :0.001335303415544331    Test Loss :0.0033111462835222483\n",
      "Epoch :0.3875    Train Loss :0.0012182681821286678    Test Loss :0.0031207932624965906\n",
      "Epoch :0.4    Train Loss :0.0011788052506744862    Test Loss :0.00301514589227736\n",
      "Epoch :0.4125    Train Loss :0.001309538260102272    Test Loss :0.0038264954928308725\n",
      "Epoch :0.425    Train Loss :0.0014412455493584275    Test Loss :0.0033767533022910357\n",
      "Epoch :0.4375    Train Loss :0.001124058966524899    Test Loss :0.0030605157371610403\n",
      "Epoch :0.45    Train Loss :0.0011424233671277761    Test Loss :0.003084216034039855\n",
      "Epoch :0.4625    Train Loss :0.0011585553875193    Test Loss :0.0032728672958910465\n",
      "Epoch :0.475    Train Loss :0.001103535178117454    Test Loss :0.0029441213700920343\n",
      "Epoch :0.4875    Train Loss :0.0009872172959148884    Test Loss :0.0028134819585829973\n",
      "Epoch :0.5    Train Loss :0.001028801198117435    Test Loss :0.0029470836743712425\n",
      "Epoch :0.5125    Train Loss :0.0010302243754267693    Test Loss :0.0029792075511068106\n",
      "Epoch :0.525    Train Loss :0.0009249020949937403    Test Loss :0.002868812996894121\n",
      "Epoch :0.5375    Train Loss :0.0009302052785642445    Test Loss :0.0027029518969357014\n",
      "Epoch :0.55    Train Loss :0.0008684062049724162    Test Loss :0.0029187051113694906\n",
      "Epoch :0.5625    Train Loss :0.0008688864763826132    Test Loss :0.0029443898238241673\n",
      "Epoch :0.575    Train Loss :0.0008440359379164875    Test Loss :0.0024999419692903757\n",
      "Epoch :0.5875    Train Loss :0.0008317595929838717    Test Loss :0.002389858942478895\n",
      "Epoch :0.6    Train Loss :0.0008672160911373794    Test Loss :0.0026458406355232\n",
      "Epoch :0.6125    Train Loss :0.0009087617509067059    Test Loss :0.0026704727206379175\n",
      "Epoch :0.625    Train Loss :0.0008900175453163683    Test Loss :0.0026622768491506577\n",
      "Epoch :0.6375    Train Loss :0.0007895648013800383    Test Loss :0.0026966624427586794\n",
      "Epoch :0.65    Train Loss :0.0007997826323844492    Test Loss :0.0027141657192260027\n",
      "Epoch :0.6625    Train Loss :0.0007789392839185894    Test Loss :0.002496642293408513\n",
      "Epoch :0.675    Train Loss :0.0008497246890328825    Test Loss :0.002617867896333337\n",
      "Epoch :0.6875    Train Loss :0.0007516346522606909    Test Loss :0.002526876050978899\n",
      "Epoch :0.7    Train Loss :0.0007607484585605562    Test Loss :0.0025851791724562645\n",
      "Epoch :0.7125    Train Loss :0.0008313432917930186    Test Loss :0.002580238040536642\n",
      "Epoch :0.725    Train Loss :0.0007746666087768972    Test Loss :0.0024788787122815847\n",
      "Epoch :0.7375    Train Loss :0.0007232932257466018    Test Loss :0.0024705000687390566\n",
      "Epoch :0.75    Train Loss :0.0007215722580440342    Test Loss :0.002425549319013953\n",
      "Epoch :0.7625    Train Loss :0.0007114150212146342    Test Loss :0.00239680171944201\n",
      "Epoch :0.775    Train Loss :0.0007465173257514834    Test Loss :0.002431904897093773\n",
      "Epoch :0.7875    Train Loss :0.0009553105919621885    Test Loss :0.0028267884626984596\n",
      "Epoch :0.8    Train Loss :0.0008305149967782199    Test Loss :0.0024555795826017857\n",
      "Epoch :0.8125    Train Loss :0.0008373900782316923    Test Loss :0.002269383752718568\n",
      "Epoch :0.825    Train Loss :0.0007722408627159894    Test Loss :0.002774897264316678\n",
      "Epoch :0.8375    Train Loss :0.0008035845239646733    Test Loss :0.0023677970748394728\n",
      "Epoch :0.85    Train Loss :0.0007584880804643035    Test Loss :0.002295253798365593\n",
      "Epoch :0.8625    Train Loss :0.0007109884172677994    Test Loss :0.0024647123645991087\n",
      "Epoch :0.875    Train Loss :0.0006668764399364591    Test Loss :0.002343307016417384\n",
      "Epoch :0.8875    Train Loss :0.0007041051867417991    Test Loss :0.0024462935980409384\n",
      "Epoch :0.9    Train Loss :0.0006049636867828667    Test Loss :0.0024292333982884884\n",
      "Epoch :0.9125    Train Loss :0.0007363201002590358    Test Loss :0.002318605547770858\n",
      "Epoch :0.925    Train Loss :0.000820548739284277    Test Loss :0.0024822447448968887\n",
      "Epoch :0.9375    Train Loss :0.0006296327919699252    Test Loss :0.002284181071445346\n",
      "Epoch :0.95    Train Loss :0.0006861794972792268    Test Loss :0.002493378007784486\n",
      "Epoch :0.9625    Train Loss :0.0006765045691281557    Test Loss :0.002366460394114256\n",
      "Epoch :0.975    Train Loss :0.0007005762890912592    Test Loss :0.0023136695381253958\n",
      "Epoch :0.9875    Train Loss :0.000658382021356374    Test Loss :0.002282645320519805\n",
      "Epoch :1.0    Train Loss :0.0006488393410108984    Test Loss :0.0022924162913113832\n",
      "RMSE: 8.768830423367087\n",
      "MAE: 7.168718325288711\n",
      "MAPE: 6.495375326950499%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 400, 'hid_size': 50, 'lr': 0.01, 'num_layers': 5}\n",
      "total:  56.00000000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.17092397809028625    Test Loss :0.48365944623947144\n",
      "Epoch :0.025    Train Loss :0.12805262207984924    Test Loss :0.2642625570297241\n",
      "Epoch :0.0375    Train Loss :0.05403872951865196    Test Loss :0.08764565736055374\n",
      "Epoch :0.05    Train Loss :0.05211874097585678    Test Loss :0.1437378227710724\n",
      "Epoch :0.0625    Train Loss :0.03872719034552574    Test Loss :0.050753403455019\n",
      "Epoch :0.075    Train Loss :0.027548475190997124    Test Loss :0.04462413489818573\n",
      "Epoch :0.0875    Train Loss :0.011483014561235905    Test Loss :0.01978311687707901\n",
      "Epoch :0.1    Train Loss :0.004879781510680914    Test Loss :0.018791163340210915\n",
      "Epoch :0.1125    Train Loss :0.005261456128209829    Test Loss :0.01301205437630415\n",
      "Epoch :0.125    Train Loss :0.0037112415302544832    Test Loss :0.006017136387526989\n",
      "Epoch :0.1375    Train Loss :0.002705260878428817    Test Loss :0.0076441639102995396\n",
      "Epoch :0.15    Train Loss :0.00209442968480289    Test Loss :0.004614644218236208\n",
      "Epoch :0.1625    Train Loss :0.0021278010681271553    Test Loss :0.0035774169955402613\n",
      "Epoch :0.175    Train Loss :0.001891209394671023    Test Loss :0.003840320510789752\n",
      "Epoch :0.1875    Train Loss :0.0016724277520552278    Test Loss :0.0028376313857734203\n",
      "Epoch :0.2    Train Loss :0.0016667007002979517    Test Loss :0.0032747238874435425\n",
      "Epoch :0.2125    Train Loss :0.001474717864766717    Test Loss :0.002545158378779888\n",
      "Epoch :0.225    Train Loss :0.0014614068204537034    Test Loss :0.002894085366278887\n",
      "Epoch :0.2375    Train Loss :0.0012626488460227847    Test Loss :0.0023110383190214634\n",
      "Epoch :0.25    Train Loss :0.0012482409365475178    Test Loss :0.00244191219098866\n",
      "Epoch :0.2625    Train Loss :0.001198585843667388    Test Loss :0.0022698186803609133\n",
      "Epoch :0.275    Train Loss :0.001187554677017033    Test Loss :0.0024291928857564926\n",
      "Epoch :0.2875    Train Loss :0.0011186223709955812    Test Loss :0.0022481430787593126\n",
      "Epoch :0.3    Train Loss :0.0010262272553518414    Test Loss :0.0020663838367909193\n",
      "Epoch :0.3125    Train Loss :0.0010769616346806288    Test Loss :0.0021236524917185307\n",
      "Epoch :0.325    Train Loss :0.0010169981978833675    Test Loss :0.0020279886666685343\n",
      "Epoch :0.3375    Train Loss :0.0008930700714699924    Test Loss :0.001991420052945614\n",
      "Epoch :0.35    Train Loss :0.0009212741861119866    Test Loss :0.0019377212738618255\n",
      "Epoch :0.3625    Train Loss :0.0009273132309317589    Test Loss :0.001780886435881257\n",
      "Epoch :0.375    Train Loss :0.0008975734235718846    Test Loss :0.0019521440844982862\n",
      "Epoch :0.3875    Train Loss :0.0008569831261411309    Test Loss :0.0018859850242733955\n",
      "Epoch :0.4    Train Loss :0.0009227428818121552    Test Loss :0.0017181679140776396\n",
      "Epoch :0.4125    Train Loss :0.000948052853345871    Test Loss :0.001702002133242786\n",
      "Epoch :0.425    Train Loss :0.0008276992593891919    Test Loss :0.0018950995290651917\n",
      "Epoch :0.4375    Train Loss :0.0008636320126242936    Test Loss :0.001771812792867422\n",
      "Epoch :0.45    Train Loss :0.0007878917967900634    Test Loss :0.001772447256371379\n",
      "Epoch :0.4625    Train Loss :0.0008291554404422641    Test Loss :0.001737095881253481\n",
      "Epoch :0.475    Train Loss :0.0007781501626595855    Test Loss :0.0016586942365393043\n",
      "Epoch :0.4875    Train Loss :0.0007178550004027784    Test Loss :0.0016520030330866575\n",
      "Epoch :0.5    Train Loss :0.000802376598585397    Test Loss :0.001730949617922306\n",
      "Epoch :0.5125    Train Loss :0.0008070567273534834    Test Loss :0.0016319629503414035\n",
      "Epoch :0.525    Train Loss :0.0007256079697981477    Test Loss :0.0015601806808263063\n",
      "Epoch :0.5375    Train Loss :0.0007074607419781387    Test Loss :0.001482668798416853\n",
      "Epoch :0.55    Train Loss :0.0007402448682114482    Test Loss :0.0014460039092227817\n",
      "Epoch :0.5625    Train Loss :0.0007250998169183731    Test Loss :0.0015416010282933712\n",
      "Epoch :0.575    Train Loss :0.0007028055260889232    Test Loss :0.0015594193246215582\n",
      "Epoch :0.5875    Train Loss :0.0007002729107625782    Test Loss :0.001689271186478436\n",
      "Epoch :0.6    Train Loss :0.0007284801104106009    Test Loss :0.0016307559562847018\n",
      "Epoch :0.6125    Train Loss :0.0006994859431870282    Test Loss :0.0015684411628171802\n",
      "Epoch :0.625    Train Loss :0.0006997421733103693    Test Loss :0.0014003324322402477\n",
      "Epoch :0.6375    Train Loss :0.0007349407533183694    Test Loss :0.001394108054228127\n",
      "Epoch :0.65    Train Loss :0.0006631842697970569    Test Loss :0.001429267693310976\n",
      "Epoch :0.6625    Train Loss :0.0006184729863889515    Test Loss :0.0012637650361284614\n",
      "Epoch :0.675    Train Loss :0.0006708474829792976    Test Loss :0.0013455725274980068\n",
      "Epoch :0.6875    Train Loss :0.0006708467844873667    Test Loss :0.001318087917752564\n",
      "Epoch :0.7    Train Loss :0.000650089408736676    Test Loss :0.001394741702824831\n",
      "Epoch :0.7125    Train Loss :0.0006193352164700627    Test Loss :0.001271105371415615\n",
      "Epoch :0.725    Train Loss :0.000769052654504776    Test Loss :0.0013061665231361985\n",
      "Epoch :0.7375    Train Loss :0.0006469315849244595    Test Loss :0.0013232205528765917\n",
      "Epoch :0.75    Train Loss :0.0006232125451788306    Test Loss :0.0013511684956029058\n",
      "Epoch :0.7625    Train Loss :0.0006429125205613673    Test Loss :0.0012234181631356478\n",
      "Epoch :0.775    Train Loss :0.0006122381309978664    Test Loss :0.0013029985129833221\n",
      "Epoch :0.7875    Train Loss :0.0006253793253563344    Test Loss :0.0012151912087574601\n",
      "Epoch :0.8    Train Loss :0.0006029538926668465    Test Loss :0.001337227993644774\n",
      "Epoch :0.8125    Train Loss :0.0005910565378144383    Test Loss :0.0012908375356346369\n",
      "Epoch :0.825    Train Loss :0.0005548573099076748    Test Loss :0.0011924328282475471\n",
      "Epoch :0.8375    Train Loss :0.0005872144247405231    Test Loss :0.0013692709617316723\n",
      "Epoch :0.85    Train Loss :0.0006441602017730474    Test Loss :0.0011148297926411033\n",
      "Epoch :0.8625    Train Loss :0.0005216507124714553    Test Loss :0.001106083975173533\n",
      "Epoch :0.875    Train Loss :0.0006859934073872864    Test Loss :0.001745025278069079\n",
      "Epoch :0.8875    Train Loss :0.0005301582277752459    Test Loss :0.0010376439895480871\n",
      "Epoch :0.9    Train Loss :0.0005855727940797806    Test Loss :0.001121705980040133\n",
      "Epoch :0.9125    Train Loss :0.0005369582213461399    Test Loss :0.0011305033694952726\n",
      "Epoch :0.925    Train Loss :0.0005526996101252735    Test Loss :0.0011290436377748847\n",
      "Epoch :0.9375    Train Loss :0.0005415803752839565    Test Loss :0.000980811775662005\n",
      "Epoch :0.95    Train Loss :0.0005726927192881703    Test Loss :0.0014228321379050612\n",
      "Epoch :0.9625    Train Loss :0.0005883800913579762    Test Loss :0.0011075662914663553\n",
      "Epoch :0.975    Train Loss :0.0005462291301228106    Test Loss :0.0011606531916186213\n",
      "Epoch :0.9875    Train Loss :0.0005041271215304732    Test Loss :0.0009631748544052243\n",
      "Epoch :1.0    Train Loss :0.0005317164468578994    Test Loss :0.0012494439724832773\n",
      "RMSE: 33.60824699434943\n",
      "MAE: 30.93284404493913\n",
      "MAPE: 27.092055527174562%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 400, 'hid_size': 50, 'lr': 0.02, 'num_layers': 2}\n",
      "total:  59.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.07241218537092209    Test Loss :0.14788289368152618\n",
      "Epoch :0.025    Train Loss :0.049849577248096466    Test Loss :0.09150966256856918\n",
      "Epoch :0.0375    Train Loss :0.047953516244888306    Test Loss :0.10913987457752228\n",
      "Epoch :0.05    Train Loss :0.03151660040020943    Test Loss :0.022335601970553398\n",
      "Epoch :0.0625    Train Loss :0.028270181268453598    Test Loss :0.023783810436725616\n",
      "Epoch :0.075    Train Loss :0.017543859779834747    Test Loss :0.01958056539297104\n",
      "Epoch :0.0875    Train Loss :0.009561721235513687    Test Loss :0.03376659005880356\n",
      "Epoch :0.1    Train Loss :0.00825561210513115    Test Loss :0.010849975980818272\n",
      "Epoch :0.1125    Train Loss :0.005379136651754379    Test Loss :0.013220088556408882\n",
      "Epoch :0.125    Train Loss :0.00520820589736104    Test Loss :0.006595822982490063\n",
      "Epoch :0.1375    Train Loss :0.004338182508945465    Test Loss :0.009172184392809868\n",
      "Epoch :0.15    Train Loss :0.0035949680022895336    Test Loss :0.004920694045722485\n",
      "Epoch :0.1625    Train Loss :0.0034316556993871927    Test Loss :0.006654541473835707\n",
      "Epoch :0.175    Train Loss :0.002924217376857996    Test Loss :0.006795510649681091\n",
      "Epoch :0.1875    Train Loss :0.0026493037585169077    Test Loss :0.005618525203317404\n",
      "Epoch :0.2    Train Loss :0.002688695676624775    Test Loss :0.005206226836889982\n",
      "Epoch :0.2125    Train Loss :0.002284702844917774    Test Loss :0.004485485143959522\n",
      "Epoch :0.225    Train Loss :0.0022618905641138554    Test Loss :0.0039103333838284016\n",
      "Epoch :0.2375    Train Loss :0.0023243010509759188    Test Loss :0.003733496181666851\n",
      "Epoch :0.25    Train Loss :0.0021124437917023897    Test Loss :0.00446215458214283\n",
      "Epoch :0.2625    Train Loss :0.0020769271068274975    Test Loss :0.004396920092403889\n",
      "Epoch :0.275    Train Loss :0.0019026900408789515    Test Loss :0.003660513088107109\n",
      "Epoch :0.2875    Train Loss :0.0017275300342589617    Test Loss :0.0036910756025463343\n",
      "Epoch :0.3    Train Loss :0.0017556826351210475    Test Loss :0.0035524810664355755\n",
      "Epoch :0.3125    Train Loss :0.0017155369278043509    Test Loss :0.0032454500906169415\n",
      "Epoch :0.325    Train Loss :0.001800908357836306    Test Loss :0.0032894648611545563\n",
      "Epoch :0.3375    Train Loss :0.0015802287962287664    Test Loss :0.0033076622057706118\n",
      "Epoch :0.35    Train Loss :0.0015727176796644926    Test Loss :0.0030409186147153378\n",
      "Epoch :0.3625    Train Loss :0.0015141855692490935    Test Loss :0.003418266074731946\n",
      "Epoch :0.375    Train Loss :0.0014220187440514565    Test Loss :0.002977866679430008\n",
      "Epoch :0.3875    Train Loss :0.001329444465227425    Test Loss :0.0032732405234128237\n",
      "Epoch :0.4    Train Loss :0.001461448846384883    Test Loss :0.002996617229655385\n",
      "Epoch :0.4125    Train Loss :0.0013468523975461721    Test Loss :0.003037477610632777\n",
      "Epoch :0.425    Train Loss :0.0013263470027595758    Test Loss :0.0030572772957384586\n",
      "Epoch :0.4375    Train Loss :0.0012970177922397852    Test Loss :0.00323958951048553\n",
      "Epoch :0.45    Train Loss :0.0012863378506153822    Test Loss :0.0029342761263251305\n",
      "Epoch :0.4625    Train Loss :0.0012744071427732706    Test Loss :0.0031142011284828186\n",
      "Epoch :0.475    Train Loss :0.0011759111657738686    Test Loss :0.003431629156693816\n",
      "Epoch :0.4875    Train Loss :0.001096692169085145    Test Loss :0.0028496526647359133\n",
      "Epoch :0.5    Train Loss :0.0011670661624521017    Test Loss :0.0034267837181687355\n",
      "Epoch :0.5125    Train Loss :0.0013090467546135187    Test Loss :0.0030814872588962317\n",
      "Epoch :0.525    Train Loss :0.0011550203198567033    Test Loss :0.0029432657174766064\n",
      "Epoch :0.5375    Train Loss :0.0012253612512722611    Test Loss :0.002984090009704232\n",
      "Epoch :0.55    Train Loss :0.0012060899753123522    Test Loss :0.0027759852819144726\n",
      "Epoch :0.5625    Train Loss :0.001083924900740385    Test Loss :0.002789751859381795\n",
      "Epoch :0.575    Train Loss :0.0010756042320281267    Test Loss :0.002898277947679162\n",
      "Epoch :0.5875    Train Loss :0.0010211683111265302    Test Loss :0.002648603171110153\n",
      "Epoch :0.6    Train Loss :0.0009776594815775752    Test Loss :0.002709096297621727\n",
      "Epoch :0.6125    Train Loss :0.0010083660017699003    Test Loss :0.0026356345042586327\n",
      "Epoch :0.625    Train Loss :0.0009929593652486801    Test Loss :0.002628088928759098\n",
      "Epoch :0.6375    Train Loss :0.0009804515866562724    Test Loss :0.002629494061693549\n",
      "Epoch :0.65    Train Loss :0.0010308618657290936    Test Loss :0.002744985045865178\n",
      "Epoch :0.6625    Train Loss :0.0010640083346515894    Test Loss :0.002426285995170474\n",
      "Epoch :0.675    Train Loss :0.0009710294543765485    Test Loss :0.002717716619372368\n",
      "Epoch :0.6875    Train Loss :0.0009386484161950648    Test Loss :0.0027306636329740286\n",
      "Epoch :0.7    Train Loss :0.0009249355643987656    Test Loss :0.0024902464356273413\n",
      "Epoch :0.7125    Train Loss :0.0009437845437787473    Test Loss :0.002324167639017105\n",
      "Epoch :0.725    Train Loss :0.0009274113108403981    Test Loss :0.0023078424856066704\n",
      "Epoch :0.7375    Train Loss :0.0008694238495081663    Test Loss :0.002485973760485649\n",
      "Epoch :0.75    Train Loss :0.0008597907144576311    Test Loss :0.002193621126934886\n",
      "Epoch :0.7625    Train Loss :0.0007841293700039387    Test Loss :0.002394384704530239\n",
      "Epoch :0.775    Train Loss :0.000833577592857182    Test Loss :0.002556805731728673\n",
      "Epoch :0.7875    Train Loss :0.0007923613302409649    Test Loss :0.002369231078773737\n",
      "Epoch :0.8    Train Loss :0.0008877593209035695    Test Loss :0.0028552082367241383\n",
      "Epoch :0.8125    Train Loss :0.0008009894518181682    Test Loss :0.002161413663998246\n",
      "Epoch :0.825    Train Loss :0.0007977837231010199    Test Loss :0.0025828424841165543\n",
      "Epoch :0.8375    Train Loss :0.0008003332186490297    Test Loss :0.002256855834275484\n",
      "Epoch :0.85    Train Loss :0.0007806801586411893    Test Loss :0.0026793198194354773\n",
      "Epoch :0.8625    Train Loss :0.0008025813149288297    Test Loss :0.002281570341438055\n",
      "Epoch :0.875    Train Loss :0.0008234281558543444    Test Loss :0.002394822658970952\n",
      "Epoch :0.8875    Train Loss :0.0007271435461007059    Test Loss :0.0023290070239454508\n",
      "Epoch :0.9    Train Loss :0.0007060541538521647    Test Loss :0.002296571619808674\n",
      "Epoch :0.9125    Train Loss :0.0007745153270661831    Test Loss :0.0025375308468937874\n",
      "Epoch :0.925    Train Loss :0.0008565487223677337    Test Loss :0.0022730878554284573\n",
      "Epoch :0.9375    Train Loss :0.0007452501449733973    Test Loss :0.002465180354192853\n",
      "Epoch :0.95    Train Loss :0.0007941409712657332    Test Loss :0.002369603142142296\n",
      "Epoch :0.9625    Train Loss :0.0007117184577509761    Test Loss :0.0024620438925921917\n",
      "Epoch :0.975    Train Loss :0.0008190326043404639    Test Loss :0.002242645714432001\n",
      "Epoch :0.9875    Train Loss :0.0008896260405890644    Test Loss :0.0025286758318543434\n",
      "Epoch :1.0    Train Loss :0.0007113994797691703    Test Loss :0.0025079858023673296\n",
      "RMSE: 13.108406814312017\n",
      "MAE: 10.479629911631262\n",
      "MAPE: 9.055255318626203%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 400, 'hid_size': 50, 'lr': 0.02, 'num_layers': 5}\n",
      "total:  62.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.14989231526851654    Test Loss :0.45281508564949036\n",
      "Epoch :0.025    Train Loss :0.14036931097507477    Test Loss :0.32846879959106445\n",
      "Epoch :0.0375    Train Loss :0.08759249001741409    Test Loss :0.03579305484890938\n",
      "Epoch :0.05    Train Loss :0.05527899041771889    Test Loss :0.164643794298172\n",
      "Epoch :0.0625    Train Loss :0.04053919017314911    Test Loss :0.07384902983903885\n",
      "Epoch :0.075    Train Loss :0.03787066042423248    Test Loss :0.03274096921086311\n",
      "Epoch :0.0875    Train Loss :0.023532360792160034    Test Loss :0.041709307581186295\n",
      "Epoch :0.1    Train Loss :0.007512107491493225    Test Loss :0.017143987119197845\n",
      "Epoch :0.1125    Train Loss :0.007761913817375898    Test Loss :0.011777881532907486\n",
      "Epoch :0.125    Train Loss :0.004283644258975983    Test Loss :0.006147677078843117\n",
      "Epoch :0.1375    Train Loss :0.0030845156870782375    Test Loss :0.008054418489336967\n",
      "Epoch :0.15    Train Loss :0.0020157170947641134    Test Loss :0.004046742804348469\n",
      "Epoch :0.1625    Train Loss :0.0017220041481778026    Test Loss :0.004079075064510107\n",
      "Epoch :0.175    Train Loss :0.0017637570854276419    Test Loss :0.006677413824945688\n",
      "Epoch :0.1875    Train Loss :0.0014201413141563535    Test Loss :0.003071325598284602\n",
      "Epoch :0.2    Train Loss :0.0012765659485012293    Test Loss :0.002542843809351325\n",
      "Epoch :0.2125    Train Loss :0.001214098883792758    Test Loss :0.0030194453429430723\n",
      "Epoch :0.225    Train Loss :0.0010913338046520948    Test Loss :0.002439496573060751\n",
      "Epoch :0.2375    Train Loss :0.0011663982877507806    Test Loss :0.0022158746141940355\n",
      "Epoch :0.25    Train Loss :0.0010937073966488242    Test Loss :0.002440391108393669\n",
      "Epoch :0.2625    Train Loss :0.0010278234258294106    Test Loss :0.0022481069900095463\n",
      "Epoch :0.275    Train Loss :0.0009964349446818233    Test Loss :0.0022607005666941404\n",
      "Epoch :0.2875    Train Loss :0.0009762948611751199    Test Loss :0.0020623619202524424\n",
      "Epoch :0.3    Train Loss :0.0009421493741683662    Test Loss :0.001979484921321273\n",
      "Epoch :0.3125    Train Loss :0.0008890851750038564    Test Loss :0.0019601332023739815\n",
      "Epoch :0.325    Train Loss :0.0008646966889500618    Test Loss :0.001998958410695195\n",
      "Epoch :0.3375    Train Loss :0.0009494852274656296    Test Loss :0.0019419325981289148\n",
      "Epoch :0.35    Train Loss :0.0009270732989534736    Test Loss :0.0017705707577988505\n",
      "Epoch :0.3625    Train Loss :0.0008583441376686096    Test Loss :0.001588649582117796\n",
      "Epoch :0.375    Train Loss :0.0008081273990683258    Test Loss :0.0017890279414132237\n",
      "Epoch :0.3875    Train Loss :0.0008094305521808565    Test Loss :0.001653087674640119\n",
      "Epoch :0.4    Train Loss :0.0007780020241625607    Test Loss :0.001643420080654323\n",
      "Epoch :0.4125    Train Loss :0.000799646892119199    Test Loss :0.0016567182028666139\n",
      "Epoch :0.425    Train Loss :0.0007664566510356963    Test Loss :0.0015946569619700313\n",
      "Epoch :0.4375    Train Loss :0.0007593109039589763    Test Loss :0.0015442990697920322\n",
      "Epoch :0.45    Train Loss :0.0007978507201187313    Test Loss :0.0014896778156980872\n",
      "Epoch :0.4625    Train Loss :0.0006980574107728899    Test Loss :0.0015228436095640063\n",
      "Epoch :0.475    Train Loss :0.0007233773358166218    Test Loss :0.0014666413189843297\n",
      "Epoch :0.4875    Train Loss :0.0007008644170127809    Test Loss :0.0015096674906089902\n",
      "Epoch :0.5    Train Loss :0.0007511561852879822    Test Loss :0.0013936588075011969\n",
      "Epoch :0.5125    Train Loss :0.0006949534290470183    Test Loss :0.001345369964838028\n",
      "Epoch :0.525    Train Loss :0.000706071441527456    Test Loss :0.0014122670982033014\n",
      "Epoch :0.5375    Train Loss :0.000682415789924562    Test Loss :0.0014921894762665033\n",
      "Epoch :0.55    Train Loss :0.0006763531127944589    Test Loss :0.0013331194641068578\n",
      "Epoch :0.5625    Train Loss :0.0007032594294287264    Test Loss :0.0014043474802747369\n",
      "Epoch :0.575    Train Loss :0.0006415273528546095    Test Loss :0.0014652564423158765\n",
      "Epoch :0.5875    Train Loss :0.0006362370913848281    Test Loss :0.001274089445360005\n",
      "Epoch :0.6    Train Loss :0.0006567491218447685    Test Loss :0.0014156325487419963\n",
      "Epoch :0.6125    Train Loss :0.0006383227882906795    Test Loss :0.0012566904770210385\n",
      "Epoch :0.625    Train Loss :0.0006571170524694026    Test Loss :0.001261899247765541\n",
      "Epoch :0.6375    Train Loss :0.0006357795791700482    Test Loss :0.0012720476370304823\n",
      "Epoch :0.65    Train Loss :0.000639591773506254    Test Loss :0.001380440080538392\n",
      "Epoch :0.6625    Train Loss :0.0006369392503984272    Test Loss :0.00133428571280092\n",
      "Epoch :0.675    Train Loss :0.0006098579033277929    Test Loss :0.0014839048963040113\n",
      "Epoch :0.6875    Train Loss :0.0006264926632866263    Test Loss :0.0011745622614398599\n",
      "Epoch :0.7    Train Loss :0.0006133035640232265    Test Loss :0.001283013029024005\n",
      "Epoch :0.7125    Train Loss :0.0006204239907674491    Test Loss :0.0012704797554761171\n",
      "Epoch :0.725    Train Loss :0.0006041806773282588    Test Loss :0.001171541283838451\n",
      "Epoch :0.7375    Train Loss :0.0006475301343016326    Test Loss :0.001243940438143909\n",
      "Epoch :0.75    Train Loss :0.0006141195190139115    Test Loss :0.0012015230022370815\n",
      "Epoch :0.7625    Train Loss :0.0006229847786016762    Test Loss :0.0012779083335772157\n",
      "Epoch :0.775    Train Loss :0.0005731558194383979    Test Loss :0.001204160857014358\n",
      "Epoch :0.7875    Train Loss :0.0006281961686909199    Test Loss :0.001202078303322196\n",
      "Epoch :0.8    Train Loss :0.0005678535671904683    Test Loss :0.0012379231629893184\n",
      "Epoch :0.8125    Train Loss :0.000592748518101871    Test Loss :0.0011668269289657474\n",
      "Epoch :0.825    Train Loss :0.0005951090133748949    Test Loss :0.001266303937882185\n",
      "Epoch :0.8375    Train Loss :0.0005674859276041389    Test Loss :0.001111600431613624\n",
      "Epoch :0.85    Train Loss :0.0005652879481203854    Test Loss :0.0011042632395401597\n",
      "Epoch :0.8625    Train Loss :0.0005792017327621579    Test Loss :0.0011434537591412663\n",
      "Epoch :0.875    Train Loss :0.0005447898292914033    Test Loss :0.0012137263547629118\n",
      "Epoch :0.8875    Train Loss :0.0005740353954024613    Test Loss :0.0011081484844908118\n",
      "Epoch :0.9    Train Loss :0.0005744598456658423    Test Loss :0.0011018262011930346\n",
      "Epoch :0.9125    Train Loss :0.0005316328606568277    Test Loss :0.0011816491605713964\n",
      "Epoch :0.925    Train Loss :0.000544783950317651    Test Loss :0.0011143176816403866\n",
      "Epoch :0.9375    Train Loss :0.0005374557804316282    Test Loss :0.0011513142380863428\n",
      "Epoch :0.95    Train Loss :0.0005316485185176134    Test Loss :0.0009924875339493155\n",
      "Epoch :0.9625    Train Loss :0.0005538425175473094    Test Loss :0.0010319093707948923\n",
      "Epoch :0.975    Train Loss :0.0005447674193419516    Test Loss :0.001036428613588214\n",
      "Epoch :0.9875    Train Loss :0.0005421998212113976    Test Loss :0.0010516253532841802\n",
      "Epoch :1.0    Train Loss :0.0005535887903533876    Test Loss :0.0010196371003985405\n",
      "RMSE: 8.285409124718601\n",
      "MAE: 6.641606111270514\n",
      "MAPE: 5.948088508486758%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 400, 'hid_size': 100, 'lr': 0.01, 'num_layers': 2}\n",
      "total:  66.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.08215173333883286    Test Loss :0.07643218338489532\n",
      "Epoch :0.025    Train Loss :0.04849761724472046    Test Loss :0.07093826681375504\n",
      "Epoch :0.0375    Train Loss :0.025215571746230125    Test Loss :0.03228970244526863\n",
      "Epoch :0.05    Train Loss :0.01642817258834839    Test Loss :0.05563979968428612\n",
      "Epoch :0.0625    Train Loss :0.007670175284147263    Test Loss :0.009986823424696922\n",
      "Epoch :0.075    Train Loss :0.007986962795257568    Test Loss :0.011119435541331768\n",
      "Epoch :0.0875    Train Loss :0.004353209864348173    Test Loss :0.006300340872257948\n",
      "Epoch :0.1    Train Loss :0.003348227823153138    Test Loss :0.004918980412185192\n",
      "Epoch :0.1125    Train Loss :0.0027822835836559534    Test Loss :0.00911671295762062\n",
      "Epoch :0.125    Train Loss :0.0021117969881743193    Test Loss :0.005301371216773987\n",
      "Epoch :0.1375    Train Loss :0.002131787361577153    Test Loss :0.0034113889560103416\n",
      "Epoch :0.15    Train Loss :0.0017235191771760583    Test Loss :0.0031279793474823236\n",
      "Epoch :0.1625    Train Loss :0.001567741739563644    Test Loss :0.0036930134519934654\n",
      "Epoch :0.175    Train Loss :0.0015039050485938787    Test Loss :0.0032662181183695793\n",
      "Epoch :0.1875    Train Loss :0.0014975906815379858    Test Loss :0.003364022122696042\n",
      "Epoch :0.2    Train Loss :0.001259489799849689    Test Loss :0.002993387868627906\n",
      "Epoch :0.2125    Train Loss :0.0012225678656250238    Test Loss :0.0027200060430914164\n",
      "Epoch :0.225    Train Loss :0.001229819841682911    Test Loss :0.002577218459919095\n",
      "Epoch :0.2375    Train Loss :0.0010860466863960028    Test Loss :0.0027532053645700216\n",
      "Epoch :0.25    Train Loss :0.0011288244277238846    Test Loss :0.0025647005531936884\n",
      "Epoch :0.2625    Train Loss :0.0010351550299674273    Test Loss :0.002596580423414707\n",
      "Epoch :0.275    Train Loss :0.0010222411947324872    Test Loss :0.002619048347696662\n",
      "Epoch :0.2875    Train Loss :0.0009754061466082931    Test Loss :0.002919612918049097\n",
      "Epoch :0.3    Train Loss :0.0010570205049589276    Test Loss :0.0027594203129410744\n",
      "Epoch :0.3125    Train Loss :0.0009276191121898592    Test Loss :0.0027074364479631186\n",
      "Epoch :0.325    Train Loss :0.0009053573594428599    Test Loss :0.0025168966967612505\n",
      "Epoch :0.3375    Train Loss :0.0009366991580463946    Test Loss :0.002432961016893387\n",
      "Epoch :0.35    Train Loss :0.0014072961639612913    Test Loss :0.003334606299176812\n",
      "Epoch :0.3625    Train Loss :0.0011116190580651164    Test Loss :0.002622079337015748\n",
      "Epoch :0.375    Train Loss :0.000806742231361568    Test Loss :0.0023788646794855595\n",
      "Epoch :0.3875    Train Loss :0.0009699813090264797    Test Loss :0.0025977957993745804\n",
      "Epoch :0.4    Train Loss :0.0007978189969435334    Test Loss :0.002514248713850975\n",
      "Epoch :0.4125    Train Loss :0.0007557875942438841    Test Loss :0.002430110704153776\n",
      "Epoch :0.425    Train Loss :0.0007333186804316938    Test Loss :0.0024201495107263327\n",
      "Epoch :0.4375    Train Loss :0.0007730402867309749    Test Loss :0.002495391061529517\n",
      "Epoch :0.45    Train Loss :0.0007283040904439986    Test Loss :0.0025600618682801723\n",
      "Epoch :0.4625    Train Loss :0.0008048460585996509    Test Loss :0.002424959558993578\n",
      "Epoch :0.475    Train Loss :0.0008186757331714034    Test Loss :0.002367311157286167\n",
      "Epoch :0.4875    Train Loss :0.0008370346622541547    Test Loss :0.002545227762311697\n",
      "Epoch :0.5    Train Loss :0.0007640216499567032    Test Loss :0.0025430615060031414\n",
      "Epoch :0.5125    Train Loss :0.0006525160279124975    Test Loss :0.0024384085554629564\n",
      "Epoch :0.525    Train Loss :0.0007343802717514336    Test Loss :0.0024572780821472406\n",
      "Epoch :0.5375    Train Loss :0.0006863831076771021    Test Loss :0.002400108380243182\n",
      "Epoch :0.55    Train Loss :0.0006301752873696387    Test Loss :0.0023251331876963377\n",
      "Epoch :0.5625    Train Loss :0.0006267208373174071    Test Loss :0.002348783891648054\n",
      "Epoch :0.575    Train Loss :0.0006507984362542629    Test Loss :0.002313371514901519\n",
      "Epoch :0.5875    Train Loss :0.0006689107976853848    Test Loss :0.0023437044583261013\n",
      "Epoch :0.6    Train Loss :0.0006717309006489813    Test Loss :0.0023228731006383896\n",
      "Epoch :0.6125    Train Loss :0.0005972818471491337    Test Loss :0.002269132761284709\n",
      "Epoch :0.625    Train Loss :0.0006049591465853155    Test Loss :0.0022861601319164038\n",
      "Epoch :0.6375    Train Loss :0.000785446201916784    Test Loss :0.0025970314163714647\n",
      "Epoch :0.65    Train Loss :0.0005944615113548934    Test Loss :0.00231560948304832\n",
      "Epoch :0.6625    Train Loss :0.0007549374713562429    Test Loss :0.0024332066532224417\n",
      "Epoch :0.675    Train Loss :0.0005971401114948094    Test Loss :0.002292977413162589\n",
      "Epoch :0.6875    Train Loss :0.0006878828280605376    Test Loss :0.002426224760711193\n",
      "Epoch :0.7    Train Loss :0.0006104099447838962    Test Loss :0.0022864274214953184\n",
      "Epoch :0.7125    Train Loss :0.000713495712261647    Test Loss :0.0024259008932858706\n",
      "Epoch :0.725    Train Loss :0.0008115247474052012    Test Loss :0.002102244645357132\n",
      "Epoch :0.7375    Train Loss :0.0007335435366258025    Test Loss :0.0023327104281634092\n",
      "Epoch :0.75    Train Loss :0.0007220119587145746    Test Loss :0.00240080407820642\n",
      "Epoch :0.7625    Train Loss :0.0006390315247699618    Test Loss :0.002238610992208123\n",
      "Epoch :0.775    Train Loss :0.0007714501698501408    Test Loss :0.0021909400820732117\n",
      "Epoch :0.7875    Train Loss :0.0007598357042297721    Test Loss :0.0022350861690938473\n",
      "Epoch :0.8    Train Loss :0.000662370934151113    Test Loss :0.0022171372547745705\n",
      "Epoch :0.8125    Train Loss :0.000568450428545475    Test Loss :0.0022400631569325924\n",
      "Epoch :0.825    Train Loss :0.0005518189282156527    Test Loss :0.002149551408365369\n",
      "Epoch :0.8375    Train Loss :0.0005505214212462306    Test Loss :0.0020953703206032515\n",
      "Epoch :0.85    Train Loss :0.0005505926092155278    Test Loss :0.002132655354216695\n",
      "Epoch :0.8625    Train Loss :0.0006536587607115507    Test Loss :0.0020772528368979692\n",
      "Epoch :0.875    Train Loss :0.0005418926593847573    Test Loss :0.0020057091023772955\n",
      "Epoch :0.8875    Train Loss :0.0005803452804684639    Test Loss :0.0022098589688539505\n",
      "Epoch :0.9    Train Loss :0.0005047012236900628    Test Loss :0.002077538752928376\n",
      "Epoch :0.9125    Train Loss :0.0005136501858942211    Test Loss :0.002030589384958148\n",
      "Epoch :0.925    Train Loss :0.0005579690332524478    Test Loss :0.002097706776112318\n",
      "Epoch :0.9375    Train Loss :0.0005171785596758127    Test Loss :0.002037906553596258\n",
      "Epoch :0.95    Train Loss :0.0004845424846280366    Test Loss :0.0019915259908884764\n",
      "Epoch :0.9625    Train Loss :0.000505398609675467    Test Loss :0.0019952389411628246\n",
      "Epoch :0.975    Train Loss :0.0005622277967631817    Test Loss :0.001982191577553749\n",
      "Epoch :0.9875    Train Loss :0.0010687527246773243    Test Loss :0.003249191679060459\n",
      "Epoch :1.0    Train Loss :0.0007608728483319283    Test Loss :0.002143049845471978\n",
      "RMSE: 8.730227055129621\n",
      "MAE: 7.145941414874727\n",
      "MAPE: 6.492581805334933%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 400, 'hid_size': 100, 'lr': 0.01, 'num_layers': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  69.0\n",
      "Epoch :0.0125    Train Loss :0.174862802028656    Test Loss :0.1844761222600937\n",
      "Epoch :0.025    Train Loss :0.07387198507785797    Test Loss :0.19435438513755798\n",
      "Epoch :0.0375    Train Loss :0.04507119953632355    Test Loss :0.0352276973426342\n",
      "Epoch :0.05    Train Loss :0.04227223992347717    Test Loss :0.09698396921157837\n",
      "Epoch :0.0625    Train Loss :0.043260470032691956    Test Loss :0.10592690855264664\n",
      "Epoch :0.075    Train Loss :0.03125673905014992    Test Loss :0.040254198014736176\n",
      "Epoch :0.0875    Train Loss :0.006979446858167648    Test Loss :0.015358000993728638\n",
      "Epoch :0.1    Train Loss :0.0035171937197446823    Test Loss :0.0068816859275102615\n",
      "Epoch :0.1125    Train Loss :0.006082254461944103    Test Loss :0.013779167085886002\n",
      "Epoch :0.125    Train Loss :0.0028739997651427984    Test Loss :0.0035184600856155157\n",
      "Epoch :0.1375    Train Loss :0.0022364824544638395    Test Loss :0.004570946097373962\n",
      "Epoch :0.15    Train Loss :0.002328727161511779    Test Loss :0.003634499153122306\n",
      "Epoch :0.1625    Train Loss :0.001856595859862864    Test Loss :0.002663344843313098\n",
      "Epoch :0.175    Train Loss :0.0017079440876841545    Test Loss :0.0033681895583868027\n",
      "Epoch :0.1875    Train Loss :0.0016072086291387677    Test Loss :0.004065617453306913\n",
      "Epoch :0.2    Train Loss :0.001510990085080266    Test Loss :0.002820908557623625\n",
      "Epoch :0.2125    Train Loss :0.0013283517910167575    Test Loss :0.0030922882724553347\n",
      "Epoch :0.225    Train Loss :0.0013476712629199028    Test Loss :0.0029155181255191565\n",
      "Epoch :0.2375    Train Loss :0.0012694640317931771    Test Loss :0.002480375114828348\n",
      "Epoch :0.25    Train Loss :0.0012175037991255522    Test Loss :0.002199196256697178\n",
      "Epoch :0.2625    Train Loss :0.0012229300336912274    Test Loss :0.0023032883182168007\n",
      "Epoch :0.275    Train Loss :0.001154756173491478    Test Loss :0.0023967651650309563\n",
      "Epoch :0.2875    Train Loss :0.0011414213804528117    Test Loss :0.002457990776747465\n",
      "Epoch :0.3    Train Loss :0.0010523516684770584    Test Loss :0.0020467061549425125\n",
      "Epoch :0.3125    Train Loss :0.001051291124895215    Test Loss :0.0021811022888869047\n",
      "Epoch :0.325    Train Loss :0.0010712638031691313    Test Loss :0.0019358466379344463\n",
      "Epoch :0.3375    Train Loss :0.0010118396021425724    Test Loss :0.0021444829180836678\n",
      "Epoch :0.35    Train Loss :0.001052277977578342    Test Loss :0.001992427511140704\n",
      "Epoch :0.3625    Train Loss :0.0009756468934938312    Test Loss :0.0018691299483180046\n",
      "Epoch :0.375    Train Loss :0.0009080888703465462    Test Loss :0.0018372677732259035\n",
      "Epoch :0.3875    Train Loss :0.0010338883148506284    Test Loss :0.0017842318629845977\n",
      "Epoch :0.4    Train Loss :0.0009176845196634531    Test Loss :0.0017032218165695667\n",
      "Epoch :0.4125    Train Loss :0.0008822122472338378    Test Loss :0.0021817630622535944\n",
      "Epoch :0.425    Train Loss :0.000889273127540946    Test Loss :0.0017753418069332838\n",
      "Epoch :0.4375    Train Loss :0.0009124843636527658    Test Loss :0.001682902337051928\n",
      "Epoch :0.45    Train Loss :0.0008697188459336758    Test Loss :0.0016256209928542376\n",
      "Epoch :0.4625    Train Loss :0.0009259360958822072    Test Loss :0.001649694168008864\n",
      "Epoch :0.475    Train Loss :0.0008444332052022219    Test Loss :0.0015781227266415954\n",
      "Epoch :0.4875    Train Loss :0.0008584510069340467    Test Loss :0.0015284238616004586\n",
      "Epoch :0.5    Train Loss :0.0007679105037823319    Test Loss :0.0016954735619947314\n",
      "Epoch :0.5125    Train Loss :0.0008245825301855803    Test Loss :0.0014989617047831416\n",
      "Epoch :0.525    Train Loss :0.0007663092692382634    Test Loss :0.0014718625461682677\n",
      "Epoch :0.5375    Train Loss :0.0007290624780580401    Test Loss :0.001405655755661428\n",
      "Epoch :0.55    Train Loss :0.0007610070751979947    Test Loss :0.0014488871674984694\n",
      "Epoch :0.5625    Train Loss :0.0007257419056259096    Test Loss :0.001405776129104197\n",
      "Epoch :0.575    Train Loss :0.0008089886396192014    Test Loss :0.0014578507980331779\n",
      "Epoch :0.5875    Train Loss :0.0007262594881467521    Test Loss :0.0013360329903662205\n",
      "Epoch :0.6    Train Loss :0.0007008201209828258    Test Loss :0.0013320872094482183\n",
      "Epoch :0.6125    Train Loss :0.0007145178969949484    Test Loss :0.001300253439694643\n",
      "Epoch :0.625    Train Loss :0.0006413788651116192    Test Loss :0.0012850809143856168\n",
      "Epoch :0.6375    Train Loss :0.0006761708646081388    Test Loss :0.001390781020745635\n",
      "Epoch :0.65    Train Loss :0.0006517434376291931    Test Loss :0.0012656317558139563\n",
      "Epoch :0.6625    Train Loss :0.0006580809131264687    Test Loss :0.0011917377123609185\n",
      "Epoch :0.675    Train Loss :0.0006405059830285609    Test Loss :0.0011558379046618938\n",
      "Epoch :0.6875    Train Loss :0.000628835812676698    Test Loss :0.001079967594705522\n",
      "Epoch :0.7    Train Loss :0.0006053268443793058    Test Loss :0.0009946331847459078\n",
      "Epoch :0.7125    Train Loss :0.0006399362464435399    Test Loss :0.00104230223223567\n",
      "Epoch :0.725    Train Loss :0.0006439133430831134    Test Loss :0.0013423854252323508\n",
      "Epoch :0.7375    Train Loss :0.0006325588910840452    Test Loss :0.001206177519634366\n",
      "Epoch :0.75    Train Loss :0.0005391345475800335    Test Loss :0.0010229231556877494\n",
      "Epoch :0.7625    Train Loss :0.0006684350082650781    Test Loss :0.0010674915974959731\n",
      "Epoch :0.775    Train Loss :0.0005718659376725554    Test Loss :0.0011062375269830227\n",
      "Epoch :0.7875    Train Loss :0.0005587620544247329    Test Loss :0.0011706722434610128\n",
      "Epoch :0.8    Train Loss :0.0005803866079077125    Test Loss :0.0010061664506793022\n",
      "Epoch :0.8125    Train Loss :0.0006163815269246697    Test Loss :0.0012053630780428648\n",
      "Epoch :0.825    Train Loss :0.0005701523623429239    Test Loss :0.001120598055422306\n",
      "Epoch :0.8375    Train Loss :0.000545164046343416    Test Loss :0.0009745822753757238\n",
      "Epoch :0.85    Train Loss :0.0005632854299619794    Test Loss :0.0009950244566425681\n",
      "Epoch :0.8625    Train Loss :0.0005637566209770739    Test Loss :0.0009360656258650124\n",
      "Epoch :0.875    Train Loss :0.0005452458863146603    Test Loss :0.0009852651273831725\n",
      "Epoch :0.8875    Train Loss :0.0005518151447176933    Test Loss :0.001144752255640924\n",
      "Epoch :0.9    Train Loss :0.000566031550988555    Test Loss :0.0010463754879310727\n",
      "Epoch :0.9125    Train Loss :0.0005736088496632874    Test Loss :0.0010146331042051315\n",
      "Epoch :0.925    Train Loss :0.0005456416984088719    Test Loss :0.0010955692268908024\n",
      "Epoch :0.9375    Train Loss :0.0006522449548356235    Test Loss :0.0011960762785747647\n",
      "Epoch :0.95    Train Loss :0.0005986512405797839    Test Loss :0.000988852814771235\n",
      "Epoch :0.9625    Train Loss :0.0004716171242762357    Test Loss :0.0009672401356510818\n",
      "Epoch :0.975    Train Loss :0.0005100638372823596    Test Loss :0.0009513134718872607\n",
      "Epoch :0.9875    Train Loss :0.00047128129517659545    Test Loss :0.0008978222031146288\n",
      "Epoch :1.0    Train Loss :0.0004961137310601771    Test Loss :0.0009224361274391413\n",
      "RMSE: 9.31649315940858\n",
      "MAE: 7.492350344736108\n",
      "MAPE: 6.6835859793846%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 400, 'hid_size': 100, 'lr': 0.02, 'num_layers': 2}\n",
      "total:  72.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.603184163570404    Test Loss :0.3193516433238983\n",
      "Epoch :0.025    Train Loss :0.11482303589582443    Test Loss :0.285066694021225\n",
      "Epoch :0.0375    Train Loss :0.06540422886610031    Test Loss :0.1067681685090065\n",
      "Epoch :0.05    Train Loss :0.04870007932186127    Test Loss :0.05982718616724014\n",
      "Epoch :0.0625    Train Loss :0.04935754835605621    Test Loss :0.15998214483261108\n",
      "Epoch :0.075    Train Loss :0.0509040467441082    Test Loss :0.0596320740878582\n",
      "Epoch :0.0875    Train Loss :0.05176689475774765    Test Loss :0.1414584368467331\n",
      "Epoch :0.1    Train Loss :0.05107851326465607    Test Loss :0.08069565892219543\n",
      "Epoch :0.1125    Train Loss :0.04956886172294617    Test Loss :0.10564424842596054\n",
      "Epoch :0.125    Train Loss :0.048576049506664276    Test Loss :0.10998446494340897\n",
      "Epoch :0.1375    Train Loss :0.04860248044133186    Test Loss :0.08787993341684341\n",
      "Epoch :0.15    Train Loss :0.04884720966219902    Test Loss :0.11002914607524872\n",
      "Epoch :0.1625    Train Loss :0.04861842095851898    Test Loss :0.10108841210603714\n",
      "Epoch :0.175    Train Loss :0.048471901565790176    Test Loss :0.09533223509788513\n",
      "Epoch :0.1875    Train Loss :0.04853928089141846    Test Loss :0.10500119626522064\n",
      "Epoch :0.2    Train Loss :0.048552803695201874    Test Loss :0.10208336263895035\n",
      "Epoch :0.2125    Train Loss :0.048501480370759964    Test Loss :0.09785401821136475\n",
      "Epoch :0.225    Train Loss :0.04856989532709122    Test Loss :0.10143425315618515\n",
      "Epoch :0.2375    Train Loss :0.04852049797773361    Test Loss :0.10266353189945221\n",
      "Epoch :0.25    Train Loss :0.04853825271129608    Test Loss :0.10037443786859512\n",
      "Epoch :0.2625    Train Loss :0.0484924241900444    Test Loss :0.09985731542110443\n",
      "Epoch :0.275    Train Loss :0.04855336621403694    Test Loss :0.10120084881782532\n",
      "Epoch :0.2875    Train Loss :0.048528410494327545    Test Loss :0.10174388438463211\n",
      "Epoch :0.3    Train Loss :0.04850342497229576    Test Loss :0.10079540312290192\n",
      "Epoch :0.3125    Train Loss :0.048486918210983276    Test Loss :0.10037006437778473\n",
      "Epoch :0.325    Train Loss :0.048476628959178925    Test Loss :0.10072055459022522\n",
      "Epoch :0.3375    Train Loss :0.048519834876060486    Test Loss :0.10114516317844391\n",
      "Epoch :0.35    Train Loss :0.04849383607506752    Test Loss :0.10123593360185623\n",
      "Epoch :0.3625    Train Loss :0.048513900488615036    Test Loss :0.1007867380976677\n",
      "Epoch :0.375    Train Loss :0.048505473881959915    Test Loss :0.10071664303541183\n",
      "Epoch :0.3875    Train Loss :0.04846150800585747    Test Loss :0.1008664220571518\n",
      "Epoch :0.4    Train Loss :0.04852362722158432    Test Loss :0.10100021213293076\n",
      "Epoch :0.4125    Train Loss :0.04851050674915314    Test Loss :0.1009758859872818\n",
      "Epoch :0.425    Train Loss :0.04850802198052406    Test Loss :0.10091588646173477\n",
      "Epoch :0.4375    Train Loss :0.048510562628507614    Test Loss :0.100972481071949\n",
      "Epoch :0.45    Train Loss :0.048491619527339935    Test Loss :0.1008838638663292\n",
      "Epoch :0.4625    Train Loss :0.04852309450507164    Test Loss :0.1009027436375618\n",
      "Epoch :0.475    Train Loss :0.04848266392946243    Test Loss :0.10094399005174637\n",
      "Epoch :0.4875    Train Loss :0.04847530275583267    Test Loss :0.10104216635227203\n",
      "Epoch :0.5    Train Loss :0.048533037304878235    Test Loss :0.10109947621822357\n",
      "Epoch :0.5125    Train Loss :0.04854930564761162    Test Loss :0.10101883858442307\n",
      "Epoch :0.525    Train Loss :0.0484607107937336    Test Loss :0.1009349450469017\n",
      "Epoch :0.5375    Train Loss :0.04852555692195892    Test Loss :0.1008801981806755\n",
      "Epoch :0.55    Train Loss :0.04853149503469467    Test Loss :0.10080187767744064\n",
      "Epoch :0.5625    Train Loss :0.04851113632321358    Test Loss :0.1007884219288826\n",
      "Epoch :0.575    Train Loss :0.048496268689632416    Test Loss :0.10084857046604156\n",
      "Epoch :0.5875    Train Loss :0.04848792403936386    Test Loss :0.10091067850589752\n",
      "Epoch :0.6    Train Loss :0.04850778728723526    Test Loss :0.10079533606767654\n",
      "Epoch :0.6125    Train Loss :0.048502106219530106    Test Loss :0.10080961138010025\n",
      "Epoch :0.625    Train Loss :0.04848757013678551    Test Loss :0.10090494900941849\n",
      "Epoch :0.6375    Train Loss :0.0485072061419487    Test Loss :0.1010717898607254\n",
      "Epoch :0.65    Train Loss :0.04849955812096596    Test Loss :0.10098422318696976\n",
      "Epoch :0.6625    Train Loss :0.04849059507250786    Test Loss :0.10099321603775024\n",
      "Epoch :0.675    Train Loss :0.04853106662631035    Test Loss :0.10103916376829147\n",
      "Epoch :0.6875    Train Loss :0.048504963517189026    Test Loss :0.10102245956659317\n",
      "Epoch :0.7    Train Loss :0.04848571866750717    Test Loss :0.10113286972045898\n",
      "Epoch :0.7125    Train Loss :0.04850258305668831    Test Loss :0.10100795328617096\n",
      "Epoch :0.725    Train Loss :0.048487626016139984    Test Loss :0.10109907388687134\n",
      "Epoch :0.7375    Train Loss :0.048508599400520325    Test Loss :0.10109248757362366\n",
      "Epoch :0.75    Train Loss :0.04849415645003319    Test Loss :0.10107625275850296\n",
      "Epoch :0.7625    Train Loss :0.04852994531393051    Test Loss :0.1011100560426712\n",
      "Epoch :0.775    Train Loss :0.048521846532821655    Test Loss :0.10113202035427094\n",
      "Epoch :0.7875    Train Loss :0.04853137955069542    Test Loss :0.10102397203445435\n",
      "Epoch :0.8    Train Loss :0.0484946183860302    Test Loss :0.10106775909662247\n",
      "Epoch :0.8125    Train Loss :0.048515282571315765    Test Loss :0.10100135952234268\n",
      "Epoch :0.825    Train Loss :0.0484963096678257    Test Loss :0.10105251520872116\n",
      "Epoch :0.8375    Train Loss :0.048523157835006714    Test Loss :0.1010078489780426\n",
      "Epoch :0.85    Train Loss :0.04851756989955902    Test Loss :0.10088334232568741\n",
      "Epoch :0.8625    Train Loss :0.04853455349802971    Test Loss :0.10094350576400757\n",
      "Epoch :0.875    Train Loss :0.048525046557188034    Test Loss :0.1009274572134018\n",
      "Epoch :0.8875    Train Loss :0.04850658029317856    Test Loss :0.10075578838586807\n",
      "Epoch :0.9    Train Loss :0.048505350947380066    Test Loss :0.1009899377822876\n",
      "Epoch :0.9125    Train Loss :0.04849337413907051    Test Loss :0.10085512697696686\n",
      "Epoch :0.925    Train Loss :0.048490218818187714    Test Loss :0.10080864280462265\n",
      "Epoch :0.9375    Train Loss :0.04851618409156799    Test Loss :0.10066607594490051\n",
      "Epoch :0.95    Train Loss :0.04851420596241951    Test Loss :0.10078005492687225\n",
      "Epoch :0.9625    Train Loss :0.048504430800676346    Test Loss :0.10091762989759445\n",
      "Epoch :0.975    Train Loss :0.04848097637295723    Test Loss :0.10099441558122635\n",
      "Epoch :0.9875    Train Loss :0.04849280044436455    Test Loss :0.10096229612827301\n",
      "Epoch :1.0    Train Loss :0.048532940447330475    Test Loss :0.10079188644886017\n",
      "RMSE: 30.708544611509343\n",
      "MAE: 29.685417962277906\n",
      "MAPE: 25.849992689487404%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 400, 'hid_size': 100, 'lr': 0.02, 'num_layers': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  75.0\n",
      "Epoch :0.008333333333333333    Train Loss :0.04640703275799751    Test Loss :0.13576067984104156\n",
      "Epoch :0.016666666666666666    Train Loss :0.04423554986715317    Test Loss :0.08353651314973831\n",
      "Epoch :0.025    Train Loss :0.03259241580963135    Test Loss :0.04125507175922394\n",
      "Epoch :0.03333333333333333    Train Loss :0.006966351997107267    Test Loss :0.04447737708687782\n",
      "Epoch :0.041666666666666664    Train Loss :0.00508143613114953    Test Loss :0.015328072011470795\n",
      "Epoch :0.05    Train Loss :0.006579321343451738    Test Loss :0.016138583421707153\n",
      "Epoch :0.058333333333333334    Train Loss :0.0030037355609238148    Test Loss :0.004599696956574917\n",
      "Epoch :0.06666666666666667    Train Loss :0.0021572462283074856    Test Loss :0.004832705948501825\n",
      "Epoch :0.075    Train Loss :0.0023514912463724613    Test Loss :0.007140966132283211\n",
      "Epoch :0.08333333333333333    Train Loss :0.0021358123049139977    Test Loss :0.0035166728775948286\n",
      "Epoch :0.09166666666666666    Train Loss :0.00143334548920393    Test Loss :0.0032893898896872997\n",
      "Epoch :0.1    Train Loss :0.0015680963406339288    Test Loss :0.003121384186670184\n",
      "Epoch :0.10833333333333334    Train Loss :0.0014133199583739042    Test Loss :0.0025368949864059687\n",
      "Epoch :0.11666666666666667    Train Loss :0.0013724141754209995    Test Loss :0.0028150705620646477\n",
      "Epoch :0.125    Train Loss :0.001230220659635961    Test Loss :0.002702076453715563\n",
      "Epoch :0.13333333333333333    Train Loss :0.0011485809227451682    Test Loss :0.0027085726615041494\n",
      "Epoch :0.14166666666666666    Train Loss :0.0011645137565210462    Test Loss :0.0026141058187931776\n",
      "Epoch :0.15    Train Loss :0.0011117609683424234    Test Loss :0.0022489922121167183\n",
      "Epoch :0.15833333333333333    Train Loss :0.0010252707870677114    Test Loss :0.0024544205516576767\n",
      "Epoch :0.16666666666666666    Train Loss :0.001000086311250925    Test Loss :0.0021194731816649437\n",
      "Epoch :0.175    Train Loss :0.001023847726173699    Test Loss :0.0020944150164723396\n",
      "Epoch :0.18333333333333332    Train Loss :0.0009579482139088213    Test Loss :0.0019501164788380265\n",
      "Epoch :0.19166666666666668    Train Loss :0.0009065650519914925    Test Loss :0.0018881052965298295\n",
      "Epoch :0.2    Train Loss :0.0009459077846258879    Test Loss :0.0018922231392934918\n",
      "Epoch :0.20833333333333334    Train Loss :0.0009165051742456853    Test Loss :0.0019663257990032434\n",
      "Epoch :0.21666666666666667    Train Loss :0.0008486091974191368    Test Loss :0.0016886867815628648\n",
      "Epoch :0.225    Train Loss :0.0007692580693401396    Test Loss :0.0017892573960125446\n",
      "Epoch :0.23333333333333334    Train Loss :0.0008343032095581293    Test Loss :0.001676406478509307\n",
      "Epoch :0.24166666666666667    Train Loss :0.0007908031111583114    Test Loss :0.0015616638120263815\n",
      "Epoch :0.25    Train Loss :0.000777697132434696    Test Loss :0.001592450775206089\n",
      "Epoch :0.25833333333333336    Train Loss :0.0007692892104387283    Test Loss :0.0014255150454118848\n",
      "Epoch :0.26666666666666666    Train Loss :0.0007513938471674919    Test Loss :0.001524527557194233\n",
      "Epoch :0.275    Train Loss :0.0006837567198090255    Test Loss :0.0013391064712777734\n",
      "Epoch :0.2833333333333333    Train Loss :0.0008033381891436875    Test Loss :0.001389659009873867\n",
      "Epoch :0.2916666666666667    Train Loss :0.0006886061164550483    Test Loss :0.001325175748206675\n",
      "Epoch :0.3    Train Loss :0.000713760731741786    Test Loss :0.0013317748671397567\n",
      "Epoch :0.30833333333333335    Train Loss :0.000684368540532887    Test Loss :0.0013989924918860197\n",
      "Epoch :0.31666666666666665    Train Loss :0.0006739752716384828    Test Loss :0.0013179739471524954\n",
      "Epoch :0.325    Train Loss :0.0006796300876885653    Test Loss :0.0013819410232827067\n",
      "Epoch :0.3333333333333333    Train Loss :0.0006602727808058262    Test Loss :0.0013069146079942584\n",
      "Epoch :0.3416666666666667    Train Loss :0.000656352611258626    Test Loss :0.001328607788309455\n",
      "Epoch :0.35    Train Loss :0.000662907084915787    Test Loss :0.001285611535422504\n",
      "Epoch :0.35833333333333334    Train Loss :0.0007019774056971073    Test Loss :0.0013307405170053244\n",
      "Epoch :0.36666666666666664    Train Loss :0.0006293374462984502    Test Loss :0.0012567943194881082\n",
      "Epoch :0.375    Train Loss :0.0006714714108966291    Test Loss :0.0013004594948142767\n",
      "Epoch :0.38333333333333336    Train Loss :0.0006418625125661492    Test Loss :0.0014294913271442056\n",
      "Epoch :0.39166666666666666    Train Loss :0.0006551964906975627    Test Loss :0.0011137063847854733\n",
      "Epoch :0.4    Train Loss :0.0006238337955437601    Test Loss :0.0011085731675848365\n",
      "Epoch :0.4083333333333333    Train Loss :0.0006347723538056016    Test Loss :0.0012537214206531644\n",
      "Epoch :0.4166666666666667    Train Loss :0.000616662495303899    Test Loss :0.001138084102421999\n",
      "Epoch :0.425    Train Loss :0.0006206078105606139    Test Loss :0.0013096181210130453\n",
      "Epoch :0.43333333333333335    Train Loss :0.0006225280230864882    Test Loss :0.001257129362784326\n",
      "Epoch :0.44166666666666665    Train Loss :0.0005871448665857315    Test Loss :0.001159763429313898\n",
      "Epoch :0.45    Train Loss :0.0006081988685764372    Test Loss :0.0011368334526196122\n",
      "Epoch :0.4583333333333333    Train Loss :0.0006386848399415612    Test Loss :0.0011344245867803693\n",
      "Epoch :0.4666666666666667    Train Loss :0.0006079244194552302    Test Loss :0.001126372953876853\n",
      "Epoch :0.475    Train Loss :0.0006084021297283471    Test Loss :0.0011347635881975293\n",
      "Epoch :0.48333333333333334    Train Loss :0.0005946349701844156    Test Loss :0.001187408110126853\n",
      "Epoch :0.49166666666666664    Train Loss :0.0006404956220649183    Test Loss :0.0011216123821213841\n",
      "Epoch :0.5    Train Loss :0.0006107724038884044    Test Loss :0.001606792095117271\n",
      "Epoch :0.5083333333333333    Train Loss :0.000685565231833607    Test Loss :0.0014027297729626298\n",
      "Epoch :0.5166666666666667    Train Loss :0.00065817084396258    Test Loss :0.0011979700066149235\n",
      "Epoch :0.525    Train Loss :0.0006074662087485194    Test Loss :0.001054639695212245\n",
      "Epoch :0.5333333333333333    Train Loss :0.0006140605546534061    Test Loss :0.0010869645047932863\n",
      "Epoch :0.5416666666666666    Train Loss :0.0005636920686811209    Test Loss :0.0011694316053763032\n",
      "Epoch :0.55    Train Loss :0.0006057945429347456    Test Loss :0.0014208524953573942\n",
      "Epoch :0.5583333333333333    Train Loss :0.0005953911459073424    Test Loss :0.001438711304217577\n",
      "Epoch :0.5666666666666667    Train Loss :0.0007139758090488613    Test Loss :0.0013892846181988716\n",
      "Epoch :0.575    Train Loss :0.0006170850247144699    Test Loss :0.0011340484488755465\n",
      "Epoch :0.5833333333333334    Train Loss :0.0005431531462818384    Test Loss :0.0010909573175013065\n",
      "Epoch :0.5916666666666667    Train Loss :0.0005860800156369805    Test Loss :0.0011894232593476772\n",
      "Epoch :0.6    Train Loss :0.0006128234672360122    Test Loss :0.0010453354334458709\n",
      "Epoch :0.6083333333333333    Train Loss :0.0005311361746862531    Test Loss :0.0010068397969007492\n",
      "Epoch :0.6166666666666667    Train Loss :0.0005410356679931283    Test Loss :0.001016707275994122\n",
      "Epoch :0.625    Train Loss :0.0005777431069873273    Test Loss :0.001332692801952362\n",
      "Epoch :0.6333333333333333    Train Loss :0.0005295821465551853    Test Loss :0.0010847076773643494\n",
      "Epoch :0.6416666666666667    Train Loss :0.0005476955557242036    Test Loss :0.0010684035951271653\n",
      "Epoch :0.65    Train Loss :0.0007884403457865119    Test Loss :0.0013080623466521502\n",
      "Epoch :0.6583333333333333    Train Loss :0.0005441482062451541    Test Loss :0.001002574572339654\n",
      "Epoch :0.6666666666666666    Train Loss :0.0005645177443511784    Test Loss :0.001029529026709497\n",
      "Epoch :0.675    Train Loss :0.000599804159719497    Test Loss :0.0010149057488888502\n",
      "Epoch :0.6833333333333333    Train Loss :0.0006762310513295233    Test Loss :0.0009991005063056946\n",
      "Epoch :0.6916666666666667    Train Loss :0.0005694034043699503    Test Loss :0.0010106710251420736\n",
      "Epoch :0.7    Train Loss :0.0005418738001026213    Test Loss :0.001050810213200748\n",
      "Epoch :0.7083333333333334    Train Loss :0.0005254532443359494    Test Loss :0.0010057042818516493\n",
      "Epoch :0.7166666666666667    Train Loss :0.0005544337909668684    Test Loss :0.0008797964546829462\n",
      "Epoch :0.725    Train Loss :0.0004989885492250323    Test Loss :0.0009284695261158049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7333333333333333    Train Loss :0.000512738130055368    Test Loss :0.0008666710346005857\n",
      "Epoch :0.7416666666666667    Train Loss :0.0004586205759551376    Test Loss :0.0009251867886632681\n",
      "Epoch :0.75    Train Loss :0.00045993609819561243    Test Loss :0.0011007181601598859\n",
      "Epoch :0.7583333333333333    Train Loss :0.0009958597365766764    Test Loss :0.0015951552195474505\n",
      "Epoch :0.7666666666666667    Train Loss :0.0007526484550908208    Test Loss :0.0010521244257688522\n",
      "Epoch :0.775    Train Loss :0.0004805230419151485    Test Loss :0.0012604621006175876\n",
      "Epoch :0.7833333333333333    Train Loss :0.0005767088150605559    Test Loss :0.001024880213662982\n",
      "Epoch :0.7916666666666666    Train Loss :0.000502477865666151    Test Loss :0.001009740517474711\n",
      "Epoch :0.8    Train Loss :0.0004794937267433852    Test Loss :0.0011791227152571082\n",
      "Epoch :0.8083333333333333    Train Loss :0.0004657742101699114    Test Loss :0.0008576102554798126\n",
      "Epoch :0.8166666666666667    Train Loss :0.00043344200821593404    Test Loss :0.0008635249105282128\n",
      "Epoch :0.825    Train Loss :0.0004303583991713822    Test Loss :0.0008072400814853609\n",
      "Epoch :0.8333333333333334    Train Loss :0.0004217913665343076    Test Loss :0.0007759789586998522\n",
      "Epoch :0.8416666666666667    Train Loss :0.0004499410861171782    Test Loss :0.0007964623509906232\n",
      "Epoch :0.85    Train Loss :0.00045412639155983925    Test Loss :0.0007604444399476051\n",
      "Epoch :0.8583333333333333    Train Loss :0.0004871372366324067    Test Loss :0.0007703732699155807\n",
      "Epoch :0.8666666666666667    Train Loss :0.0003825541934929788    Test Loss :0.00084679014980793\n",
      "Epoch :0.875    Train Loss :0.0004239346890244633    Test Loss :0.0007675892557017505\n",
      "Epoch :0.8833333333333333    Train Loss :0.0004167480510659516    Test Loss :0.0008091757772490382\n",
      "Epoch :0.8916666666666667    Train Loss :0.000395569164538756    Test Loss :0.0008231821702793241\n",
      "Epoch :0.9    Train Loss :0.0003989182005170733    Test Loss :0.0007495705503970385\n",
      "Epoch :0.9083333333333333    Train Loss :0.00038189091719686985    Test Loss :0.0007763880421407521\n",
      "Epoch :0.9166666666666666    Train Loss :0.00040576831088401377    Test Loss :0.0007142277900129557\n",
      "Epoch :0.925    Train Loss :0.0003620630013756454    Test Loss :0.0007520648650825024\n",
      "Epoch :0.9333333333333333    Train Loss :0.00043433328391984105    Test Loss :0.0007091721636243165\n",
      "Epoch :0.9416666666666667    Train Loss :0.00040138556505553424    Test Loss :0.0008027941803447902\n",
      "Epoch :0.95    Train Loss :0.00036732631269842386    Test Loss :0.0007086073746904731\n",
      "Epoch :0.9583333333333334    Train Loss :0.0003920397721230984    Test Loss :0.0007095490000210702\n",
      "Epoch :0.9666666666666667    Train Loss :0.0004864053044002503    Test Loss :0.0006588861579075456\n",
      "Epoch :0.975    Train Loss :0.00036924442974850535    Test Loss :0.0006806337041780353\n",
      "Epoch :0.9833333333333333    Train Loss :0.0003729850286617875    Test Loss :0.0007354854024015367\n",
      "Epoch :0.9916666666666667    Train Loss :0.00035427219700068235    Test Loss :0.0007097558118402958\n",
      "Epoch :1.0    Train Loss :0.00037482960033230484    Test Loss :0.0008110592607408762\n",
      "RMSE: 51.60936320468899\n",
      "MAE: 50.07333772124914\n",
      "MAPE: 44.21977296046597%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 50, 'lr': 0.01, 'num_layers': 2}\n",
      "total:  78.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.07574199885129929    Test Loss :0.2833413779735565\n",
      "Epoch :0.016666666666666666    Train Loss :0.08430658280849457    Test Loss :0.22096534073352814\n",
      "Epoch :0.025    Train Loss :0.061919040977954865    Test Loss :0.076927550137043\n",
      "Epoch :0.03333333333333333    Train Loss :0.04983999580144882    Test Loss :0.12064055353403091\n",
      "Epoch :0.041666666666666664    Train Loss :0.04891517013311386    Test Loss :0.10480894148349762\n",
      "Epoch :0.05    Train Loss :0.04891309142112732    Test Loss :0.08963260054588318\n",
      "Epoch :0.058333333333333334    Train Loss :0.048654865473508835    Test Loss :0.09838194400072098\n",
      "Epoch :0.06666666666666667    Train Loss :0.048798028379678726    Test Loss :0.10712752491235733\n",
      "Epoch :0.075    Train Loss :0.048308394849300385    Test Loss :0.1008794978260994\n",
      "Epoch :0.08333333333333333    Train Loss :0.04605705663561821    Test Loss :0.08541692793369293\n",
      "Epoch :0.09166666666666666    Train Loss :0.031590692698955536    Test Loss :0.04007662460207939\n",
      "Epoch :0.1    Train Loss :0.020598791539669037    Test Loss :0.028457235544919968\n",
      "Epoch :0.10833333333333334    Train Loss :0.015510092489421368    Test Loss :0.042021363973617554\n",
      "Epoch :0.11666666666666667    Train Loss :0.013406096957623959    Test Loss :0.021711403504014015\n",
      "Epoch :0.125    Train Loss :0.010618606582283974    Test Loss :0.030155343934893608\n",
      "Epoch :0.13333333333333333    Train Loss :0.008083087392151356    Test Loss :0.011254655197262764\n",
      "Epoch :0.14166666666666666    Train Loss :0.006815639324486256    Test Loss :0.012359640561044216\n",
      "Epoch :0.15    Train Loss :0.005001923069357872    Test Loss :0.008203375153243542\n",
      "Epoch :0.15833333333333333    Train Loss :0.004268183838576078    Test Loss :0.006913799326866865\n",
      "Epoch :0.16666666666666666    Train Loss :0.0037436785642057657    Test Loss :0.0068296147510409355\n",
      "Epoch :0.175    Train Loss :0.0033244788646698    Test Loss :0.006906649563461542\n",
      "Epoch :0.18333333333333332    Train Loss :0.0029628912452608347    Test Loss :0.0054710847325623035\n",
      "Epoch :0.19166666666666668    Train Loss :0.002558015054091811    Test Loss :0.004806532524526119\n",
      "Epoch :0.2    Train Loss :0.0024990681558847427    Test Loss :0.004514459520578384\n",
      "Epoch :0.20833333333333334    Train Loss :0.0022048719692975283    Test Loss :0.004261421505361795\n",
      "Epoch :0.21666666666666667    Train Loss :0.0021900255233049393    Test Loss :0.004705179948359728\n",
      "Epoch :0.225    Train Loss :0.0020668907091021538    Test Loss :0.004241040907800198\n",
      "Epoch :0.23333333333333334    Train Loss :0.0018634423613548279    Test Loss :0.004084083717316389\n",
      "Epoch :0.24166666666666667    Train Loss :0.0018035380635410547    Test Loss :0.004158445168286562\n",
      "Epoch :0.25    Train Loss :0.0017950739711523056    Test Loss :0.0041578346863389015\n",
      "Epoch :0.25833333333333336    Train Loss :0.0017000888474285603    Test Loss :0.003765236586332321\n",
      "Epoch :0.26666666666666666    Train Loss :0.0016441511688753963    Test Loss :0.00402848469093442\n",
      "Epoch :0.275    Train Loss :0.001584082143381238    Test Loss :0.0036794578190892935\n",
      "Epoch :0.2833333333333333    Train Loss :0.0016856956062838435    Test Loss :0.0036394267808645964\n",
      "Epoch :0.2916666666666667    Train Loss :0.0015179823385551572    Test Loss :0.0037354561500251293\n",
      "Epoch :0.3    Train Loss :0.0014713280834257603    Test Loss :0.0036059736739844084\n",
      "Epoch :0.30833333333333335    Train Loss :0.0013553376775234938    Test Loss :0.003611915744841099\n",
      "Epoch :0.31666666666666665    Train Loss :0.0014192074304446578    Test Loss :0.0036272434517741203\n",
      "Epoch :0.325    Train Loss :0.0013790406519547105    Test Loss :0.003499106038361788\n",
      "Epoch :0.3333333333333333    Train Loss :0.0014590476639568806    Test Loss :0.0032368928659707308\n",
      "Epoch :0.3416666666666667    Train Loss :0.0013207055162638426    Test Loss :0.003997088875621557\n",
      "Epoch :0.35    Train Loss :0.001562641584314406    Test Loss :0.0035120719112455845\n",
      "Epoch :0.35833333333333334    Train Loss :0.0012099157320335507    Test Loss :0.0031856612768024206\n",
      "Epoch :0.36666666666666664    Train Loss :0.0012666888069361448    Test Loss :0.003157202620059252\n",
      "Epoch :0.375    Train Loss :0.0012505979975685477    Test Loss :0.003377557499334216\n",
      "Epoch :0.38333333333333336    Train Loss :0.0012690412113443017    Test Loss :0.003337897127494216\n",
      "Epoch :0.39166666666666666    Train Loss :0.0012897357810288668    Test Loss :0.0033505905885249376\n",
      "Epoch :0.4    Train Loss :0.0011365724494680762    Test Loss :0.003237552475184202\n",
      "Epoch :0.4083333333333333    Train Loss :0.0011786172399297357    Test Loss :0.0031310548074543476\n",
      "Epoch :0.4166666666666667    Train Loss :0.0011140011483803391    Test Loss :0.0032168589532375336\n",
      "Epoch :0.425    Train Loss :0.0011686988873407245    Test Loss :0.0031327754259109497\n",
      "Epoch :0.43333333333333335    Train Loss :0.0011066112201660872    Test Loss :0.003146373201161623\n",
      "Epoch :0.44166666666666665    Train Loss :0.0010839366586878896    Test Loss :0.0033206050284206867\n",
      "Epoch :0.45    Train Loss :0.0011841191444545984    Test Loss :0.0031528284307569265\n",
      "Epoch :0.4583333333333333    Train Loss :0.0011251983232796192    Test Loss :0.0029335771687328815\n",
      "Epoch :0.4666666666666667    Train Loss :0.0010917350882664323    Test Loss :0.0029150061309337616\n",
      "Epoch :0.475    Train Loss :0.0012696088524535298    Test Loss :0.003239742945879698\n",
      "Epoch :0.48333333333333334    Train Loss :0.001013919827528298    Test Loss :0.0033186308573931456\n",
      "Epoch :0.49166666666666664    Train Loss :0.0011674610432237387    Test Loss :0.003033335553482175\n",
      "Epoch :0.5    Train Loss :0.0011591322254389524    Test Loss :0.0028565397951751947\n",
      "Epoch :0.5083333333333333    Train Loss :0.0010336656123399734    Test Loss :0.003255225019529462\n",
      "Epoch :0.5166666666666667    Train Loss :0.0009969042148441076    Test Loss :0.0030026836320757866\n",
      "Epoch :0.525    Train Loss :0.0010509665589779615    Test Loss :0.0029159740079194307\n",
      "Epoch :0.5333333333333333    Train Loss :0.0011743901995941997    Test Loss :0.0029984782449901104\n",
      "Epoch :0.5416666666666666    Train Loss :0.0009935720590874553    Test Loss :0.0028422956820577383\n",
      "Epoch :0.55    Train Loss :0.0009998984169214964    Test Loss :0.0026888977736234665\n",
      "Epoch :0.5583333333333333    Train Loss :0.0009425245807506144    Test Loss :0.0027804977726191282\n",
      "Epoch :0.5666666666666667    Train Loss :0.0009750188328325748    Test Loss :0.0029335252474993467\n",
      "Epoch :0.575    Train Loss :0.0009706959826871753    Test Loss :0.002844699192792177\n",
      "Epoch :0.5833333333333334    Train Loss :0.0009092796826735139    Test Loss :0.002673541894182563\n",
      "Epoch :0.5916666666666667    Train Loss :0.0009180973866023123    Test Loss :0.0028186540585011244\n",
      "Epoch :0.6    Train Loss :0.0009379339753650129    Test Loss :0.0027880140114575624\n",
      "Epoch :0.6083333333333333    Train Loss :0.0009109181701205671    Test Loss :0.002870961558073759\n",
      "Epoch :0.6166666666666667    Train Loss :0.0008795776520855725    Test Loss :0.002815856831148267\n",
      "Epoch :0.625    Train Loss :0.0008721701451577246    Test Loss :0.002724012127146125\n",
      "Epoch :0.6333333333333333    Train Loss :0.0009023785241879523    Test Loss :0.002708073705434799\n",
      "Epoch :0.6416666666666667    Train Loss :0.0008475009235553443    Test Loss :0.0027690217830240726\n",
      "Epoch :0.65    Train Loss :0.0008905823342502117    Test Loss :0.0026325308717787266\n",
      "Epoch :0.6583333333333333    Train Loss :0.0008810950093902647    Test Loss :0.0024636792950332165\n",
      "Epoch :0.6666666666666666    Train Loss :0.0008240204770117998    Test Loss :0.002750889863818884\n",
      "Epoch :0.675    Train Loss :0.0008539130794815719    Test Loss :0.002616563346236944\n",
      "Epoch :0.6833333333333333    Train Loss :0.0007813948323018849    Test Loss :0.0026763055939227343\n",
      "Epoch :0.6916666666666667    Train Loss :0.0008165462058968842    Test Loss :0.002495135646313429\n",
      "Epoch :0.7    Train Loss :0.0008232428226619959    Test Loss :0.002474396489560604\n",
      "Epoch :0.7083333333333334    Train Loss :0.0009277373319491744    Test Loss :0.00266981590539217\n",
      "Epoch :0.7166666666666667    Train Loss :0.0009296250646002591    Test Loss :0.0026265366468578577\n",
      "Epoch :0.725    Train Loss :0.0009217331535182893    Test Loss :0.002489338628947735\n",
      "Epoch :0.7333333333333333    Train Loss :0.000869089737534523    Test Loss :0.002485662465915084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7416666666666667    Train Loss :0.0008463566191494465    Test Loss :0.0024592045228928328\n",
      "Epoch :0.75    Train Loss :0.0007645972655154765    Test Loss :0.0026282237377017736\n",
      "Epoch :0.7583333333333333    Train Loss :0.0007633130298927426    Test Loss :0.00258656800724566\n",
      "Epoch :0.7666666666666667    Train Loss :0.0007304609171114862    Test Loss :0.002541676862165332\n",
      "Epoch :0.775    Train Loss :0.0007368387887254357    Test Loss :0.0024200906045734882\n",
      "Epoch :0.7833333333333333    Train Loss :0.0007314389804378152    Test Loss :0.0025761197321116924\n",
      "Epoch :0.7916666666666666    Train Loss :0.0009546320070512593    Test Loss :0.0024942911695688963\n",
      "Epoch :0.8    Train Loss :0.000981617602519691    Test Loss :0.002886744448915124\n",
      "Epoch :0.8083333333333333    Train Loss :0.000840563967358321    Test Loss :0.0025241661351174116\n",
      "Epoch :0.8166666666666667    Train Loss :0.0006894651451148093    Test Loss :0.002467628102749586\n",
      "Epoch :0.825    Train Loss :0.0008125604945234954    Test Loss :0.002454459900036454\n",
      "Epoch :0.8333333333333334    Train Loss :0.000694886373821646    Test Loss :0.002412880305200815\n",
      "Epoch :0.8416666666666667    Train Loss :0.0007218066602945328    Test Loss :0.0022728207986801863\n",
      "Epoch :0.85    Train Loss :0.0006985586951486766    Test Loss :0.0023473382461816072\n",
      "Epoch :0.8583333333333333    Train Loss :0.0007244727457873523    Test Loss :0.002427409403026104\n",
      "Epoch :0.8666666666666667    Train Loss :0.0006994378054514527    Test Loss :0.0024824293795973063\n",
      "Epoch :0.875    Train Loss :0.0007078988710418344    Test Loss :0.00262878998182714\n",
      "Epoch :0.8833333333333333    Train Loss :0.0008318002801388502    Test Loss :0.0025108675472438335\n",
      "Epoch :0.8916666666666667    Train Loss :0.0007006294326856732    Test Loss :0.0023586461320519447\n",
      "Epoch :0.9    Train Loss :0.0006890411023050547    Test Loss :0.002271936973556876\n",
      "Epoch :0.9083333333333333    Train Loss :0.0006504749180749059    Test Loss :0.002219853922724724\n",
      "Epoch :0.9166666666666666    Train Loss :0.000688867992721498    Test Loss :0.00223181233741343\n",
      "Epoch :0.925    Train Loss :0.0006587716052308679    Test Loss :0.0023130211047828197\n",
      "Epoch :0.9333333333333333    Train Loss :0.0006665897672064602    Test Loss :0.0022816965356469154\n",
      "Epoch :0.9416666666666667    Train Loss :0.0006608549738302827    Test Loss :0.0020749520044773817\n",
      "Epoch :0.95    Train Loss :0.000648374785669148    Test Loss :0.0021915945690125227\n",
      "Epoch :0.9583333333333334    Train Loss :0.0006642598891630769    Test Loss :0.0022628449369221926\n",
      "Epoch :0.9666666666666667    Train Loss :0.0006324539426714182    Test Loss :0.0024230307899415493\n",
      "Epoch :0.975    Train Loss :0.0006033772369846702    Test Loss :0.0022229370661079884\n",
      "Epoch :0.9833333333333333    Train Loss :0.0006279179360717535    Test Loss :0.002245707204565406\n",
      "Epoch :0.9916666666666667    Train Loss :0.0006615730817429721    Test Loss :0.002248558448627591\n",
      "Epoch :1.0    Train Loss :0.000607935362495482    Test Loss :0.002298156963661313\n",
      "RMSE: 8.570587498276009\n",
      "MAE: 6.994362271566264\n",
      "MAPE: 6.308018134384205%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 50, 'lr': 0.01, 'num_layers': 5}\n",
      "total:  81.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.14276552200317383    Test Loss :0.4769653379917145\n",
      "Epoch :0.016666666666666666    Train Loss :0.10318977385759354    Test Loss :0.19177819788455963\n",
      "Epoch :0.025    Train Loss :0.05337146669626236    Test Loss :0.08868054300546646\n",
      "Epoch :0.03333333333333333    Train Loss :0.055109020322561264    Test Loss :0.16060957312583923\n",
      "Epoch :0.041666666666666664    Train Loss :0.04266439005732536    Test Loss :0.07977831363677979\n",
      "Epoch :0.05    Train Loss :0.04279918968677521    Test Loss :0.046107254922389984\n",
      "Epoch :0.058333333333333334    Train Loss :0.030345486477017403    Test Loss :0.060478873550891876\n",
      "Epoch :0.06666666666666667    Train Loss :0.016718678176403046    Test Loss :0.03666364774107933\n",
      "Epoch :0.075    Train Loss :0.0096382275223732    Test Loss :0.01804291643202305\n",
      "Epoch :0.08333333333333333    Train Loss :0.004856723360717297    Test Loss :0.012506362050771713\n",
      "Epoch :0.09166666666666666    Train Loss :0.004094116855412722    Test Loss :0.007152087055146694\n",
      "Epoch :0.1    Train Loss :0.002960037672892213    Test Loss :0.008417710661888123\n",
      "Epoch :0.10833333333333334    Train Loss :0.002020204206928611    Test Loss :0.005117905791848898\n",
      "Epoch :0.11666666666666667    Train Loss :0.0019256771774962544    Test Loss :0.003510508919134736\n",
      "Epoch :0.125    Train Loss :0.001805220264941454    Test Loss :0.0030847876332700253\n",
      "Epoch :0.13333333333333333    Train Loss :0.0017290371470153332    Test Loss :0.002899455837905407\n",
      "Epoch :0.14166666666666666    Train Loss :0.0016118655912578106    Test Loss :0.003657620633020997\n",
      "Epoch :0.15    Train Loss :0.0014618764398619533    Test Loss :0.004025517497211695\n",
      "Epoch :0.15833333333333333    Train Loss :0.0013977800263091922    Test Loss :0.0033899445552378893\n",
      "Epoch :0.16666666666666666    Train Loss :0.0013874396681785583    Test Loss :0.00274010025896132\n",
      "Epoch :0.175    Train Loss :0.0012477556010708213    Test Loss :0.0022903205826878548\n",
      "Epoch :0.18333333333333332    Train Loss :0.0012016668915748596    Test Loss :0.002326404210180044\n",
      "Epoch :0.19166666666666668    Train Loss :0.0010976613266393542    Test Loss :0.002073303796350956\n",
      "Epoch :0.2    Train Loss :0.0010317746782675385    Test Loss :0.0021291556768119335\n",
      "Epoch :0.20833333333333334    Train Loss :0.0010064083617180586    Test Loss :0.002108800457790494\n",
      "Epoch :0.21666666666666667    Train Loss :0.0009790821932256222    Test Loss :0.0019305419409647584\n",
      "Epoch :0.225    Train Loss :0.0009784771827980876    Test Loss :0.0018721523229032755\n",
      "Epoch :0.23333333333333334    Train Loss :0.001000391785055399    Test Loss :0.0020975542720407248\n",
      "Epoch :0.24166666666666667    Train Loss :0.0009224687819369137    Test Loss :0.0018303036922588944\n",
      "Epoch :0.25    Train Loss :0.0009204988018609583    Test Loss :0.0018725850386545062\n",
      "Epoch :0.25833333333333336    Train Loss :0.0008882285328581929    Test Loss :0.0018800506368279457\n",
      "Epoch :0.26666666666666666    Train Loss :0.0008212158572860062    Test Loss :0.001781633822247386\n",
      "Epoch :0.275    Train Loss :0.0008491738699376583    Test Loss :0.0018427653703838587\n",
      "Epoch :0.2833333333333333    Train Loss :0.0007908188854344189    Test Loss :0.0017935060895979404\n",
      "Epoch :0.2916666666666667    Train Loss :0.000835722719784826    Test Loss :0.0017873491160571575\n",
      "Epoch :0.3    Train Loss :0.0008197861607186496    Test Loss :0.0017427671700716019\n",
      "Epoch :0.30833333333333335    Train Loss :0.0007353669498115778    Test Loss :0.0015945330960676074\n",
      "Epoch :0.31666666666666665    Train Loss :0.000771828053984791    Test Loss :0.0017123179277405143\n",
      "Epoch :0.325    Train Loss :0.000763937656302005    Test Loss :0.0016314167296513915\n",
      "Epoch :0.3333333333333333    Train Loss :0.0007246755412779748    Test Loss :0.001546409330330789\n",
      "Epoch :0.3416666666666667    Train Loss :0.0007473376463167369    Test Loss :0.001560260308906436\n",
      "Epoch :0.35    Train Loss :0.0007348379003815353    Test Loss :0.0015022013103589416\n",
      "Epoch :0.35833333333333334    Train Loss :0.0006943514454178512    Test Loss :0.0015947251813486218\n",
      "Epoch :0.36666666666666664    Train Loss :0.0007197305094450712    Test Loss :0.0013760261936113238\n",
      "Epoch :0.375    Train Loss :0.0006860595312900841    Test Loss :0.0015505360206589103\n",
      "Epoch :0.38333333333333336    Train Loss :0.0006898933788761497    Test Loss :0.0015629997942596674\n",
      "Epoch :0.39166666666666666    Train Loss :0.0006483033648692071    Test Loss :0.0015425803139805794\n",
      "Epoch :0.4    Train Loss :0.0006848055636510253    Test Loss :0.0014017426874488592\n",
      "Epoch :0.4083333333333333    Train Loss :0.0006448974600061774    Test Loss :0.0015569909010082483\n",
      "Epoch :0.4166666666666667    Train Loss :0.0006547793745994568    Test Loss :0.0013767802156507969\n",
      "Epoch :0.425    Train Loss :0.0006601841887459159    Test Loss :0.0013755513355135918\n",
      "Epoch :0.43333333333333335    Train Loss :0.0006468908977694809    Test Loss :0.0013880209298804402\n",
      "Epoch :0.44166666666666665    Train Loss :0.0006441421573981643    Test Loss :0.001335660694167018\n",
      "Epoch :0.45    Train Loss :0.0006310825701802969    Test Loss :0.001377370790578425\n",
      "Epoch :0.4583333333333333    Train Loss :0.0006273340550251305    Test Loss :0.0013310756767168641\n",
      "Epoch :0.4666666666666667    Train Loss :0.0005839147488586605    Test Loss :0.0013740967260673642\n",
      "Epoch :0.475    Train Loss :0.0005985762691125274    Test Loss :0.0012482735328376293\n",
      "Epoch :0.48333333333333334    Train Loss :0.000651918351650238    Test Loss :0.0012615763116627932\n",
      "Epoch :0.49166666666666664    Train Loss :0.0006250196602195501    Test Loss :0.0012996516888961196\n",
      "Epoch :0.5    Train Loss :0.0005797073245048523    Test Loss :0.001180163468234241\n",
      "Epoch :0.5083333333333333    Train Loss :0.00061688048299402    Test Loss :0.001215500058606267\n",
      "Epoch :0.5166666666666667    Train Loss :0.0005765340756624937    Test Loss :0.0012363954447209835\n",
      "Epoch :0.525    Train Loss :0.0005999476998113096    Test Loss :0.0012006799224764109\n",
      "Epoch :0.5333333333333333    Train Loss :0.0005674073472619057    Test Loss :0.0013635727809742093\n",
      "Epoch :0.5416666666666666    Train Loss :0.0006243413663469255    Test Loss :0.0014838209608569741\n",
      "Epoch :0.55    Train Loss :0.0005895409267395735    Test Loss :0.0011683907359838486\n",
      "Epoch :0.5583333333333333    Train Loss :0.0005934881628490984    Test Loss :0.0011078936513513327\n",
      "Epoch :0.5666666666666667    Train Loss :0.0005806554690934718    Test Loss :0.0011151422513648868\n",
      "Epoch :0.575    Train Loss :0.0005517668905667961    Test Loss :0.001256858347915113\n",
      "Epoch :0.5833333333333334    Train Loss :0.0005743570509366691    Test Loss :0.0012070058146491647\n",
      "Epoch :0.5916666666666667    Train Loss :0.0005817580968141556    Test Loss :0.0011257033329457045\n",
      "Epoch :0.6    Train Loss :0.0005596739356406033    Test Loss :0.0010942963417619467\n",
      "Epoch :0.6083333333333333    Train Loss :0.0005540723213925958    Test Loss :0.0011074752546846867\n",
      "Epoch :0.6166666666666667    Train Loss :0.0005613779067061841    Test Loss :0.0012231095461174846\n",
      "Epoch :0.625    Train Loss :0.0005455363425426185    Test Loss :0.001097887521609664\n",
      "Epoch :0.6333333333333333    Train Loss :0.0005383130628615618    Test Loss :0.0010346896015107632\n",
      "Epoch :0.6416666666666667    Train Loss :0.0005262579070404172    Test Loss :0.0009956848807632923\n",
      "Epoch :0.65    Train Loss :0.0005254652351140976    Test Loss :0.0010883299401029944\n",
      "Epoch :0.6583333333333333    Train Loss :0.0005163178429938853    Test Loss :0.000999341020360589\n",
      "Epoch :0.6666666666666666    Train Loss :0.0005211791140027344    Test Loss :0.0010844890493899584\n",
      "Epoch :0.675    Train Loss :0.0005243568448349833    Test Loss :0.0010575507767498493\n",
      "Epoch :0.6833333333333333    Train Loss :0.0005126484320499003    Test Loss :0.0010462934151291847\n",
      "Epoch :0.6916666666666667    Train Loss :0.0005026413127779961    Test Loss :0.0011122174328193069\n",
      "Epoch :0.7    Train Loss :0.000495649641379714    Test Loss :0.0010171022731810808\n",
      "Epoch :0.7083333333333334    Train Loss :0.00048267177771776915    Test Loss :0.0009556976146996021\n",
      "Epoch :0.7166666666666667    Train Loss :0.0005152150988578796    Test Loss :0.001039218856021762\n",
      "Epoch :0.725    Train Loss :0.0004792960244230926    Test Loss :0.0009824142325669527\n",
      "Epoch :0.7333333333333333    Train Loss :0.0004936783225275576    Test Loss :0.0009576310403645039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7416666666666667    Train Loss :0.0008107290486805141    Test Loss :0.0019539070781320333\n",
      "Epoch :0.75    Train Loss :0.0004806755168829113    Test Loss :0.0014591289218515158\n",
      "Epoch :0.7583333333333333    Train Loss :0.000600548810325563    Test Loss :0.0015522497706115246\n",
      "Epoch :0.7666666666666667    Train Loss :0.00047360596363432705    Test Loss :0.0010513912420719862\n",
      "Epoch :0.775    Train Loss :0.0005272112903185189    Test Loss :0.0009696031920611858\n",
      "Epoch :0.7833333333333333    Train Loss :0.0004558902292046696    Test Loss :0.0010691822972148657\n",
      "Epoch :0.7916666666666666    Train Loss :0.0005027518491260707    Test Loss :0.0010746554471552372\n",
      "Epoch :0.8    Train Loss :0.0004559361841529608    Test Loss :0.0009324203711003065\n",
      "Epoch :0.8083333333333333    Train Loss :0.0005034825881011784    Test Loss :0.0009330863831564784\n",
      "Epoch :0.8166666666666667    Train Loss :0.000490096805151552    Test Loss :0.0008860043017193675\n",
      "Epoch :0.825    Train Loss :0.0004644008877221495    Test Loss :0.0009105313802137971\n",
      "Epoch :0.8333333333333334    Train Loss :0.0004374106356408447    Test Loss :0.0008926693699322641\n",
      "Epoch :0.8416666666666667    Train Loss :0.0004359578888397664    Test Loss :0.0008988708141259849\n",
      "Epoch :0.85    Train Loss :0.00043163332156836987    Test Loss :0.0009019635035656393\n",
      "Epoch :0.8583333333333333    Train Loss :0.0004635167133528739    Test Loss :0.0008715924341231585\n",
      "Epoch :0.8666666666666667    Train Loss :0.00042320098145864904    Test Loss :0.0009707177523523569\n",
      "Epoch :0.875    Train Loss :0.00045664384379051626    Test Loss :0.0009230855503119528\n",
      "Epoch :0.8833333333333333    Train Loss :0.0004494983877521008    Test Loss :0.0008856700733304024\n",
      "Epoch :0.8916666666666667    Train Loss :0.0003934644046239555    Test Loss :0.0007674210937693715\n",
      "Epoch :0.9    Train Loss :0.0004773025866597891    Test Loss :0.0009231048170477152\n",
      "Epoch :0.9083333333333333    Train Loss :0.0004295922990422696    Test Loss :0.0007649883045814931\n",
      "Epoch :0.9166666666666666    Train Loss :0.000418912066379562    Test Loss :0.0007697796681895852\n",
      "Epoch :0.925    Train Loss :0.0005395108019001782    Test Loss :0.0011342419311404228\n",
      "Epoch :0.9333333333333333    Train Loss :0.0005507873720489442    Test Loss :0.0008175643160939217\n",
      "Epoch :0.9416666666666667    Train Loss :0.0004112333117518574    Test Loss :0.0008199069416150451\n",
      "Epoch :0.95    Train Loss :0.00041615572990849614    Test Loss :0.0008316712337546051\n",
      "Epoch :0.9583333333333334    Train Loss :0.00040849155629985034    Test Loss :0.0008215665584430099\n",
      "Epoch :0.9666666666666667    Train Loss :0.0004195721121504903    Test Loss :0.0007470448035746813\n",
      "Epoch :0.975    Train Loss :0.0004261743451934308    Test Loss :0.0008114808006212115\n",
      "Epoch :0.9833333333333333    Train Loss :0.00042098070844076574    Test Loss :0.0008081240812316537\n",
      "Epoch :0.9916666666666667    Train Loss :0.0004506428958848119    Test Loss :0.000740524148568511\n",
      "Epoch :1.0    Train Loss :0.00039407919393852353    Test Loss :0.0007082836236804724\n",
      "RMSE: 8.014520485176137\n",
      "MAE: 6.546478951148611\n",
      "MAPE: 5.8978400334916605%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 50, 'lr': 0.02, 'num_layers': 2}\n",
      "total:  84.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.0811508372426033    Test Loss :0.3554096817970276\n",
      "Epoch :0.016666666666666666    Train Loss :0.07161151617765427    Test Loss :0.126125305891037\n",
      "Epoch :0.025    Train Loss :0.0498194582760334    Test Loss :0.12949122488498688\n",
      "Epoch :0.03333333333333333    Train Loss :0.050935205072164536    Test Loss :0.11556283384561539\n",
      "Epoch :0.041666666666666664    Train Loss :0.050503943115472794    Test Loss :0.07648703455924988\n",
      "Epoch :0.05    Train Loss :0.048134807497262955    Test Loss :0.1127125471830368\n",
      "Epoch :0.058333333333333334    Train Loss :0.04177367687225342    Test Loss :0.0901157557964325\n",
      "Epoch :0.06666666666666667    Train Loss :0.027155181393027306    Test Loss :0.04795576259493828\n",
      "Epoch :0.075    Train Loss :0.017299484461545944    Test Loss :0.04647665098309517\n",
      "Epoch :0.08333333333333333    Train Loss :0.014426338486373425    Test Loss :0.027545055374503136\n",
      "Epoch :0.09166666666666666    Train Loss :0.011522308923304081    Test Loss :0.0173934493213892\n",
      "Epoch :0.1    Train Loss :0.00940263457596302    Test Loss :0.011966889724135399\n",
      "Epoch :0.10833333333333334    Train Loss :0.007150528021156788    Test Loss :0.009706852957606316\n",
      "Epoch :0.11666666666666667    Train Loss :0.005991136189550161    Test Loss :0.010258814319968224\n",
      "Epoch :0.125    Train Loss :0.005326713901013136    Test Loss :0.008784192614257336\n",
      "Epoch :0.13333333333333333    Train Loss :0.004789371974766254    Test Loss :0.008141020312905312\n",
      "Epoch :0.14166666666666666    Train Loss :0.004456694703549147    Test Loss :0.007222665473818779\n",
      "Epoch :0.15    Train Loss :0.0041471305303275585    Test Loss :0.007629022467881441\n",
      "Epoch :0.15833333333333333    Train Loss :0.0038402520585805178    Test Loss :0.007408132776618004\n",
      "Epoch :0.16666666666666666    Train Loss :0.003476071869954467    Test Loss :0.0067702122032642365\n",
      "Epoch :0.175    Train Loss :0.003292989917099476    Test Loss :0.006073351483792067\n",
      "Epoch :0.18333333333333332    Train Loss :0.003063506679609418    Test Loss :0.006141413934528828\n",
      "Epoch :0.19166666666666668    Train Loss :0.0031279076356440783    Test Loss :0.006444454193115234\n",
      "Epoch :0.2    Train Loss :0.002881098771467805    Test Loss :0.005369907710701227\n",
      "Epoch :0.20833333333333334    Train Loss :0.0029222879093140364    Test Loss :0.005432968493551016\n",
      "Epoch :0.21666666666666667    Train Loss :0.002909124130383134    Test Loss :0.005867917090654373\n",
      "Epoch :0.225    Train Loss :0.002449922962114215    Test Loss :0.0052446941845119\n",
      "Epoch :0.23333333333333334    Train Loss :0.002320536645129323    Test Loss :0.00525661651045084\n",
      "Epoch :0.24166666666666667    Train Loss :0.0023166800383478403    Test Loss :0.005200450774282217\n",
      "Epoch :0.25    Train Loss :0.002263478236272931    Test Loss :0.004718288779258728\n",
      "Epoch :0.25833333333333336    Train Loss :0.0022750592324882746    Test Loss :0.005060297437012196\n",
      "Epoch :0.26666666666666666    Train Loss :0.0020652974490076303    Test Loss :0.004611297510564327\n",
      "Epoch :0.275    Train Loss :0.0022274726070463657    Test Loss :0.00507772620767355\n",
      "Epoch :0.2833333333333333    Train Loss :0.0020324639044702053    Test Loss :0.004185048397630453\n",
      "Epoch :0.2916666666666667    Train Loss :0.0020768579561263323    Test Loss :0.004638281185179949\n",
      "Epoch :0.3    Train Loss :0.001916771288961172    Test Loss :0.004401440732181072\n",
      "Epoch :0.30833333333333335    Train Loss :0.0018534670816734433    Test Loss :0.004463633522391319\n",
      "Epoch :0.31666666666666665    Train Loss :0.0017449668375775218    Test Loss :0.004226041492074728\n",
      "Epoch :0.325    Train Loss :0.0017269984818995    Test Loss :0.004469961393624544\n",
      "Epoch :0.3333333333333333    Train Loss :0.0018231680151075125    Test Loss :0.004164900165051222\n",
      "Epoch :0.3416666666666667    Train Loss :0.0017569396877661347    Test Loss :0.0045291525311768055\n",
      "Epoch :0.35    Train Loss :0.0016539155039936304    Test Loss :0.003855288727208972\n",
      "Epoch :0.35833333333333334    Train Loss :0.0016819203738123178    Test Loss :0.0038875851314514875\n",
      "Epoch :0.36666666666666664    Train Loss :0.0015748030273243785    Test Loss :0.0038539189845323563\n",
      "Epoch :0.375    Train Loss :0.0015902372542768717    Test Loss :0.003865721169859171\n",
      "Epoch :0.38333333333333336    Train Loss :0.0015385301085188985    Test Loss :0.003920190501958132\n",
      "Epoch :0.39166666666666666    Train Loss :0.0014134388184174895    Test Loss :0.003730651456862688\n",
      "Epoch :0.4    Train Loss :0.0015228328993543983    Test Loss :0.0040123495273292065\n",
      "Epoch :0.4083333333333333    Train Loss :0.0016622256953269243    Test Loss :0.004535621963441372\n",
      "Epoch :0.4166666666666667    Train Loss :0.0017122698482125998    Test Loss :0.0035996222868561745\n",
      "Epoch :0.425    Train Loss :0.0014808847336098552    Test Loss :0.0038254514802247286\n",
      "Epoch :0.43333333333333335    Train Loss :0.0014308214886114001    Test Loss :0.004032120108604431\n",
      "Epoch :0.44166666666666665    Train Loss :0.0013792130630463362    Test Loss :0.0032270627561956644\n",
      "Epoch :0.45    Train Loss :0.001447006594389677    Test Loss :0.003933499101549387\n",
      "Epoch :0.4583333333333333    Train Loss :0.001282476936466992    Test Loss :0.0037357714027166367\n",
      "Epoch :0.4666666666666667    Train Loss :0.0015157778980210423    Test Loss :0.003739813109859824\n",
      "Epoch :0.475    Train Loss :0.0016325608594343066    Test Loss :0.003919266164302826\n",
      "Epoch :0.48333333333333334    Train Loss :0.0012340728426352143    Test Loss :0.0040052663534879684\n",
      "Epoch :0.49166666666666664    Train Loss :0.0020412581507116556    Test Loss :0.0034810376819223166\n",
      "Epoch :0.5    Train Loss :0.0013674027286469936    Test Loss :0.0042184945195913315\n",
      "Epoch :0.5083333333333333    Train Loss :0.0013538378989323974    Test Loss :0.003766947891563177\n",
      "Epoch :0.5166666666666667    Train Loss :0.0012257278431206942    Test Loss :0.0037424173206090927\n",
      "Epoch :0.525    Train Loss :0.0012049672659486532    Test Loss :0.003929374739527702\n",
      "Epoch :0.5333333333333333    Train Loss :0.0011823616223409772    Test Loss :0.0034960589837282896\n",
      "Epoch :0.5416666666666666    Train Loss :0.0012066466733813286    Test Loss :0.0033514557871967554\n",
      "Epoch :0.55    Train Loss :0.0011488733580335975    Test Loss :0.0032933326438069344\n",
      "Epoch :0.5583333333333333    Train Loss :0.0010952312732115388    Test Loss :0.003594218986108899\n",
      "Epoch :0.5666666666666667    Train Loss :0.001191832241602242    Test Loss :0.0035248477943241596\n",
      "Epoch :0.575    Train Loss :0.0011285129003226757    Test Loss :0.00349501078017056\n",
      "Epoch :0.5833333333333334    Train Loss :0.0012729568406939507    Test Loss :0.0035907926503568888\n",
      "Epoch :0.5916666666666667    Train Loss :0.001134743681177497    Test Loss :0.0035279521252959967\n",
      "Epoch :0.6    Train Loss :0.0011131211649626493    Test Loss :0.003384130774065852\n",
      "Epoch :0.6083333333333333    Train Loss :0.0012585582444444299    Test Loss :0.00352483126334846\n",
      "Epoch :0.6166666666666667    Train Loss :0.0011296267621219158    Test Loss :0.003778742626309395\n",
      "Epoch :0.625    Train Loss :0.0010000995825976133    Test Loss :0.0032952178735285997\n",
      "Epoch :0.6333333333333333    Train Loss :0.000987059436738491    Test Loss :0.003434925340116024\n",
      "Epoch :0.6416666666666667    Train Loss :0.0011849318398162723    Test Loss :0.0033431020565330982\n",
      "Epoch :0.65    Train Loss :0.001015947898849845    Test Loss :0.0033444524742662907\n",
      "Epoch :0.6583333333333333    Train Loss :0.001065016258507967    Test Loss :0.0032437206245958805\n",
      "Epoch :0.6666666666666666    Train Loss :0.0010190567700192332    Test Loss :0.0032054567709565163\n",
      "Epoch :0.675    Train Loss :0.0010870251571759582    Test Loss :0.003497530473396182\n",
      "Epoch :0.6833333333333333    Train Loss :0.0010110720759257674    Test Loss :0.0034553310833871365\n",
      "Epoch :0.6916666666666667    Train Loss :0.0009400454582646489    Test Loss :0.0031223567202687263\n",
      "Epoch :0.7    Train Loss :0.0010501202195882797    Test Loss :0.003958362620323896\n",
      "Epoch :0.7083333333333334    Train Loss :0.0009781791595742106    Test Loss :0.0030410930048674345\n",
      "Epoch :0.7166666666666667    Train Loss :0.0009782215347513556    Test Loss :0.0033612102270126343\n",
      "Epoch :0.725    Train Loss :0.002657345263287425    Test Loss :0.005877453368157148\n",
      "Epoch :0.7333333333333333    Train Loss :0.0027738860808312893    Test Loss :0.004451948683708906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7416666666666667    Train Loss :0.0015324351843446493    Test Loss :0.004860890097916126\n",
      "Epoch :0.75    Train Loss :0.0011091253254562616    Test Loss :0.004895210266113281\n",
      "Epoch :0.7583333333333333    Train Loss :0.001051677973009646    Test Loss :0.003901472082361579\n",
      "Epoch :0.7666666666666667    Train Loss :0.0010174816707149148    Test Loss :0.004933045245707035\n",
      "Epoch :0.775    Train Loss :0.0009321208344772458    Test Loss :0.0039007230661809444\n",
      "Epoch :0.7833333333333333    Train Loss :0.0009285208070650697    Test Loss :0.005654003471136093\n",
      "Epoch :0.7916666666666666    Train Loss :0.0009508958319202065    Test Loss :0.003623205004259944\n",
      "Epoch :0.8    Train Loss :0.0010014987783506513    Test Loss :0.004321782384067774\n",
      "Epoch :0.8083333333333333    Train Loss :0.0009892627131193876    Test Loss :0.003986613359302282\n",
      "Epoch :0.8166666666666667    Train Loss :0.000874156248755753    Test Loss :0.0038865217939019203\n",
      "Epoch :0.825    Train Loss :0.0008691838011145592    Test Loss :0.003940236289054155\n",
      "Epoch :0.8333333333333334    Train Loss :0.0008417066419497132    Test Loss :0.0041791098192334175\n",
      "Epoch :0.8416666666666667    Train Loss :0.0008772876462899148    Test Loss :0.003094359068199992\n",
      "Epoch :0.85    Train Loss :0.0008237900328822434    Test Loss :0.004253997467458248\n",
      "Epoch :0.8583333333333333    Train Loss :0.0009486337658017874    Test Loss :0.004337646998465061\n",
      "Epoch :0.8666666666666667    Train Loss :0.000855006801430136    Test Loss :0.005094138439744711\n",
      "Epoch :0.875    Train Loss :0.0008697118028067052    Test Loss :0.003997781779617071\n",
      "Epoch :0.8833333333333333    Train Loss :0.0008833011379465461    Test Loss :0.004153289832174778\n",
      "Epoch :0.8916666666666667    Train Loss :0.00077590043656528    Test Loss :0.0036981021985411644\n",
      "Epoch :0.9    Train Loss :0.0007894698064774275    Test Loss :0.00521509675309062\n",
      "Epoch :0.9083333333333333    Train Loss :0.0009508445509709418    Test Loss :0.003621149342507124\n",
      "Epoch :0.9166666666666666    Train Loss :0.0008376790792681277    Test Loss :0.004345947410911322\n",
      "Epoch :0.925    Train Loss :0.0008420079830102623    Test Loss :0.005211506970226765\n",
      "Epoch :0.9333333333333333    Train Loss :0.0008162761223502457    Test Loss :0.004352249670773745\n",
      "Epoch :0.9416666666666667    Train Loss :0.0008533431682735682    Test Loss :0.003746161935850978\n",
      "Epoch :0.95    Train Loss :0.0007727623451501131    Test Loss :0.00446971133351326\n",
      "Epoch :0.9583333333333334    Train Loss :0.0008183916215784848    Test Loss :0.003950797952711582\n",
      "Epoch :0.9666666666666667    Train Loss :0.000964007806032896    Test Loss :0.00555223785340786\n",
      "Epoch :0.975    Train Loss :0.0008269976824522018    Test Loss :0.00736886914819479\n",
      "Epoch :0.9833333333333333    Train Loss :0.0009881997248157859    Test Loss :0.006327762734144926\n",
      "Epoch :0.9916666666666667    Train Loss :0.000798707886133343    Test Loss :0.0037651543971151114\n",
      "Epoch :1.0    Train Loss :0.0007845746586099267    Test Loss :0.004671016242355108\n",
      "RMSE: 8.784924093496352\n",
      "MAE: 7.188452477957196\n",
      "MAPE: 6.521110601317832%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 50, 'lr': 0.02, 'num_layers': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  88.0\n",
      "Epoch :0.008333333333333333    Train Loss :0.11886792629957199    Test Loss :0.3139630854129791\n",
      "Epoch :0.016666666666666666    Train Loss :0.045873161405324936    Test Loss :0.014814103953540325\n",
      "Epoch :0.025    Train Loss :0.04086138308048248    Test Loss :0.10939578711986542\n",
      "Epoch :0.03333333333333333    Train Loss :0.016970349475741386    Test Loss :0.023752879351377487\n",
      "Epoch :0.041666666666666664    Train Loss :0.01985321193933487    Test Loss :0.010717128403484821\n",
      "Epoch :0.05    Train Loss :0.004915380384773016    Test Loss :0.006430686917155981\n",
      "Epoch :0.058333333333333334    Train Loss :0.004727572202682495    Test Loss :0.01183787640184164\n",
      "Epoch :0.06666666666666667    Train Loss :0.004352397285401821    Test Loss :0.005171068944036961\n",
      "Epoch :0.075    Train Loss :0.002336406148970127    Test Loss :0.0037937858141958714\n",
      "Epoch :0.08333333333333333    Train Loss :0.0015900835860520601    Test Loss :0.005749205593019724\n",
      "Epoch :0.09166666666666666    Train Loss :0.001800324534997344    Test Loss :0.0030201964545994997\n",
      "Epoch :0.1    Train Loss :0.0014802159275859594    Test Loss :0.002372360322624445\n",
      "Epoch :0.10833333333333334    Train Loss :0.0011572749353945255    Test Loss :0.0028728318866342306\n",
      "Epoch :0.11666666666666667    Train Loss :0.0012317020446062088    Test Loss :0.0020356932654976845\n",
      "Epoch :0.125    Train Loss :0.0010138845536857843    Test Loss :0.0018983983900398016\n",
      "Epoch :0.13333333333333333    Train Loss :0.0010657538659870625    Test Loss :0.00214115041308105\n",
      "Epoch :0.14166666666666666    Train Loss :0.000945019070059061    Test Loss :0.0019755978137254715\n",
      "Epoch :0.15    Train Loss :0.0009709819569252431    Test Loss :0.0016029725084081292\n",
      "Epoch :0.15833333333333333    Train Loss :0.0008711706032045186    Test Loss :0.0014954650541767478\n",
      "Epoch :0.16666666666666666    Train Loss :0.0008267193334177136    Test Loss :0.0014207876520231366\n",
      "Epoch :0.175    Train Loss :0.0007861574413254857    Test Loss :0.001529577188193798\n",
      "Epoch :0.18333333333333332    Train Loss :0.0007629708270542324    Test Loss :0.0014232678804546595\n",
      "Epoch :0.19166666666666668    Train Loss :0.0007544664549641311    Test Loss :0.0014053374761715531\n",
      "Epoch :0.2    Train Loss :0.0007283437880687416    Test Loss :0.00145173748023808\n",
      "Epoch :0.20833333333333334    Train Loss :0.0007014211732894182    Test Loss :0.0013846424408257008\n",
      "Epoch :0.21666666666666667    Train Loss :0.0007365753990598023    Test Loss :0.0013199998065829277\n",
      "Epoch :0.225    Train Loss :0.0006693583563901484    Test Loss :0.0012768306769430637\n",
      "Epoch :0.23333333333333334    Train Loss :0.0006321055698208511    Test Loss :0.0012604601215571165\n",
      "Epoch :0.24166666666666667    Train Loss :0.0006024070316925645    Test Loss :0.0012588120298460126\n",
      "Epoch :0.25    Train Loss :0.0006279164226725698    Test Loss :0.001157352700829506\n",
      "Epoch :0.25833333333333336    Train Loss :0.0006020780419930816    Test Loss :0.0012509283842518926\n",
      "Epoch :0.26666666666666666    Train Loss :0.0005946746678091586    Test Loss :0.001138909487053752\n",
      "Epoch :0.275    Train Loss :0.0005563782178796828    Test Loss :0.0013085086829960346\n",
      "Epoch :0.2833333333333333    Train Loss :0.0005574966780841351    Test Loss :0.0010617183288559318\n",
      "Epoch :0.2916666666666667    Train Loss :0.0005686252843588591    Test Loss :0.0011985973687842488\n",
      "Epoch :0.3    Train Loss :0.0005500722909346223    Test Loss :0.0011476220097392797\n",
      "Epoch :0.30833333333333335    Train Loss :0.0005522743449546397    Test Loss :0.0010876066517084837\n",
      "Epoch :0.31666666666666665    Train Loss :0.0005618438008241355    Test Loss :0.0011057108640670776\n",
      "Epoch :0.325    Train Loss :0.0005308114341460168    Test Loss :0.0011075204238295555\n",
      "Epoch :0.3333333333333333    Train Loss :0.0005257016746327281    Test Loss :0.0010098478524014354\n",
      "Epoch :0.3416666666666667    Train Loss :0.0005221387255005538    Test Loss :0.0011124376906082034\n",
      "Epoch :0.35    Train Loss :0.0005272174603305757    Test Loss :0.0010501209180802107\n",
      "Epoch :0.35833333333333334    Train Loss :0.00054259441094473    Test Loss :0.001048414153046906\n",
      "Epoch :0.36666666666666664    Train Loss :0.0005797416670247912    Test Loss :0.0009940052404999733\n",
      "Epoch :0.375    Train Loss :0.0006762669654563069    Test Loss :0.0010081076761707664\n",
      "Epoch :0.38333333333333336    Train Loss :0.0005118882982060313    Test Loss :0.0011497640516608953\n",
      "Epoch :0.39166666666666666    Train Loss :0.0005169950891286135    Test Loss :0.0010553469182923436\n",
      "Epoch :0.4    Train Loss :0.0005105125601403415    Test Loss :0.0010309198405593634\n",
      "Epoch :0.4083333333333333    Train Loss :0.0004972430760972202    Test Loss :0.0009787892922759056\n",
      "Epoch :0.4166666666666667    Train Loss :0.00047395628644153476    Test Loss :0.000912312010768801\n",
      "Epoch :0.425    Train Loss :0.0004770984232891351    Test Loss :0.0009282850078307092\n",
      "Epoch :0.43333333333333335    Train Loss :0.0004687779291998595    Test Loss :0.00105481396894902\n",
      "Epoch :0.44166666666666665    Train Loss :0.00048789966967888176    Test Loss :0.0009737833752296865\n",
      "Epoch :0.45    Train Loss :0.0005339424242265522    Test Loss :0.0009033940732479095\n",
      "Epoch :0.4583333333333333    Train Loss :0.0004915411118417978    Test Loss :0.0009600595803931355\n",
      "Epoch :0.4666666666666667    Train Loss :0.0005037266528233886    Test Loss :0.0009805960580706596\n",
      "Epoch :0.475    Train Loss :0.0006249969592317939    Test Loss :0.0010197889059782028\n",
      "Epoch :0.48333333333333334    Train Loss :0.0005210475064814091    Test Loss :0.000975781527813524\n",
      "Epoch :0.49166666666666664    Train Loss :0.0005564250750467181    Test Loss :0.0010254167718812823\n",
      "Epoch :0.5    Train Loss :0.000494052772410214    Test Loss :0.0009176101302728057\n",
      "Epoch :0.5083333333333333    Train Loss :0.0005107845063321292    Test Loss :0.0008969538030214608\n",
      "Epoch :0.5166666666666667    Train Loss :0.0004714967799372971    Test Loss :0.0009168880642391741\n",
      "Epoch :0.525    Train Loss :0.0004435379814822227    Test Loss :0.0009982663905248046\n",
      "Epoch :0.5333333333333333    Train Loss :0.0004559525696095079    Test Loss :0.0008994113886728883\n",
      "Epoch :0.5416666666666666    Train Loss :0.00046177153126336634    Test Loss :0.0010810061357915401\n",
      "Epoch :0.55    Train Loss :0.0004342279280535877    Test Loss :0.0009991463739424944\n",
      "Epoch :0.5583333333333333    Train Loss :0.000478829606436193    Test Loss :0.0011067477753385901\n",
      "Epoch :0.5666666666666667    Train Loss :0.0004425556107889861    Test Loss :0.0012372327037155628\n",
      "Epoch :0.575    Train Loss :0.00042686040978878736    Test Loss :0.0009550484828650951\n",
      "Epoch :0.5833333333333334    Train Loss :0.0004955599433742464    Test Loss :0.00089702766854316\n",
      "Epoch :0.5916666666666667    Train Loss :0.0004480759962461889    Test Loss :0.0008371143485419452\n",
      "Epoch :0.6    Train Loss :0.00042719539487734437    Test Loss :0.0009563724743202329\n",
      "Epoch :0.6083333333333333    Train Loss :0.00043525229557417333    Test Loss :0.001080840127542615\n",
      "Epoch :0.6166666666666667    Train Loss :0.0004217925597913563    Test Loss :0.0008602357702329755\n",
      "Epoch :0.625    Train Loss :0.0004612009215634316    Test Loss :0.0007948834681883454\n",
      "Epoch :0.6333333333333333    Train Loss :0.00043614167952910066    Test Loss :0.0008849005680531263\n",
      "Epoch :0.6416666666666667    Train Loss :0.0005468296585604548    Test Loss :0.0009544377098791301\n",
      "Epoch :0.65    Train Loss :0.0004877280443906784    Test Loss :0.000909679860342294\n",
      "Epoch :0.6583333333333333    Train Loss :0.0004906858084723353    Test Loss :0.0009206078830175102\n",
      "Epoch :0.6666666666666666    Train Loss :0.00041579027310945094    Test Loss :0.0008342103683389723\n",
      "Epoch :0.675    Train Loss :0.00040540695772506297    Test Loss :0.0008661379106342793\n",
      "Epoch :0.6833333333333333    Train Loss :0.00044168083695694804    Test Loss :0.0009246408590115607\n",
      "Epoch :0.6916666666666667    Train Loss :0.00047049522981978953    Test Loss :0.0007858718745410442\n",
      "Epoch :0.7    Train Loss :0.0004936866462230682    Test Loss :0.0007930383435450494\n",
      "Epoch :0.7083333333333334    Train Loss :0.0005170247750356793    Test Loss :0.0007988564902916551\n",
      "Epoch :0.7166666666666667    Train Loss :0.0004725455946754664    Test Loss :0.0007846514345146716\n",
      "Epoch :0.725    Train Loss :0.0005131572252139449    Test Loss :0.0008187390631064773\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7333333333333333    Train Loss :0.0004582199326250702    Test Loss :0.000798384309746325\n",
      "Epoch :0.7416666666666667    Train Loss :0.00039652545819990337    Test Loss :0.0009033566457219422\n",
      "Epoch :0.75    Train Loss :0.00038029049756005406    Test Loss :0.0008482391131110489\n",
      "Epoch :0.7583333333333333    Train Loss :0.0004080423968844116    Test Loss :0.0009104142081923783\n",
      "Epoch :0.7666666666666667    Train Loss :0.0004219259135425091    Test Loss :0.0010481628123670816\n",
      "Epoch :0.775    Train Loss :0.00042528839549049735    Test Loss :0.0009552512783557177\n",
      "Epoch :0.7833333333333333    Train Loss :0.00044651300413534045    Test Loss :0.000714364112354815\n",
      "Epoch :0.7916666666666666    Train Loss :0.0003664778487291187    Test Loss :0.0007565769483335316\n",
      "Epoch :0.8    Train Loss :0.0003751477343030274    Test Loss :0.0007274395320564508\n",
      "Epoch :0.8083333333333333    Train Loss :0.0003512185940053314    Test Loss :0.0008276661392301321\n",
      "Epoch :0.8166666666666667    Train Loss :0.0003850935027003288    Test Loss :0.0008595514227636158\n",
      "Epoch :0.825    Train Loss :0.000773085979744792    Test Loss :0.0014478836674243212\n",
      "Epoch :0.8333333333333334    Train Loss :0.0003851348883472383    Test Loss :0.0008261615876108408\n",
      "Epoch :0.8416666666666667    Train Loss :0.0004789657541550696    Test Loss :0.0007676912937313318\n",
      "Epoch :0.85    Train Loss :0.0003926046483684331    Test Loss :0.0009282060200348496\n",
      "Epoch :0.8583333333333333    Train Loss :0.00037577273906208575    Test Loss :0.0008793066954240203\n",
      "Epoch :0.8666666666666667    Train Loss :0.0003890163206961006    Test Loss :0.0008035984355956316\n",
      "Epoch :0.875    Train Loss :0.0004006673989351839    Test Loss :0.0007970784790813923\n",
      "Epoch :0.8833333333333333    Train Loss :0.0003997339808847755    Test Loss :0.0007732542580924928\n",
      "Epoch :0.8916666666666667    Train Loss :0.0003683892427943647    Test Loss :0.0006707530119456351\n",
      "Epoch :0.9    Train Loss :0.0004270613135304302    Test Loss :0.00074026855872944\n",
      "Epoch :0.9083333333333333    Train Loss :0.0004119068617001176    Test Loss :0.0007716252584941685\n",
      "Epoch :0.9166666666666666    Train Loss :0.00038241513539105654    Test Loss :0.000753596774302423\n",
      "Epoch :0.925    Train Loss :0.0003514835552778095    Test Loss :0.0007507750997319818\n",
      "Epoch :0.9333333333333333    Train Loss :0.0003894497931469232    Test Loss :0.0008477953379042447\n",
      "Epoch :0.9416666666666667    Train Loss :0.0003667780547402799    Test Loss :0.0009006711770780385\n",
      "Epoch :0.95    Train Loss :0.0003520285536069423    Test Loss :0.0007002991042099893\n",
      "Epoch :0.9583333333333334    Train Loss :0.0004111535381525755    Test Loss :0.001017871662043035\n",
      "Epoch :0.9666666666666667    Train Loss :0.0003453439276199788    Test Loss :0.000788078352343291\n",
      "Epoch :0.975    Train Loss :0.0004036652680952102    Test Loss :0.0007552684401161969\n",
      "Epoch :0.9833333333333333    Train Loss :0.00034401865559630096    Test Loss :0.0007554771145805717\n",
      "Epoch :0.9916666666666667    Train Loss :0.000378115801140666    Test Loss :0.0008400640217587352\n",
      "Epoch :1.0    Train Loss :0.0003374932857695967    Test Loss :0.0007692868821322918\n",
      "RMSE: 9.495358507049431\n",
      "MAE: 7.850399748046121\n",
      "MAPE: 6.692793165328117%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 100, 'lr': 0.01, 'num_layers': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  91.0\n",
      "Epoch :0.008333333333333333    Train Loss :0.15295188128948212    Test Loss :0.45265138149261475\n",
      "Epoch :0.016666666666666666    Train Loss :0.11179084330797195    Test Loss :0.20791460573673248\n",
      "Epoch :0.025    Train Loss :0.05313001200556755    Test Loss :0.14389246702194214\n",
      "Epoch :0.03333333333333333    Train Loss :0.049586959183216095    Test Loss :0.08785835653543472\n",
      "Epoch :0.041666666666666664    Train Loss :0.04957563802599907    Test Loss :0.10569317638874054\n",
      "Epoch :0.05    Train Loss :0.04921447113156319    Test Loss :0.11346151679754257\n",
      "Epoch :0.058333333333333334    Train Loss :0.049409907311201096    Test Loss :0.08673396706581116\n",
      "Epoch :0.06666666666666667    Train Loss :0.048415590077638626    Test Loss :0.10906526446342468\n",
      "Epoch :0.075    Train Loss :0.045429617166519165    Test Loss :0.08473353832960129\n",
      "Epoch :0.08333333333333333    Train Loss :0.016790660098195076    Test Loss :0.014494058676064014\n",
      "Epoch :0.09166666666666666    Train Loss :0.0161027442663908    Test Loss :0.07187198847532272\n",
      "Epoch :0.1    Train Loss :0.013417846523225307    Test Loss :0.017141761258244514\n",
      "Epoch :0.10833333333333334    Train Loss :0.011524294503033161    Test Loss :0.012639140710234642\n",
      "Epoch :0.11666666666666667    Train Loss :0.009457824751734734    Test Loss :0.021310502663254738\n",
      "Epoch :0.125    Train Loss :0.007455004379153252    Test Loss :0.014296005479991436\n",
      "Epoch :0.13333333333333333    Train Loss :0.005698591936379671    Test Loss :0.013906504958868027\n",
      "Epoch :0.14166666666666666    Train Loss :0.004252902697771788    Test Loss :0.0072363573126494884\n",
      "Epoch :0.15    Train Loss :0.003801794024184346    Test Loss :0.005229446571320295\n",
      "Epoch :0.15833333333333333    Train Loss :0.0030462497379630804    Test Loss :0.004098015837371349\n",
      "Epoch :0.16666666666666666    Train Loss :0.0026244197506457567    Test Loss :0.004549222532659769\n",
      "Epoch :0.175    Train Loss :0.0024102937895804644    Test Loss :0.004765678197145462\n",
      "Epoch :0.18333333333333332    Train Loss :0.0022993814200162888    Test Loss :0.004797370173037052\n",
      "Epoch :0.19166666666666668    Train Loss :0.002041776431724429    Test Loss :0.004192490130662918\n",
      "Epoch :0.2    Train Loss :0.0020101016853004694    Test Loss :0.0037214336916804314\n",
      "Epoch :0.20833333333333334    Train Loss :0.0019085193052887917    Test Loss :0.003922601696103811\n",
      "Epoch :0.21666666666666667    Train Loss :0.0018253171583637595    Test Loss :0.0038298582658171654\n",
      "Epoch :0.225    Train Loss :0.0016475088195875287    Test Loss :0.0038856943137943745\n",
      "Epoch :0.23333333333333334    Train Loss :0.001636180211789906    Test Loss :0.003656198736280203\n",
      "Epoch :0.24166666666666667    Train Loss :0.0016329704085364938    Test Loss :0.0034373507369309664\n",
      "Epoch :0.25    Train Loss :0.0015560559695586562    Test Loss :0.0036102659069001675\n",
      "Epoch :0.25833333333333336    Train Loss :0.001494079944677651    Test Loss :0.003519895486533642\n",
      "Epoch :0.26666666666666666    Train Loss :0.0014810726279392838    Test Loss :0.003506865119561553\n",
      "Epoch :0.275    Train Loss :0.0013289923081174493    Test Loss :0.0034957195166498423\n",
      "Epoch :0.2833333333333333    Train Loss :0.0014865943230688572    Test Loss :0.0034178062342107296\n",
      "Epoch :0.2916666666666667    Train Loss :0.0013594423653557897    Test Loss :0.0034947888925671577\n",
      "Epoch :0.3    Train Loss :0.0014232342364266515    Test Loss :0.0037756965029984713\n",
      "Epoch :0.30833333333333335    Train Loss :0.001442741951905191    Test Loss :0.0033827610313892365\n",
      "Epoch :0.31666666666666665    Train Loss :0.001318819122388959    Test Loss :0.0032387548126280308\n",
      "Epoch :0.325    Train Loss :0.0012210372369736433    Test Loss :0.003168301424011588\n",
      "Epoch :0.3333333333333333    Train Loss :0.001430816831998527    Test Loss :0.0031297302339226007\n",
      "Epoch :0.3416666666666667    Train Loss :0.001158947590738535    Test Loss :0.0032793665304780006\n",
      "Epoch :0.35    Train Loss :0.0012374427169561386    Test Loss :0.00323781231418252\n",
      "Epoch :0.35833333333333334    Train Loss :0.0012010461650788784    Test Loss :0.0031973551958799362\n",
      "Epoch :0.36666666666666664    Train Loss :0.0011023877887055278    Test Loss :0.002898354781791568\n",
      "Epoch :0.375    Train Loss :0.0010261049028486013    Test Loss :0.002880990272387862\n",
      "Epoch :0.38333333333333336    Train Loss :0.0010760248405858874    Test Loss :0.002946963533759117\n",
      "Epoch :0.39166666666666666    Train Loss :0.0010455360170453787    Test Loss :0.0027946364134550095\n",
      "Epoch :0.4    Train Loss :0.0010707102483138442    Test Loss :0.0029153567738831043\n",
      "Epoch :0.4083333333333333    Train Loss :0.001082992646843195    Test Loss :0.002941341605037451\n",
      "Epoch :0.4166666666666667    Train Loss :0.0010579406516626477    Test Loss :0.002779862144961953\n",
      "Epoch :0.425    Train Loss :0.0011238051811233163    Test Loss :0.0028020204044878483\n",
      "Epoch :0.43333333333333335    Train Loss :0.0010951473377645016    Test Loss :0.0028026767540723085\n",
      "Epoch :0.44166666666666665    Train Loss :0.0010345381451770663    Test Loss :0.0028335433453321457\n",
      "Epoch :0.45    Train Loss :0.0009449176723137498    Test Loss :0.0028961561620235443\n",
      "Epoch :0.4583333333333333    Train Loss :0.0009350241161882877    Test Loss :0.0028161685913801193\n",
      "Epoch :0.4666666666666667    Train Loss :0.0008990872302092612    Test Loss :0.0027732066810131073\n",
      "Epoch :0.475    Train Loss :0.001172220567241311    Test Loss :0.002867950825020671\n",
      "Epoch :0.48333333333333334    Train Loss :0.0011465921998023987    Test Loss :0.002868189476430416\n",
      "Epoch :0.49166666666666664    Train Loss :0.0013814829289913177    Test Loss :0.0031478935852646828\n",
      "Epoch :0.5    Train Loss :0.0010213537607342005    Test Loss :0.0028188463766127825\n",
      "Epoch :0.5083333333333333    Train Loss :0.0009057186543941498    Test Loss :0.0026017448399215937\n",
      "Epoch :0.5166666666666667    Train Loss :0.000924890860915184    Test Loss :0.0025799660943448544\n",
      "Epoch :0.525    Train Loss :0.0008565980824641883    Test Loss :0.0026731984689831734\n",
      "Epoch :0.5333333333333333    Train Loss :0.0008577643311582506    Test Loss :0.002696516690775752\n",
      "Epoch :0.5416666666666666    Train Loss :0.0008814161992631853    Test Loss :0.0027333588805049658\n",
      "Epoch :0.55    Train Loss :0.0008903080015443265    Test Loss :0.002655202290043235\n",
      "Epoch :0.5583333333333333    Train Loss :0.0008432459435425699    Test Loss :0.0025682414416223764\n",
      "Epoch :0.5666666666666667    Train Loss :0.0008435978670604527    Test Loss :0.0025287603493779898\n",
      "Epoch :0.575    Train Loss :0.0008477176306769252    Test Loss :0.0026436520274728537\n",
      "Epoch :0.5833333333333334    Train Loss :0.0008642131579108536    Test Loss :0.0026421649381518364\n",
      "Epoch :0.5916666666666667    Train Loss :0.0008580789435654879    Test Loss :0.0026568686589598656\n",
      "Epoch :0.6    Train Loss :0.0021990714594721794    Test Loss :0.0041283308528363705\n",
      "Epoch :0.6083333333333333    Train Loss :0.0009339199750684202    Test Loss :0.0032673641107976437\n",
      "Epoch :0.6166666666666667    Train Loss :0.0008880584500730038    Test Loss :0.002482715994119644\n",
      "Epoch :0.625    Train Loss :0.000849363743327558    Test Loss :0.0025794256944209337\n",
      "Epoch :0.6333333333333333    Train Loss :0.0009348614839836955    Test Loss :0.0025469379033893347\n",
      "Epoch :0.6416666666666667    Train Loss :0.0008920071995817125    Test Loss :0.0024514691904187202\n",
      "Epoch :0.65    Train Loss :0.000806046009529382    Test Loss :0.0026284195482730865\n",
      "Epoch :0.6583333333333333    Train Loss :0.0008346896502189338    Test Loss :0.002458963543176651\n",
      "Epoch :0.6666666666666666    Train Loss :0.0007837149896658957    Test Loss :0.0024748551659286022\n",
      "Epoch :0.675    Train Loss :0.0007709317724220455    Test Loss :0.002528166165575385\n",
      "Epoch :0.6833333333333333    Train Loss :0.0007535852491855621    Test Loss :0.002521354239434004\n",
      "Epoch :0.6916666666666667    Train Loss :0.0007057623588480055    Test Loss :0.0023711363319307566\n",
      "Epoch :0.7    Train Loss :0.0007544081308878958    Test Loss :0.002390615176409483\n",
      "Epoch :0.7083333333333334    Train Loss :0.0007400260074064136    Test Loss :0.0024578834418207407\n",
      "Epoch :0.7166666666666667    Train Loss :0.000822171161416918    Test Loss :0.0024322927929461002\n",
      "Epoch :0.725    Train Loss :0.0007785517373122275    Test Loss :0.0025316921528428793\n",
      "Epoch :0.7333333333333333    Train Loss :0.00075828394619748    Test Loss :0.0024598208256065845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7416666666666667    Train Loss :0.0007583203841932118    Test Loss :0.002669393550604582\n",
      "Epoch :0.75    Train Loss :0.0007682534051127732    Test Loss :0.0025002257898449898\n",
      "Epoch :0.7583333333333333    Train Loss :0.0007555711199529469    Test Loss :0.0022709600161761045\n",
      "Epoch :0.7666666666666667    Train Loss :0.0007838343735784292    Test Loss :0.002499964088201523\n",
      "Epoch :0.775    Train Loss :0.0006943881162442267    Test Loss :0.0024589351378381252\n",
      "Epoch :0.7833333333333333    Train Loss :0.0006996464799158275    Test Loss :0.0024475676473230124\n",
      "Epoch :0.7916666666666666    Train Loss :0.0007403050549328327    Test Loss :0.0025871461257338524\n",
      "Epoch :0.8    Train Loss :0.000700661854352802    Test Loss :0.0022288518957793713\n",
      "Epoch :0.8083333333333333    Train Loss :0.0007276460528373718    Test Loss :0.0022671788465231657\n",
      "Epoch :0.8166666666666667    Train Loss :0.0006936464924365282    Test Loss :0.0022469377145171165\n",
      "Epoch :0.825    Train Loss :0.0006763184792362154    Test Loss :0.002356771845370531\n",
      "Epoch :0.8333333333333334    Train Loss :0.0007534514297731221    Test Loss :0.002363311592489481\n",
      "Epoch :0.8416666666666667    Train Loss :0.0007424937793985009    Test Loss :0.002466145670041442\n",
      "Epoch :0.85    Train Loss :0.0007911028806120157    Test Loss :0.002599777653813362\n",
      "Epoch :0.8583333333333333    Train Loss :0.0006741685210727155    Test Loss :0.00248994049616158\n",
      "Epoch :0.8666666666666667    Train Loss :0.0007914943853393197    Test Loss :0.0025270693004131317\n",
      "Epoch :0.875    Train Loss :0.000744273595046252    Test Loss :0.0023341814521700144\n",
      "Epoch :0.8833333333333333    Train Loss :0.0006700340309180319    Test Loss :0.002364904386922717\n",
      "Epoch :0.8916666666666667    Train Loss :0.0007076553301885724    Test Loss :0.0023524111602455378\n",
      "Epoch :0.9    Train Loss :0.0007539329235441983    Test Loss :0.0023270519450306892\n",
      "Epoch :0.9083333333333333    Train Loss :0.000661433965433389    Test Loss :0.002301466651260853\n",
      "Epoch :0.9166666666666666    Train Loss :0.0006631749565713108    Test Loss :0.0022885689977556467\n",
      "Epoch :0.925    Train Loss :0.0006718231015838683    Test Loss :0.0023803161457180977\n",
      "Epoch :0.9333333333333333    Train Loss :0.0006716243224218488    Test Loss :0.0023663032334297895\n",
      "Epoch :0.9416666666666667    Train Loss :0.000735183828510344    Test Loss :0.0022843412589281797\n",
      "Epoch :0.95    Train Loss :0.0006399966077879071    Test Loss :0.0023031209129840136\n",
      "Epoch :0.9583333333333334    Train Loss :0.0006996659212745726    Test Loss :0.0023688385263085365\n",
      "Epoch :0.9666666666666667    Train Loss :0.0006999876350164413    Test Loss :0.0022839917801320553\n",
      "Epoch :0.975    Train Loss :0.0006918821600265801    Test Loss :0.0022904938086867332\n",
      "Epoch :0.9833333333333333    Train Loss :0.0006555813015438616    Test Loss :0.002488489495590329\n",
      "Epoch :0.9916666666666667    Train Loss :0.0008471151813864708    Test Loss :0.002452277112752199\n",
      "Epoch :1.0    Train Loss :0.0006112951668910682    Test Loss :0.002247545635327697\n",
      "RMSE: 8.621712373709983\n",
      "MAE: 7.035826601820912\n",
      "MAPE: 6.377721469559591%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 100, 'lr': 0.01, 'num_layers': 5}\n",
      "total:  94.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.12391818314790726    Test Loss :0.12781313061714172\n",
      "Epoch :0.016666666666666666    Train Loss :0.03680628910660744    Test Loss :0.027639389038085938\n",
      "Epoch :0.025    Train Loss :0.024409951642155647    Test Loss :0.06762296706438065\n",
      "Epoch :0.03333333333333333    Train Loss :0.010049902833998203    Test Loss :0.04216510057449341\n",
      "Epoch :0.041666666666666664    Train Loss :0.010889420285820961    Test Loss :0.017115170136094093\n",
      "Epoch :0.05    Train Loss :0.007666415069252253    Test Loss :0.012671134434640408\n",
      "Epoch :0.058333333333333334    Train Loss :0.005070023704320192    Test Loss :0.007342710625380278\n",
      "Epoch :0.06666666666666667    Train Loss :0.0043258508667349815    Test Loss :0.007334110327064991\n",
      "Epoch :0.075    Train Loss :0.0026700072921812534    Test Loss :0.006308337673544884\n",
      "Epoch :0.08333333333333333    Train Loss :0.002587428782135248    Test Loss :0.004936125595122576\n",
      "Epoch :0.09166666666666666    Train Loss :0.0019464580109342933    Test Loss :0.00407091248780489\n",
      "Epoch :0.1    Train Loss :0.0016064578667283058    Test Loss :0.0033569149672985077\n",
      "Epoch :0.10833333333333334    Train Loss :0.001481674611568451    Test Loss :0.002886792179197073\n",
      "Epoch :0.11666666666666667    Train Loss :0.0013398807495832443    Test Loss :0.0026995015796273947\n",
      "Epoch :0.125    Train Loss :0.0011763001093640924    Test Loss :0.0026987078599631786\n",
      "Epoch :0.13333333333333333    Train Loss :0.0011147753102704883    Test Loss :0.0027509708888828754\n",
      "Epoch :0.14166666666666666    Train Loss :0.001022718963213265    Test Loss :0.0024456861428916454\n",
      "Epoch :0.15    Train Loss :0.0009512139367870986    Test Loss :0.0025469299871474504\n",
      "Epoch :0.15833333333333333    Train Loss :0.0009055879199877381    Test Loss :0.002463673707097769\n",
      "Epoch :0.16666666666666666    Train Loss :0.0008716469164937735    Test Loss :0.0025746680330485106\n",
      "Epoch :0.175    Train Loss :0.000843980407807976    Test Loss :0.002546177711337805\n",
      "Epoch :0.18333333333333332    Train Loss :0.0008102533756755292    Test Loss :0.002466248581185937\n",
      "Epoch :0.19166666666666668    Train Loss :0.0007864228682592511    Test Loss :0.0023474511690437794\n",
      "Epoch :0.2    Train Loss :0.0007467312971130013    Test Loss :0.0023287383373826742\n",
      "Epoch :0.20833333333333334    Train Loss :0.0007112572202458978    Test Loss :0.002327545080333948\n",
      "Epoch :0.21666666666666667    Train Loss :0.0007195724174380302    Test Loss :0.0023983654100447893\n",
      "Epoch :0.225    Train Loss :0.00069266720674932    Test Loss :0.002192856976762414\n",
      "Epoch :0.23333333333333334    Train Loss :0.0006751949549652636    Test Loss :0.0023341586347669363\n",
      "Epoch :0.24166666666666667    Train Loss :0.0006587865063920617    Test Loss :0.002298536943271756\n",
      "Epoch :0.25    Train Loss :0.000635713804513216    Test Loss :0.002200532238930464\n",
      "Epoch :0.25833333333333336    Train Loss :0.0006776789086870849    Test Loss :0.0021942167077213526\n",
      "Epoch :0.26666666666666666    Train Loss :0.0006323423585854471    Test Loss :0.002228648168966174\n",
      "Epoch :0.275    Train Loss :0.0006083863554522395    Test Loss :0.002219497226178646\n",
      "Epoch :0.2833333333333333    Train Loss :0.0005905077559873462    Test Loss :0.0022525503300130367\n",
      "Epoch :0.2916666666666667    Train Loss :0.0005870663444511592    Test Loss :0.0022563589736819267\n",
      "Epoch :0.3    Train Loss :0.0005769248818978667    Test Loss :0.002225162461400032\n",
      "Epoch :0.30833333333333335    Train Loss :0.0005883684498257935    Test Loss :0.0021829111501574516\n",
      "Epoch :0.31666666666666665    Train Loss :0.0005635168636217713    Test Loss :0.0021755704656243324\n",
      "Epoch :0.325    Train Loss :0.0005655937129631639    Test Loss :0.002148492494598031\n",
      "Epoch :0.3333333333333333    Train Loss :0.000558723695576191    Test Loss :0.00218517379835248\n",
      "Epoch :0.3416666666666667    Train Loss :0.0005507483147084713    Test Loss :0.0021078309509903193\n",
      "Epoch :0.35    Train Loss :0.0005548445624299347    Test Loss :0.0021518957801163197\n",
      "Epoch :0.35833333333333334    Train Loss :0.0005459411186166108    Test Loss :0.002127797110006213\n",
      "Epoch :0.36666666666666664    Train Loss :0.0005218455917201936    Test Loss :0.002131387125700712\n",
      "Epoch :0.375    Train Loss :0.00051785510731861    Test Loss :0.0019765011966228485\n",
      "Epoch :0.38333333333333336    Train Loss :0.0005255220457911491    Test Loss :0.002088025910779834\n",
      "Epoch :0.39166666666666666    Train Loss :0.0005182224558666348    Test Loss :0.00209218543022871\n",
      "Epoch :0.4    Train Loss :0.0005001876270398498    Test Loss :0.0021214098669588566\n",
      "Epoch :0.4083333333333333    Train Loss :0.000516597181558609    Test Loss :0.002104108454659581\n",
      "Epoch :0.4166666666666667    Train Loss :0.0005325203528627753    Test Loss :0.0020894838962703943\n",
      "Epoch :0.425    Train Loss :0.0004920679493807256    Test Loss :0.0020722311455756426\n",
      "Epoch :0.43333333333333335    Train Loss :0.0005125040770508349    Test Loss :0.0019707682076841593\n",
      "Epoch :0.44166666666666665    Train Loss :0.00048738112673163414    Test Loss :0.002125679748132825\n",
      "Epoch :0.45    Train Loss :0.0004907419788651168    Test Loss :0.001998333726078272\n",
      "Epoch :0.4583333333333333    Train Loss :0.0004997064243070781    Test Loss :0.0020141794811934233\n",
      "Epoch :0.4666666666666667    Train Loss :0.0004715282120741904    Test Loss :0.0020339020993560553\n",
      "Epoch :0.475    Train Loss :0.0004716124967671931    Test Loss :0.002022763714194298\n",
      "Epoch :0.48333333333333334    Train Loss :0.0004893190925940871    Test Loss :0.0020104837603867054\n",
      "Epoch :0.49166666666666664    Train Loss :0.0004748063220176846    Test Loss :0.0020042378455400467\n",
      "Epoch :0.5    Train Loss :0.00047846525558270514    Test Loss :0.0019519418710842729\n",
      "Epoch :0.5083333333333333    Train Loss :0.00045509968185797334    Test Loss :0.002002956811338663\n",
      "Epoch :0.5166666666666667    Train Loss :0.00047599163372069597    Test Loss :0.001898511778563261\n",
      "Epoch :0.525    Train Loss :0.0004716338589787483    Test Loss :0.00192844623234123\n",
      "Epoch :0.5333333333333333    Train Loss :0.0004566742281895131    Test Loss :0.0019371213857084513\n",
      "Epoch :0.5416666666666666    Train Loss :0.0004571705067064613    Test Loss :0.0019334519747644663\n",
      "Epoch :0.55    Train Loss :0.0004766130878124386    Test Loss :0.001904731267131865\n",
      "Epoch :0.5583333333333333    Train Loss :0.00047553080366924405    Test Loss :0.0020049980375915766\n",
      "Epoch :0.5666666666666667    Train Loss :0.00045232343836687505    Test Loss :0.0019992790184915066\n",
      "Epoch :0.575    Train Loss :0.0004597697115968913    Test Loss :0.0019986480474472046\n",
      "Epoch :0.5833333333333334    Train Loss :0.0004558470391202718    Test Loss :0.0018839018885046244\n",
      "Epoch :0.5916666666666667    Train Loss :0.0004553469771053642    Test Loss :0.0019246911397203803\n",
      "Epoch :0.6    Train Loss :0.0004559001245070249    Test Loss :0.0019165354315191507\n",
      "Epoch :0.6083333333333333    Train Loss :0.00045835127821192145    Test Loss :0.001960726687684655\n",
      "Epoch :0.6166666666666667    Train Loss :0.000442822725744918    Test Loss :0.0020052355248481035\n",
      "Epoch :0.625    Train Loss :0.000453185144579038    Test Loss :0.001971171936020255\n",
      "Epoch :0.6333333333333333    Train Loss :0.00045796186896041036    Test Loss :0.0019221351249143481\n",
      "Epoch :0.6416666666666667    Train Loss :0.00044301291927695274    Test Loss :0.0019478123867884278\n",
      "Epoch :0.65    Train Loss :0.0004547357093542814    Test Loss :0.0018267856212332845\n",
      "Epoch :0.6583333333333333    Train Loss :0.0004652045317925513    Test Loss :0.0018754206830635667\n",
      "Epoch :0.6666666666666666    Train Loss :0.0004452330176718533    Test Loss :0.0018973464611917734\n",
      "Epoch :0.675    Train Loss :0.0004271390789654106    Test Loss :0.0019334126263856888\n",
      "Epoch :0.6833333333333333    Train Loss :0.0004370517563074827    Test Loss :0.001884675002656877\n",
      "Epoch :0.6916666666666667    Train Loss :0.0004444129590410739    Test Loss :0.0018169879913330078\n",
      "Epoch :0.7    Train Loss :0.00044386013178154826    Test Loss :0.0018525721970945597\n",
      "Epoch :0.7083333333333334    Train Loss :0.000434577144915238    Test Loss :0.0017549467738717794\n",
      "Epoch :0.7166666666666667    Train Loss :0.00042512669460847974    Test Loss :0.0019774308893829584\n",
      "Epoch :0.725    Train Loss :0.00046705733984708786    Test Loss :0.0018859351985156536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7333333333333333    Train Loss :0.0004109693691134453    Test Loss :0.0018712376477196813\n",
      "Epoch :0.7416666666666667    Train Loss :0.00043355199159123003    Test Loss :0.001876693801023066\n",
      "Epoch :0.75    Train Loss :0.00043499129242263734    Test Loss :0.0017918245866894722\n",
      "Epoch :0.7583333333333333    Train Loss :0.0004267243202775717    Test Loss :0.001817032927647233\n",
      "Epoch :0.7666666666666667    Train Loss :0.000422170152887702    Test Loss :0.001791540184058249\n",
      "Epoch :0.775    Train Loss :0.0004066366236656904    Test Loss :0.0018142780754715204\n",
      "Epoch :0.7833333333333333    Train Loss :0.0004181654949206859    Test Loss :0.0017362877260893583\n",
      "Epoch :0.7916666666666666    Train Loss :0.0004196243826299906    Test Loss :0.0017704013735055923\n",
      "Epoch :0.8    Train Loss :0.0004025092348456383    Test Loss :0.0018254331080242991\n",
      "Epoch :0.8083333333333333    Train Loss :0.0004032233846373856    Test Loss :0.0017571391072124243\n",
      "Epoch :0.8166666666666667    Train Loss :0.00039786400157026947    Test Loss :0.0017328285612165928\n",
      "Epoch :0.825    Train Loss :0.000435197027400136    Test Loss :0.0017369872657582164\n",
      "Epoch :0.8333333333333334    Train Loss :0.0004958217614330351    Test Loss :0.0017982645658776164\n",
      "Epoch :0.8416666666666667    Train Loss :0.0003948448575101793    Test Loss :0.0018018140690401196\n",
      "Epoch :0.85    Train Loss :0.0004067598783876747    Test Loss :0.001799197867512703\n",
      "Epoch :0.8583333333333333    Train Loss :0.0004253180231899023    Test Loss :0.0018140869215130806\n",
      "Epoch :0.8666666666666667    Train Loss :0.00038115328061394393    Test Loss :0.001754912780597806\n",
      "Epoch :0.875    Train Loss :0.00040766570600681007    Test Loss :0.001741487067192793\n",
      "Epoch :0.8833333333333333    Train Loss :0.00040543373324908316    Test Loss :0.0016702731372788548\n",
      "Epoch :0.8916666666666667    Train Loss :0.0004879661719314754    Test Loss :0.0018456641118973494\n",
      "Epoch :0.9    Train Loss :0.0006373637006618083    Test Loss :0.0021087813656777143\n",
      "Epoch :0.9083333333333333    Train Loss :0.0004502836090978235    Test Loss :0.001879952265881002\n",
      "Epoch :0.9166666666666666    Train Loss :0.00041341548785567284    Test Loss :0.0018975085113197565\n",
      "Epoch :0.925    Train Loss :0.0004063917731400579    Test Loss :0.0018311889143660665\n",
      "Epoch :0.9333333333333333    Train Loss :0.00042539555579423904    Test Loss :0.0017875197809189558\n",
      "Epoch :0.9416666666666667    Train Loss :0.00041626309393905103    Test Loss :0.0017886534333229065\n",
      "Epoch :0.95    Train Loss :0.0003801075217779726    Test Loss :0.0017332180868834257\n",
      "Epoch :0.9583333333333334    Train Loss :0.0004431848938111216    Test Loss :0.0017231149831786752\n",
      "Epoch :0.9666666666666667    Train Loss :0.0004878781910520047    Test Loss :0.0017449103761464357\n",
      "Epoch :0.975    Train Loss :0.0003820907440967858    Test Loss :0.0016637700609862804\n",
      "Epoch :0.9833333333333333    Train Loss :0.00036948855267837644    Test Loss :0.0017310372786596417\n",
      "Epoch :0.9916666666666667    Train Loss :0.0004627026792149991    Test Loss :0.0017681056633591652\n",
      "Epoch :1.0    Train Loss :0.00038008176488801837    Test Loss :0.001755504752509296\n",
      "RMSE: 8.713282639988577\n",
      "MAE: 7.141961779686792\n",
      "MAPE: 6.501016443940048%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 100, 'lr': 0.02, 'num_layers': 2}\n",
      "total:  97.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.1807914823293686    Test Loss :1.0145211219787598\n",
      "Epoch :0.016666666666666666    Train Loss :0.049645617604255676    Test Loss :0.19267867505550385\n",
      "Epoch :0.025    Train Loss :0.059740398079156876    Test Loss :0.05754954740405083\n",
      "Epoch :0.03333333333333333    Train Loss :0.053043678402900696    Test Loss :0.10696156322956085\n",
      "Epoch :0.041666666666666664    Train Loss :0.048693858087062836    Test Loss :0.12476587295532227\n",
      "Epoch :0.05    Train Loss :0.04940761625766754    Test Loss :0.07444535940885544\n",
      "Epoch :0.058333333333333334    Train Loss :0.05002640187740326    Test Loss :0.11842428892850876\n",
      "Epoch :0.06666666666666667    Train Loss :0.04893321171402931    Test Loss :0.10289277136325836\n",
      "Epoch :0.075    Train Loss :0.04858334735035896    Test Loss :0.08873768150806427\n",
      "Epoch :0.08333333333333333    Train Loss :0.0488734170794487    Test Loss :0.10856740176677704\n",
      "Epoch :0.09166666666666666    Train Loss :0.04855965077877045    Test Loss :0.10476140677928925\n",
      "Epoch :0.1    Train Loss :0.048585571348667145    Test Loss :0.09475798904895782\n",
      "Epoch :0.10833333333333334    Train Loss :0.04854763299226761    Test Loss :0.09997756779193878\n",
      "Epoch :0.11666666666666667    Train Loss :0.04852058365941048    Test Loss :0.10479020327329636\n",
      "Epoch :0.125    Train Loss :0.04855550825595856    Test Loss :0.10165902227163315\n",
      "Epoch :0.13333333333333333    Train Loss :0.04853919893503189    Test Loss :0.09885361790657043\n",
      "Epoch :0.14166666666666666    Train Loss :0.04853717237710953    Test Loss :0.09974994510412216\n",
      "Epoch :0.15    Train Loss :0.04848436638712883    Test Loss :0.10149087756872177\n",
      "Epoch :0.15833333333333333    Train Loss :0.04850888252258301    Test Loss :0.10192051529884338\n",
      "Epoch :0.16666666666666666    Train Loss :0.04852692037820816    Test Loss :0.10111470520496368\n",
      "Epoch :0.175    Train Loss :0.048489075154066086    Test Loss :0.10060161352157593\n",
      "Epoch :0.18333333333333332    Train Loss :0.04852544516324997    Test Loss :0.1004146859049797\n",
      "Epoch :0.19166666666666668    Train Loss :0.04851420968770981    Test Loss :0.10071470588445663\n",
      "Epoch :0.2    Train Loss :0.04851093143224716    Test Loss :0.10085724294185638\n",
      "Epoch :0.20833333333333334    Train Loss :0.048496585339307785    Test Loss :0.10103236883878708\n",
      "Epoch :0.21666666666666667    Train Loss :0.048514872789382935    Test Loss :0.10103033483028412\n",
      "Epoch :0.225    Train Loss :0.04848785698413849    Test Loss :0.10108917951583862\n",
      "Epoch :0.23333333333333334    Train Loss :0.04850726202130318    Test Loss :0.10107355564832687\n",
      "Epoch :0.24166666666666667    Train Loss :0.04850587621331215    Test Loss :0.10105476528406143\n",
      "Epoch :0.25    Train Loss :0.048501696437597275    Test Loss :0.1010945737361908\n",
      "Epoch :0.25833333333333336    Train Loss :0.048532258719205856    Test Loss :0.1009405255317688\n",
      "Epoch :0.26666666666666666    Train Loss :0.04849877208471298    Test Loss :0.10108789056539536\n",
      "Epoch :0.275    Train Loss :0.04856238141655922    Test Loss :0.10106241703033447\n",
      "Epoch :0.2833333333333333    Train Loss :0.04848476126790047    Test Loss :0.10106921195983887\n",
      "Epoch :0.2916666666666667    Train Loss :0.048513609915971756    Test Loss :0.10104496031999588\n",
      "Epoch :0.3    Train Loss :0.04850829392671585    Test Loss :0.10102114081382751\n",
      "Epoch :0.30833333333333335    Train Loss :0.048515014350414276    Test Loss :0.1009388193488121\n",
      "Epoch :0.31666666666666665    Train Loss :0.04851488769054413    Test Loss :0.10106786340475082\n",
      "Epoch :0.325    Train Loss :0.04851413518190384    Test Loss :0.10114811360836029\n",
      "Epoch :0.3333333333333333    Train Loss :0.04849737882614136    Test Loss :0.10107435286045074\n",
      "Epoch :0.3416666666666667    Train Loss :0.04851611703634262    Test Loss :0.10108011215925217\n",
      "Epoch :0.35    Train Loss :0.04850199818611145    Test Loss :0.10097731649875641\n",
      "Epoch :0.35833333333333334    Train Loss :0.048496313393116    Test Loss :0.10088984668254852\n",
      "Epoch :0.36666666666666664    Train Loss :0.04848944768309593    Test Loss :0.10105552524328232\n",
      "Epoch :0.375    Train Loss :0.04848940297961235    Test Loss :0.10100846737623215\n",
      "Epoch :0.38333333333333336    Train Loss :0.048535823822021484    Test Loss :0.10085145384073257\n",
      "Epoch :0.39166666666666666    Train Loss :0.04852524772286415    Test Loss :0.10098671913146973\n",
      "Epoch :0.4    Train Loss :0.04850582778453827    Test Loss :0.10083281993865967\n",
      "Epoch :0.4083333333333333    Train Loss :0.04851140081882477    Test Loss :0.10075949877500534\n",
      "Epoch :0.4166666666666667    Train Loss :0.048492155969142914    Test Loss :0.10079330950975418\n",
      "Epoch :0.425    Train Loss :0.048508282750844955    Test Loss :0.10081472247838974\n",
      "Epoch :0.43333333333333335    Train Loss :0.04850327596068382    Test Loss :0.10089496523141861\n",
      "Epoch :0.44166666666666665    Train Loss :0.04851393774151802    Test Loss :0.10092868655920029\n",
      "Epoch :0.45    Train Loss :0.0484934076666832    Test Loss :0.10091908276081085\n",
      "Epoch :0.4583333333333333    Train Loss :0.04851919785141945    Test Loss :0.10099227726459503\n",
      "Epoch :0.4666666666666667    Train Loss :0.04850812628865242    Test Loss :0.10092297941446304\n",
      "Epoch :0.475    Train Loss :0.048505477607250214    Test Loss :0.10081416368484497\n",
      "Epoch :0.48333333333333334    Train Loss :0.04849490895867348    Test Loss :0.10090001672506332\n",
      "Epoch :0.49166666666666664    Train Loss :0.04849451407790184    Test Loss :0.10084289312362671\n",
      "Epoch :0.5    Train Loss :0.04853130877017975    Test Loss :0.1008375957608223\n",
      "Epoch :0.5083333333333333    Train Loss :0.04850140959024429    Test Loss :0.10076139867305756\n",
      "Epoch :0.5166666666666667    Train Loss :0.048497289419174194    Test Loss :0.10084830969572067\n",
      "Epoch :0.525    Train Loss :0.04851772263646126    Test Loss :0.10098553448915482\n",
      "Epoch :0.5333333333333333    Train Loss :0.0485193096101284    Test Loss :0.1010788306593895\n",
      "Epoch :0.5416666666666666    Train Loss :0.04850819334387779    Test Loss :0.10098673403263092\n",
      "Epoch :0.55    Train Loss :0.04851898178458214    Test Loss :0.10087092965841293\n",
      "Epoch :0.5583333333333333    Train Loss :0.04850093275308609    Test Loss :0.10094796866178513\n",
      "Epoch :0.5666666666666667    Train Loss :0.0485142357647419    Test Loss :0.10095576196908951\n",
      "Epoch :0.575    Train Loss :0.04849076643586159    Test Loss :0.1008945181965828\n",
      "Epoch :0.5833333333333334    Train Loss :0.04850665107369423    Test Loss :0.10098394751548767\n",
      "Epoch :0.5916666666666667    Train Loss :0.04849586635828018    Test Loss :0.10087231546640396\n",
      "Epoch :0.6    Train Loss :0.04849905148148537    Test Loss :0.10094089806079865\n",
      "Epoch :0.6083333333333333    Train Loss :0.048483576625585556    Test Loss :0.1009032353758812\n",
      "Epoch :0.6166666666666667    Train Loss :0.048492368310689926    Test Loss :0.10084233433008194\n",
      "Epoch :0.625    Train Loss :0.048497963696718216    Test Loss :0.10088751465082169\n",
      "Epoch :0.6333333333333333    Train Loss :0.048520278185606    Test Loss :0.10097502171993256\n",
      "Epoch :0.6416666666666667    Train Loss :0.04851262643933296    Test Loss :0.10093288123607635\n",
      "Epoch :0.65    Train Loss :0.04850243031978607    Test Loss :0.10073494911193848\n",
      "Epoch :0.6583333333333333    Train Loss :0.04850440099835396    Test Loss :0.10088604688644409\n",
      "Epoch :0.6666666666666666    Train Loss :0.04850640520453453    Test Loss :0.10099764168262482\n",
      "Epoch :0.675    Train Loss :0.048504285514354706    Test Loss :0.10090837627649307\n",
      "Epoch :0.6833333333333333    Train Loss :0.04850451648235321    Test Loss :0.10091019421815872\n",
      "Epoch :0.6916666666666667    Train Loss :0.04850488156080246    Test Loss :0.10096720606088638\n",
      "Epoch :0.7    Train Loss :0.04849787801504135    Test Loss :0.10076378285884857\n",
      "Epoch :0.7083333333333334    Train Loss :0.04851787909865379    Test Loss :0.1009664237499237\n",
      "Epoch :0.7166666666666667    Train Loss :0.04850999265909195    Test Loss :0.10086651146411896\n",
      "Epoch :0.725    Train Loss :0.04851417988538742    Test Loss :0.10090436041355133\n",
      "Epoch :0.7333333333333333    Train Loss :0.04850718006491661    Test Loss :0.10084192454814911\n",
      "Epoch :0.7416666666666667    Train Loss :0.04850265011191368    Test Loss :0.10087493062019348\n",
      "Epoch :0.75    Train Loss :0.04850446805357933    Test Loss :0.1009121835231781\n",
      "Epoch :0.7583333333333333    Train Loss :0.04848495498299599    Test Loss :0.10105850547552109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7666666666666667    Train Loss :0.048524193465709686    Test Loss :0.10087786614894867\n",
      "Epoch :0.775    Train Loss :0.0485069565474987    Test Loss :0.1008775383234024\n",
      "Epoch :0.7833333333333333    Train Loss :0.04848957434296608    Test Loss :0.10077938437461853\n",
      "Epoch :0.7916666666666666    Train Loss :0.04852207377552986    Test Loss :0.10091347992420197\n",
      "Epoch :0.8    Train Loss :0.04851005971431732    Test Loss :0.10096586495637894\n",
      "Epoch :0.8083333333333333    Train Loss :0.04851998761296272    Test Loss :0.10070157796144485\n",
      "Epoch :0.8166666666666667    Train Loss :0.04851469025015831    Test Loss :0.1011430025100708\n",
      "Epoch :0.825    Train Loss :0.0485125295817852    Test Loss :0.10073763132095337\n",
      "Epoch :0.8333333333333334    Train Loss :0.04850153625011444    Test Loss :0.10105180740356445\n",
      "Epoch :0.8416666666666667    Train Loss :0.04850633814930916    Test Loss :0.10081728547811508\n",
      "Epoch :0.85    Train Loss :0.04851142317056656    Test Loss :0.10089506208896637\n",
      "Epoch :0.8583333333333333    Train Loss :0.048494502902030945    Test Loss :0.10093437880277634\n",
      "Epoch :0.8666666666666667    Train Loss :0.04851851612329483    Test Loss :0.10086303949356079\n",
      "Epoch :0.875    Train Loss :0.048505011945962906    Test Loss :0.10089428722858429\n",
      "Epoch :0.8833333333333333    Train Loss :0.048492930829524994    Test Loss :0.10084404796361923\n",
      "Epoch :0.8916666666666667    Train Loss :0.048500824719667435    Test Loss :0.10098770260810852\n",
      "Epoch :0.9    Train Loss :0.04850214347243309    Test Loss :0.10090838372707367\n",
      "Epoch :0.9083333333333333    Train Loss :0.04849987477064133    Test Loss :0.1009097769856453\n",
      "Epoch :0.9166666666666666    Train Loss :0.04850071296095848    Test Loss :0.10082578659057617\n",
      "Epoch :0.925    Train Loss :0.04849826917052269    Test Loss :0.10093707591295242\n",
      "Epoch :0.9333333333333333    Train Loss :0.048519399017095566    Test Loss :0.1008688360452652\n",
      "Epoch :0.9416666666666667    Train Loss :0.048506204038858414    Test Loss :0.10092318803071976\n",
      "Epoch :0.95    Train Loss :0.04850255697965622    Test Loss :0.10091523081064224\n",
      "Epoch :0.9583333333333334    Train Loss :0.048509012907743454    Test Loss :0.10092215240001678\n",
      "Epoch :0.9666666666666667    Train Loss :0.04850321263074875    Test Loss :0.10088272392749786\n",
      "Epoch :0.975    Train Loss :0.04849303141236305    Test Loss :0.10098808258771896\n",
      "Epoch :0.9833333333333333    Train Loss :0.04850354790687561    Test Loss :0.10091579705476761\n",
      "Epoch :0.9916666666666667    Train Loss :0.04849792644381523    Test Loss :0.10078596323728561\n",
      "Epoch :1.0    Train Loss :0.048515401780605316    Test Loss :0.10093842446804047\n",
      "RMSE: 30.732147971672028\n",
      "MAE: 29.711840971199283\n",
      "MAPE: 25.874036334157708%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 100, 'lr': 0.02, 'num_layers': 5}\n",
      "total:  100.0\n",
      "Tempo total de execução: 12868.89765381813 segundos\n",
      "RMSE: 7.930554814887464\n",
      "MAE: 6.432557521841557\n",
      "MAPE: 5.564407815771348\n",
      "{'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 100, 'lr': 0.01, 'num_layers': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\3723300266.py:83: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "params_grid = {'num_layers':[2,5],\n",
    "               'hid_size':[50,100],\n",
    "              'lr': [0.01,0.02],\n",
    "               'epochs':[400,600],\n",
    "              'dropout_rate':[0.5,0.8]}\n",
    "grid = ParameterGrid(params_grid)\n",
    "cnt = 0\n",
    "for p in grid:\n",
    "    cnt = cnt+1\n",
    "\n",
    "print('Total Possible Models',cnt)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "model_parameters = pd.DataFrame(columns = ['RMSE','Parameters'])\n",
    "best_rmse = np.inf\n",
    "best_prediction=None\n",
    "count = 0\n",
    "start_time = time.time()\n",
    "\n",
    "for p in grid:\n",
    "    \n",
    "    lstm = LSTM(in_dim = x.shape[-1],\n",
    "                hid_dim = p['hid_size'],\n",
    "                out_dim = x.shape[-1],\n",
    "                num_layers = p['num_layers'], \n",
    "                dropout_rate= p['dropout_rate'])\n",
    "    \n",
    "    optimizer = torch.optim.Adam(lstm.parameters(), lr=p['lr'])\n",
    "    \n",
    "    loss_fun = nn.MSELoss()\n",
    "    \n",
    "    train_loss, test_loss = train_model(lstm,\n",
    "               loss_fun,\n",
    "               optimizer,\n",
    "               train_x,\n",
    "               test_x,\n",
    "               train_y,\n",
    "               test_y,\n",
    "               epochs=p['epochs'])\n",
    "\n",
    "    \n",
    "    # testing the predction model on multiple time series\n",
    "    last_x = train_x[-1].view(entradas)\n",
    "\n",
    "    prediction_val = []\n",
    "\n",
    "    while len(prediction_val)<len(test_y):\n",
    "        prediction = lstm(last_x.view(1,entradas,1))\n",
    "        prediction_val.append(prediction[0,0].item())\n",
    "\n",
    "\n",
    "        ## replace the predicted value in last x\n",
    "        last_x = torch.cat((last_x[1:],prediction[0]))\n",
    "\n",
    "    # plot the result\n",
    "    train_y_cp = scale.inverse_transform(train_y.detach().numpy())\n",
    "    test_y_cp = scale.inverse_transform(test_y.detach().numpy())\n",
    "    prediction_val = scale.inverse_transform(np.asarray(prediction_val).reshape(-1,1))\n",
    "\n",
    "    y_true = test_y_cp\n",
    "    y_pred = prediction_val\n",
    "\n",
    "    RMSE = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    print(f'RMSE: {RMSE}')\n",
    "\n",
    "    MAE = mean_absolute_error(y_true, y_pred)\n",
    "    print(f'MAE: {MAE}')\n",
    "\n",
    "    MAPE = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    print(f'MAPE: {MAPE}%')\n",
    "    \n",
    "    print(f'parametros: {p}')\n",
    "    \n",
    "    if RMSE < best_rmse:\n",
    "        best_rmse = RMSE\n",
    "        best_mae = MAE\n",
    "        best_mape = MAPE\n",
    "        best_prediction = y_pred\n",
    "        best_parameters = p\n",
    "    \n",
    "    model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n",
    "    count += 1\n",
    "    print(\"total: \" ,round(count/cnt,2)*100)\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"Tempo total de execução: {end_time - start_time} segundos\")\n",
    "parameters = model_parameters.sort_values(by=['RMSE'])\n",
    "parameters = parameters.reset_index(drop=True)\n",
    "print('RMSE:',parameters.loc[0, 'RMSE'])\n",
    "print('MAE:',parameters.loc[0, 'MAE'])\n",
    "print('MAPE:',parameters.loc[0, 'MAPE'])\n",
    "print(parameters.loc[0, 'Parameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "230a85b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 100, 'lr': 0.01, 'num_layers': 2}\n"
     ]
    }
   ],
   "source": [
    "print(parameters.loc[0, 'Parameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5cbe1aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 7.930554814887464\n",
      "MAE: 6.432557521841557\n",
      "MAPE: 5.564407815771348\n",
      "{'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 100, 'lr': 0.01, 'num_layers': 2}\n"
     ]
    }
   ],
   "source": [
    "parameters = model_parameters.sort_values(by=['MAE'])\n",
    "parameters = parameters.reset_index(drop=True)\n",
    "print('RMSE:',parameters.loc[0, 'RMSE'])\n",
    "print('MAE:',parameters.loc[0, 'MAE'])\n",
    "print('MAPE:',parameters.loc[0, 'MAPE'])\n",
    "print(parameters.loc[0, 'Parameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a849ce97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropout_rate': 0.5,\n",
       " 'epochs': 400,\n",
       " 'hid_size': 100,\n",
       " 'lr': 0.01,\n",
       " 'num_layers': 2}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "0110bf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 7.930554814887464\n",
      "MAE: 6.432557521841557\n",
      "MAPE: 5.564407815771348\n",
      "{'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 100, 'lr': 0.01, 'num_layers': 2}\n"
     ]
    }
   ],
   "source": [
    "print('RMSE:',parameters.loc[0, 'RMSE'])\n",
    "print('MAE:',parameters.loc[0, 'MAE'])\n",
    "print('MAPE:',parameters.loc[0, 'MAPE'])\n",
    "print(parameters.loc[0, 'Parameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "65b5c6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "treino_sem_pand = pd.concat([treino1,dados_pand,treino2])\n",
    "df_3 = pd.concat([treino_sem_pand, teste,previsao])\n",
    "df_treino=pd.concat([treino_sem_pand, teste])\n",
    "df_teste=pd.concat([previsao])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3cce8384",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,train_x,train_y,test_x,test_y = escalonar(df_3,df_treino, 10,scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "107cf107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.20070041716098785    Test Loss :0.6603044271469116\n",
      "Epoch :0.025    Train Loss :0.24863138794898987    Test Loss :0.5697527527809143\n",
      "Epoch :0.0375    Train Loss :0.04676124081015587    Test Loss :0.004481671378016472\n",
      "Epoch :0.05    Train Loss :0.052119310945272446    Test Loss :0.18505476415157318\n",
      "Epoch :0.0625    Train Loss :0.058802224695682526    Test Loss :0.16619820892810822\n",
      "Epoch :0.075    Train Loss :0.04228939861059189    Test Loss :0.04203690588474274\n",
      "Epoch :0.0875    Train Loss :0.035472095012664795    Test Loss :0.06026223301887512\n",
      "Epoch :0.1    Train Loss :0.025254011154174805    Test Loss :0.04066075384616852\n",
      "Epoch :0.1125    Train Loss :0.004586698021739721    Test Loss :0.011479837819933891\n",
      "Epoch :0.125    Train Loss :0.006622417829930782    Test Loss :0.0053498996421694756\n",
      "Epoch :0.1375    Train Loss :0.003034240799024701    Test Loss :0.005886945873498917\n",
      "Epoch :0.15    Train Loss :0.004072023555636406    Test Loss :0.021091721951961517\n",
      "Epoch :0.1625    Train Loss :0.002531813457608223    Test Loss :0.007849248126149178\n",
      "Epoch :0.175    Train Loss :0.0023150010965764523    Test Loss :0.0055412184447050095\n",
      "Epoch :0.1875    Train Loss :0.0016233326168730855    Test Loss :0.006553198676556349\n",
      "Epoch :0.2    Train Loss :0.0017831260338425636    Test Loss :0.004388675559312105\n",
      "Epoch :0.2125    Train Loss :0.0015006783651188016    Test Loss :0.003563892561942339\n",
      "Epoch :0.225    Train Loss :0.0013809920055791736    Test Loss :0.0034244609996676445\n",
      "Epoch :0.2375    Train Loss :0.0012309978483244777    Test Loss :0.003218354657292366\n",
      "Epoch :0.25    Train Loss :0.0012611401034519076    Test Loss :0.0030287164263427258\n",
      "Epoch :0.2625    Train Loss :0.0011122709838673472    Test Loss :0.003500827355310321\n",
      "Epoch :0.275    Train Loss :0.0010803518816828728    Test Loss :0.0023277653381228447\n",
      "Epoch :0.2875    Train Loss :0.001058401190675795    Test Loss :0.0022487386595457792\n",
      "Epoch :0.3    Train Loss :0.0010354312835261226    Test Loss :0.002178809605538845\n",
      "Epoch :0.3125    Train Loss :0.001007082755677402    Test Loss :0.0019341142615303397\n",
      "Epoch :0.325    Train Loss :0.0009550324757583439    Test Loss :0.0013645848957821727\n",
      "Epoch :0.3375    Train Loss :0.0009415462845936418    Test Loss :0.0013618055963888764\n",
      "Epoch :0.35    Train Loss :0.0008714005816727877    Test Loss :0.001206185668706894\n",
      "Epoch :0.3625    Train Loss :0.0008997935219667852    Test Loss :0.0014898409135639668\n",
      "Epoch :0.375    Train Loss :0.0008347253315150738    Test Loss :0.0015970133244991302\n",
      "Epoch :0.3875    Train Loss :0.0008586966432631016    Test Loss :0.001551921828649938\n",
      "Epoch :0.4    Train Loss :0.0008081597625277936    Test Loss :0.0013291890500113368\n",
      "Epoch :0.4125    Train Loss :0.0008560087298974395    Test Loss :0.0013723853044211864\n",
      "Epoch :0.425    Train Loss :0.0008191161905415356    Test Loss :0.0011307488894090056\n",
      "Epoch :0.4375    Train Loss :0.0007502336520701647    Test Loss :0.001116679864935577\n",
      "Epoch :0.45    Train Loss :0.0007845859508961439    Test Loss :0.0008228146471083164\n",
      "Epoch :0.4625    Train Loss :0.0007511994335800409    Test Loss :0.0010362021857872605\n",
      "Epoch :0.475    Train Loss :0.0007419018656946719    Test Loss :0.0009762560948729515\n",
      "Epoch :0.4875    Train Loss :0.000737859692890197    Test Loss :0.0011202157475054264\n",
      "Epoch :0.5    Train Loss :0.0007092321175150573    Test Loss :0.0008817811030894518\n",
      "Epoch :0.5125    Train Loss :0.0006851564394310117    Test Loss :0.0011000691447407007\n",
      "Epoch :0.525    Train Loss :0.0007175829960033298    Test Loss :0.001094921724870801\n",
      "Epoch :0.5375    Train Loss :0.0006987899541854858    Test Loss :0.00117965554818511\n",
      "Epoch :0.55    Train Loss :0.0006948508089408278    Test Loss :0.0007884102524258196\n",
      "Epoch :0.5625    Train Loss :0.0006885633338242769    Test Loss :0.0009040725417435169\n",
      "Epoch :0.575    Train Loss :0.0006754744681529701    Test Loss :0.0009895621333271265\n",
      "Epoch :0.5875    Train Loss :0.0006606963579542935    Test Loss :0.0010380134917795658\n",
      "Epoch :0.6    Train Loss :0.0006635879981331527    Test Loss :0.0008250097744166851\n",
      "Epoch :0.6125    Train Loss :0.0006325179128907621    Test Loss :0.0007705636671744287\n",
      "Epoch :0.625    Train Loss :0.0006543404306285083    Test Loss :0.0006647725822404027\n",
      "Epoch :0.6375    Train Loss :0.0006624130182899535    Test Loss :0.0006800241535529494\n",
      "Epoch :0.65    Train Loss :0.000643753563053906    Test Loss :0.0007747269119136035\n",
      "Epoch :0.6625    Train Loss :0.0006313039339147508    Test Loss :0.0006724356207996607\n",
      "Epoch :0.675    Train Loss :0.0006374211516231298    Test Loss :0.000666479580104351\n",
      "Epoch :0.6875    Train Loss :0.000630958762485534    Test Loss :0.0007140316301956773\n",
      "Epoch :0.7    Train Loss :0.0006400484126061201    Test Loss :0.0007254302618093789\n",
      "Epoch :0.7125    Train Loss :0.000615095195826143    Test Loss :0.0008829468861222267\n",
      "Epoch :0.725    Train Loss :0.0006161869969218969    Test Loss :0.0005844450206495821\n",
      "Epoch :0.7375    Train Loss :0.0005728472024202347    Test Loss :0.0006394402007572353\n",
      "Epoch :0.75    Train Loss :0.0006009065546095371    Test Loss :0.0005893161287531257\n",
      "Epoch :0.7625    Train Loss :0.0005911835469305515    Test Loss :0.000592484138906002\n",
      "Epoch :0.775    Train Loss :0.0006179793854244053    Test Loss :0.000631084549240768\n",
      "Epoch :0.7875    Train Loss :0.0005936305387876928    Test Loss :0.0006707169814035296\n",
      "Epoch :0.8    Train Loss :0.0005981000722385943    Test Loss :0.0007287852349691093\n",
      "Epoch :0.8125    Train Loss :0.0006003594608046114    Test Loss :0.0006153485155664384\n",
      "Epoch :0.825    Train Loss :0.0005489405011758208    Test Loss :0.0006095837452448905\n",
      "Epoch :0.8375    Train Loss :0.0005618579452857375    Test Loss :0.0005222409381531179\n",
      "Epoch :0.85    Train Loss :0.0005744872032664716    Test Loss :0.0006181533681228757\n",
      "Epoch :0.8625    Train Loss :0.0005507096648216248    Test Loss :0.0006406594184227288\n",
      "Epoch :0.875    Train Loss :0.0005437567597255111    Test Loss :0.0005643649492412806\n",
      "Epoch :0.8875    Train Loss :0.000564896094147116    Test Loss :0.0005332424188964069\n",
      "Epoch :0.9    Train Loss :0.0005410936428233981    Test Loss :0.0005580023862421513\n",
      "Epoch :0.9125    Train Loss :0.0005430292221717536    Test Loss :0.0005693708662874997\n",
      "Epoch :0.925    Train Loss :0.0005363775417208672    Test Loss :0.000547652191016823\n",
      "Epoch :0.9375    Train Loss :0.0005277562304399908    Test Loss :0.0005439515225589275\n",
      "Epoch :0.95    Train Loss :0.0005433788755908608    Test Loss :0.0005428142612800002\n",
      "Epoch :0.9625    Train Loss :0.0005311183631420135    Test Loss :0.0005941833951510489\n",
      "Epoch :0.975    Train Loss :0.0005502850981429219    Test Loss :0.000549245742149651\n",
      "Epoch :0.9875    Train Loss :0.0005354535533115268    Test Loss :0.0004861121124122292\n",
      "Epoch :1.0    Train Loss :0.0005255812429822981    Test Loss :0.000537585758138448\n",
      "RMSE: 5.996429566457203\n",
      "MAE: 4.983255401048338\n",
      "MAPE: 3.908982108062996%\n",
      "{'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 100, 'lr': 0.01, 'num_layers': 2}\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "lstm = LSTM(in_dim = x.shape[-1],\n",
    "            hid_dim = best_parameters['hid_size'],\n",
    "            out_dim = x.shape[-1],\n",
    "            num_layers =best_parameters['num_layers'], \n",
    "            dropout_rate= best_parameters['dropout_rate'])\n",
    "\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=best_parameters['lr'])\n",
    "\n",
    "loss_fun = nn.MSELoss()\n",
    "\n",
    "train_loss, test_loss = train_model(lstm,\n",
    "           loss_fun,\n",
    "           optimizer,\n",
    "           train_x,\n",
    "           test_x,\n",
    "           train_y,\n",
    "           test_y,\n",
    "           epochs=best_parameters['epochs'])\n",
    "\n",
    "\n",
    "# testing the predction model on multiple time series\n",
    "last_x = train_x[-1].view(entradas)\n",
    "\n",
    "prediction_val = []\n",
    "\n",
    "while len(prediction_val)<len(test_y):\n",
    "    prediction = lstm(last_x.view(1,entradas,1))\n",
    "    prediction_val.append(prediction[0,0].item())\n",
    "\n",
    "\n",
    "    ## replace the predicted value in last x\n",
    "    last_x = torch.cat((last_x[1:],prediction[0]))\n",
    "\n",
    "# plot the result\n",
    "train_y_cp = scale.inverse_transform(train_y.detach().numpy())\n",
    "test_y_cp = scale.inverse_transform(test_y.detach().numpy())\n",
    "prediction_val = scale.inverse_transform(np.asarray(prediction_val).reshape(-1,1))\n",
    "\n",
    "y_true = test_y_cp\n",
    "y_pred = prediction_val\n",
    "\n",
    "RMSE = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "print(f'RMSE: {RMSE}')\n",
    "\n",
    "MAE = mean_absolute_error(y_true, y_pred)\n",
    "print(f'MAE: {MAE}')\n",
    "\n",
    "MAPE = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "print(f'MAPE: {MAPE}%')\n",
    "\n",
    "print(best_parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "65b8c55a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1900"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(treino_sem_pand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "22342095",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotando os resultados\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Linha da série temporal\n",
    "plt.plot(treino_sem_pand.index, treino_sem_pand['valor'],color='blue', label='Dados de treino')\n",
    "plt.plot(teste.index, teste['valor'],color='green', label='Dados de validação')\n",
    "plt.plot(previsao.index, previsao['valor'],color='red', label='Dados de teste')\n",
    "\n",
    "\n",
    "# Linha das previsões\n",
    "#plt.plot(prev_teste, label='Previsões testes', color='orange')\n",
    "plt.plot(teste.index[3:],best_prediction,label='Previsões de validação',color = 'orange')\n",
    "plt.plot(previsao.index,prediction_val,label='Previsões de teste',color = 'cyan')\n",
    "\n",
    "plt.ylabel('Valores', fontsize=12)\n",
    "plt.xlabel('Datas', fontsize=12)\n",
    "plt.title('Índice Ibovespa diário previsto com o modelo LSTM')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "18411638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotando os resultados\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Linha da série temporal\n",
    "#plt.plot(treino_sem_pand.index, treino_sem_pand['valor'],color='blue', label='Dados de treino')\n",
    "plt.plot(teste.index, teste['valor'],color='green', label='Dados de validação')\n",
    "plt.plot(previsao.index, previsao['valor'],color='red', label='Dados de teste')\n",
    "\n",
    "\n",
    "# Linha das previsões\n",
    "#plt.plot(prev_teste, label='Previsões testes', color='orange')\n",
    "plt.plot(teste.index[3:],best_prediction,label='Previsões de validação',color = 'orange')\n",
    "plt.plot(previsao.index,prediction_val,label='Previsões de teste',color = 'cyan')\n",
    "\n",
    "plt.ylabel('Valores', fontsize=12)\n",
    "plt.xlabel('Datas', fontsize=12)\n",
    "plt.title('Índice Ibovespa diário previsto com o modelo LSTM')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5f3a1a",
   "metadata": {},
   "source": [
    "# Série com dados mensais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d236a09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "treino_mensal = treino.copy()\n",
    "teste_mensal = teste.copy()\n",
    "previsao_mensal = previsao.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "33431a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "treino_mensal = treino_mensal['valor'].resample('M').mean()\n",
    "teste_mensal = teste_mensal['valor'].resample('M').mean()\n",
    "previsao_mensal = previsao_mensal['valor'].resample('M').mean()\n",
    "data = treino_mensal.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "25b71934",
   "metadata": {},
   "outputs": [],
   "source": [
    "treino_mensal = treino_mensal.to_frame(name='valor')\n",
    "teste_mensal = teste_mensal.to_frame(name='valor')\n",
    "previsao_mensal = previsao_mensal.to_frame(name='valor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c9574e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "#suavizado = pd.concat([treino_mensal,teste_mensal, previsao_mensal])\n",
    "##Monthly Stock Price \n",
    "# Plotando os resultados\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Linha da série temporal\n",
    "plt.plot(treino_mensal,color='blue', label='Dados de treino')\n",
    "plt.plot(teste_mensal,color='green', label='Dados de treino')\n",
    "plt.plot(previsao_mensal,color='red', label='Dados futuros reais')\n",
    "\n",
    "\n",
    "\n",
    "plt.ylabel('Valores', fontsize=12)\n",
    "plt.xlabel('Datas', fontsize=12)\n",
    "plt.title('Índice Ibovespa mensal de 2010 a 2024')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b4f9982a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_4 = pd.concat([treino_mensal, teste_mensal])\n",
    "df_treino = treino_mensal\n",
    "df_teste=pd.concat([teste_mensal])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "54c83eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "entradas = 10\n",
    "x,y,train_x,train_y,test_x,test_y = escalonar(df_4,df_treino, entradas,scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f9b4ecd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Possible Models 32\n",
      "Epoch :0.0125    Train Loss :0.049805473536252975    Test Loss :0.234438955783844\n",
      "Epoch :0.025    Train Loss :0.02830740250647068    Test Loss :0.036422573029994965\n",
      "Epoch :0.0375    Train Loss :0.02343999221920967    Test Loss :0.02526174671947956\n",
      "Epoch :0.05    Train Loss :0.015164089389145374    Test Loss :0.019504882395267487\n",
      "Epoch :0.0625    Train Loss :0.010394592769443989    Test Loss :0.025270307436585426\n",
      "Epoch :0.075    Train Loss :0.008037928491830826    Test Loss :0.0161404088139534\n",
      "Epoch :0.0875    Train Loss :0.008632578887045383    Test Loss :0.007790144998580217\n",
      "Epoch :0.1    Train Loss :0.008902313187718391    Test Loss :0.017333034425973892\n",
      "Epoch :0.1125    Train Loss :0.0077653611078858376    Test Loss :0.012407667934894562\n",
      "Epoch :0.125    Train Loss :0.006672103423625231    Test Loss :0.006833901163190603\n",
      "Epoch :0.1375    Train Loss :0.005948383826762438    Test Loss :0.013591310009360313\n",
      "Epoch :0.15    Train Loss :0.006238238885998726    Test Loss :0.0053536975756287575\n",
      "Epoch :0.1625    Train Loss :0.006335088983178139    Test Loss :0.007926113903522491\n",
      "Epoch :0.175    Train Loss :0.006048667710274458    Test Loss :0.010568652302026749\n",
      "Epoch :0.1875    Train Loss :0.005721991416066885    Test Loss :0.00799608789384365\n",
      "Epoch :0.2    Train Loss :0.005403921473771334    Test Loss :0.01115247793495655\n",
      "Epoch :0.2125    Train Loss :0.006010236218571663    Test Loss :0.01375171635299921\n",
      "Epoch :0.225    Train Loss :0.004989856854081154    Test Loss :0.01090219896286726\n",
      "Epoch :0.2375    Train Loss :0.005251036956906319    Test Loss :0.012390920892357826\n",
      "Epoch :0.25    Train Loss :0.005046446342021227    Test Loss :0.010051023215055466\n",
      "Epoch :0.2625    Train Loss :0.004306027200073004    Test Loss :0.01561220083385706\n",
      "Epoch :0.275    Train Loss :0.005017969757318497    Test Loss :0.015025723725557327\n",
      "Epoch :0.2875    Train Loss :0.004007892217487097    Test Loss :0.021410489454865456\n",
      "Epoch :0.3    Train Loss :0.00424814922735095    Test Loss :0.020264610648155212\n",
      "Epoch :0.3125    Train Loss :0.00430303905159235    Test Loss :0.01782643422484398\n",
      "Epoch :0.325    Train Loss :0.004551563877612352    Test Loss :0.013437838293612003\n",
      "Epoch :0.3375    Train Loss :0.004438482690602541    Test Loss :0.013162597082555294\n",
      "Epoch :0.35    Train Loss :0.0038689172361046076    Test Loss :0.016410889104008675\n",
      "Epoch :0.3625    Train Loss :0.00447627529501915    Test Loss :0.010572059080004692\n",
      "Epoch :0.375    Train Loss :0.0039414470084011555    Test Loss :0.013516457751393318\n",
      "Epoch :0.3875    Train Loss :0.00405412632972002    Test Loss :0.0178753100335598\n",
      "Epoch :0.4    Train Loss :0.003432329511269927    Test Loss :0.018432805314660072\n",
      "Epoch :0.4125    Train Loss :0.0028879959136247635    Test Loss :0.014179668389260769\n",
      "Epoch :0.425    Train Loss :0.0030256162863224745    Test Loss :0.010542805306613445\n",
      "Epoch :0.4375    Train Loss :0.003424441209062934    Test Loss :0.015151361003518105\n",
      "Epoch :0.45    Train Loss :0.003857684088870883    Test Loss :0.01565348356962204\n",
      "Epoch :0.4625    Train Loss :0.0028015724383294582    Test Loss :0.015758654102683067\n",
      "Epoch :0.475    Train Loss :0.0024112616665661335    Test Loss :0.012569420039653778\n",
      "Epoch :0.4875    Train Loss :0.002869310788810253    Test Loss :0.009387247264385223\n",
      "Epoch :0.5    Train Loss :0.0027479606214910746    Test Loss :0.012621075846254826\n",
      "Epoch :0.5125    Train Loss :0.002615345409139991    Test Loss :0.011926175095140934\n",
      "Epoch :0.525    Train Loss :0.002392371417954564    Test Loss :0.01274689007550478\n",
      "Epoch :0.5375    Train Loss :0.0028894206043332815    Test Loss :0.007950959727168083\n",
      "Epoch :0.55    Train Loss :0.002747500315308571    Test Loss :0.009308029897511005\n",
      "Epoch :0.5625    Train Loss :0.0030733307357877493    Test Loss :0.01231866143643856\n",
      "Epoch :0.575    Train Loss :0.002688891487196088    Test Loss :0.005350571591407061\n",
      "Epoch :0.5875    Train Loss :0.0028133192099630833    Test Loss :0.01349237933754921\n",
      "Epoch :0.6    Train Loss :0.002611754462122917    Test Loss :0.01152500044554472\n",
      "Epoch :0.6125    Train Loss :0.002951675793156028    Test Loss :0.021944647654891014\n",
      "Epoch :0.625    Train Loss :0.002540980000048876    Test Loss :0.010784141719341278\n",
      "Epoch :0.6375    Train Loss :0.002706061815842986    Test Loss :0.019535688683390617\n",
      "Epoch :0.65    Train Loss :0.0030445000156760216    Test Loss :0.012398222461342812\n",
      "Epoch :0.6625    Train Loss :0.002611428266391158    Test Loss :0.008769622072577477\n",
      "Epoch :0.675    Train Loss :0.0027710520662367344    Test Loss :0.007236685138195753\n",
      "Epoch :0.6875    Train Loss :0.0029757600277662277    Test Loss :0.01019325852394104\n",
      "Epoch :0.7    Train Loss :0.0028715096414089203    Test Loss :0.01481686346232891\n",
      "Epoch :0.7125    Train Loss :0.002508657518774271    Test Loss :0.016347752884030342\n",
      "Epoch :0.725    Train Loss :0.0028387787751853466    Test Loss :0.010093389078974724\n",
      "Epoch :0.7375    Train Loss :0.002734503708779812    Test Loss :0.010282657109200954\n",
      "Epoch :0.75    Train Loss :0.002545254537835717    Test Loss :0.013811140321195126\n",
      "Epoch :0.7625    Train Loss :0.002185595454648137    Test Loss :0.014173585921525955\n",
      "Epoch :0.775    Train Loss :0.002561350120231509    Test Loss :0.00977640226483345\n",
      "Epoch :0.7875    Train Loss :0.0023401391226798296    Test Loss :0.008953861892223358\n",
      "Epoch :0.8    Train Loss :0.0025628237053751945    Test Loss :0.007319367956370115\n",
      "Epoch :0.8125    Train Loss :0.0021056849509477615    Test Loss :0.014697509817779064\n",
      "Epoch :0.825    Train Loss :0.0025284956209361553    Test Loss :0.01965407468378544\n",
      "Epoch :0.8375    Train Loss :0.002388610038906336    Test Loss :0.010218506678938866\n",
      "Epoch :0.85    Train Loss :0.002561473986133933    Test Loss :0.01065851654857397\n",
      "Epoch :0.8625    Train Loss :0.002247351221740246    Test Loss :0.0077021061442792416\n",
      "Epoch :0.875    Train Loss :0.002389719942584634    Test Loss :0.009276168420910835\n",
      "Epoch :0.8875    Train Loss :0.001871136948466301    Test Loss :0.009092342108488083\n",
      "Epoch :0.9    Train Loss :0.002101502614095807    Test Loss :0.01333695836365223\n",
      "Epoch :0.9125    Train Loss :0.002278844127431512    Test Loss :0.010478981770575047\n",
      "Epoch :0.925    Train Loss :0.002131178043782711    Test Loss :0.00861345324665308\n",
      "Epoch :0.9375    Train Loss :0.0021087718196213245    Test Loss :0.018090495839715004\n",
      "Epoch :0.95    Train Loss :0.00222548795863986    Test Loss :0.012517075054347515\n",
      "Epoch :0.9625    Train Loss :0.00192159169819206    Test Loss :0.017179446294903755\n",
      "Epoch :0.975    Train Loss :0.002007891424000263    Test Loss :0.01683117263019085\n",
      "Epoch :0.9875    Train Loss :0.002095707692205906    Test Loss :0.015640247613191605\n",
      "Epoch :1.0    Train Loss :0.002249942859634757    Test Loss :0.01575448364019394\n",
      "RMSE: 12.593759625420073\n",
      "MAE: 10.135777998248304\n",
      "MAPE: 8.832896190432054%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 50, 'lr': 0.01, 'num_layers': 2}\n",
      "total:  3.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.06188904121518135    Test Loss :0.39929354190826416\n",
      "Epoch :0.025    Train Loss :0.07022539526224136    Test Loss :0.3512488901615143\n",
      "Epoch :0.0375    Train Loss :0.05493953824043274    Test Loss :0.19879429042339325\n",
      "Epoch :0.05    Train Loss :0.052477072924375534    Test Loss :0.26328331232070923\n",
      "Epoch :0.0625    Train Loss :0.05113200098276138    Test Loss :0.2256251573562622\n",
      "Epoch :0.075    Train Loss :0.04903078451752663    Test Loss :0.18606093525886536\n",
      "Epoch :0.0875    Train Loss :0.023908888921141624    Test Loss :0.041824791580438614\n",
      "Epoch :0.1    Train Loss :0.034176889806985855    Test Loss :0.03633291646838188\n",
      "Epoch :0.1125    Train Loss :0.02712727151811123    Test Loss :0.11715863645076752\n",
      "Epoch :0.125    Train Loss :0.025724606588482857    Test Loss :0.11281329393386841\n",
      "Epoch :0.1375    Train Loss :0.014270718209445477    Test Loss :0.03363218903541565\n",
      "Epoch :0.15    Train Loss :0.014115003868937492    Test Loss :0.010975184850394726\n",
      "Epoch :0.1625    Train Loss :0.013496083207428455    Test Loss :0.0425911620259285\n",
      "Epoch :0.175    Train Loss :0.012488163076341152    Test Loss :0.032741207629442215\n",
      "Epoch :0.1875    Train Loss :0.011099359020590782    Test Loss :0.022582469508051872\n",
      "Epoch :0.2    Train Loss :0.010206540115177631    Test Loss :0.01988145150244236\n",
      "Epoch :0.2125    Train Loss :0.009513730183243752    Test Loss :0.02337348833680153\n",
      "Epoch :0.225    Train Loss :0.009458725340664387    Test Loss :0.019386542961001396\n",
      "Epoch :0.2375    Train Loss :0.0081398393958807    Test Loss :0.01627478003501892\n",
      "Epoch :0.25    Train Loss :0.00757318502292037    Test Loss :0.017386656254529953\n",
      "Epoch :0.2625    Train Loss :0.007525267545133829    Test Loss :0.015178398229181767\n",
      "Epoch :0.275    Train Loss :0.006782808806747198    Test Loss :0.015358262695372105\n",
      "Epoch :0.2875    Train Loss :0.007812412455677986    Test Loss :0.013383042067289352\n",
      "Epoch :0.3    Train Loss :0.0073725394904613495    Test Loss :0.01755121722817421\n",
      "Epoch :0.3125    Train Loss :0.006434971932321787    Test Loss :0.015721887350082397\n",
      "Epoch :0.325    Train Loss :0.006468364968895912    Test Loss :0.016047509387135506\n",
      "Epoch :0.3375    Train Loss :0.0064498865976929665    Test Loss :0.013504072092473507\n",
      "Epoch :0.35    Train Loss :0.006254686042666435    Test Loss :0.017560172826051712\n",
      "Epoch :0.3625    Train Loss :0.006114739924669266    Test Loss :0.01735452562570572\n",
      "Epoch :0.375    Train Loss :0.005554645787924528    Test Loss :0.016048787161707878\n",
      "Epoch :0.3875    Train Loss :0.0065389471128582954    Test Loss :0.015658188611268997\n",
      "Epoch :0.4    Train Loss :0.005689704325050116    Test Loss :0.0194710586220026\n",
      "Epoch :0.4125    Train Loss :0.005628194659948349    Test Loss :0.015128527767956257\n",
      "Epoch :0.425    Train Loss :0.0051790825091302395    Test Loss :0.01727895252406597\n",
      "Epoch :0.4375    Train Loss :0.005665156524628401    Test Loss :0.018363602459430695\n",
      "Epoch :0.45    Train Loss :0.005367443896830082    Test Loss :0.019809631630778313\n",
      "Epoch :0.4625    Train Loss :0.004380315076559782    Test Loss :0.03252289444208145\n",
      "Epoch :0.475    Train Loss :0.004545262549072504    Test Loss :0.010980671271681786\n",
      "Epoch :0.4875    Train Loss :0.0048145693726837635    Test Loss :0.025268737226724625\n",
      "Epoch :0.5    Train Loss :0.004839546978473663    Test Loss :0.020787222310900688\n",
      "Epoch :0.5125    Train Loss :0.00528029166162014    Test Loss :0.02313949167728424\n",
      "Epoch :0.525    Train Loss :0.0038794761057943106    Test Loss :0.020105909556150436\n",
      "Epoch :0.5375    Train Loss :0.005266769789159298    Test Loss :0.02153930999338627\n",
      "Epoch :0.55    Train Loss :0.004471377469599247    Test Loss :0.02054974064230919\n",
      "Epoch :0.5625    Train Loss :0.004164631012827158    Test Loss :0.02072613313794136\n",
      "Epoch :0.575    Train Loss :0.004581538029015064    Test Loss :0.010417698882520199\n",
      "Epoch :0.5875    Train Loss :0.004284929018467665    Test Loss :0.013261576183140278\n",
      "Epoch :0.6    Train Loss :0.0033460429403930902    Test Loss :0.026658151298761368\n",
      "Epoch :0.6125    Train Loss :0.0033847291488200426    Test Loss :0.018158240243792534\n",
      "Epoch :0.625    Train Loss :0.003943059127777815    Test Loss :0.022137045860290527\n",
      "Epoch :0.6375    Train Loss :0.0033841312397271395    Test Loss :0.013816727325320244\n",
      "Epoch :0.65    Train Loss :0.0030523331370204687    Test Loss :0.017419232055544853\n",
      "Epoch :0.6625    Train Loss :0.004377834498882294    Test Loss :0.017283089458942413\n",
      "Epoch :0.675    Train Loss :0.0038939386140555143    Test Loss :0.008339974097907543\n",
      "Epoch :0.6875    Train Loss :0.003359949914738536    Test Loss :0.02529493160545826\n",
      "Epoch :0.7    Train Loss :0.0030027260072529316    Test Loss :0.008438367396593094\n",
      "Epoch :0.7125    Train Loss :0.004123327322304249    Test Loss :0.016339100897312164\n",
      "Epoch :0.725    Train Loss :0.0036217281594872475    Test Loss :0.007336791604757309\n",
      "Epoch :0.7375    Train Loss :0.002547664800658822    Test Loss :0.026815572753548622\n",
      "Epoch :0.75    Train Loss :0.0034919800236821175    Test Loss :0.012518439441919327\n",
      "Epoch :0.7625    Train Loss :0.0029114149510860443    Test Loss :0.02880406565964222\n",
      "Epoch :0.775    Train Loss :0.0029603210277855396    Test Loss :0.02681790664792061\n",
      "Epoch :0.7875    Train Loss :0.0028894436545670033    Test Loss :0.05933171510696411\n",
      "Epoch :0.8    Train Loss :0.0033252236898988485    Test Loss :0.020812276750802994\n",
      "Epoch :0.8125    Train Loss :0.0029490760061889887    Test Loss :0.017309632152318954\n",
      "Epoch :0.825    Train Loss :0.003242817474529147    Test Loss :0.012004651129245758\n",
      "Epoch :0.8375    Train Loss :0.0028925566002726555    Test Loss :0.018142549321055412\n",
      "Epoch :0.85    Train Loss :0.00208724825643003    Test Loss :0.010933334939181805\n",
      "Epoch :0.8625    Train Loss :0.0034390646032989025    Test Loss :0.10114117711782455\n",
      "Epoch :0.875    Train Loss :0.0035920199006795883    Test Loss :0.013784605078399181\n",
      "Epoch :0.8875    Train Loss :0.003544384613633156    Test Loss :0.06117649003863335\n",
      "Epoch :0.9    Train Loss :0.0032255116384476423    Test Loss :0.034914351999759674\n",
      "Epoch :0.9125    Train Loss :0.00340792303904891    Test Loss :0.04570867866277695\n",
      "Epoch :0.925    Train Loss :0.0023155969101935625    Test Loss :0.03330769017338753\n",
      "Epoch :0.9375    Train Loss :0.0024197036400437355    Test Loss :0.03770437464118004\n",
      "Epoch :0.95    Train Loss :0.003013388253748417    Test Loss :0.02521693892776966\n",
      "Epoch :0.9625    Train Loss :0.0019900714978575706    Test Loss :0.014970667660236359\n",
      "Epoch :0.975    Train Loss :0.003394589526578784    Test Loss :0.034581851214170456\n",
      "Epoch :0.9875    Train Loss :0.0027520358562469482    Test Loss :0.014779210090637207\n",
      "Epoch :1.0    Train Loss :0.002367729786783457    Test Loss :0.037663109600543976\n",
      "RMSE: 15.860215266857946\n",
      "MAE: 11.438373539460468\n",
      "MAPE: 9.789405764825961%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 50, 'lr': 0.01, 'num_layers': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  6.0\n",
      "Epoch :0.0125    Train Loss :0.12947821617126465    Test Loss :0.6533507704734802\n",
      "Epoch :0.025    Train Loss :0.1118948683142662    Test Loss :0.4482984244823456\n",
      "Epoch :0.0375    Train Loss :0.059764593839645386    Test Loss :0.1574867218732834\n",
      "Epoch :0.05    Train Loss :0.05313997343182564    Test Loss :0.2857801914215088\n",
      "Epoch :0.0625    Train Loss :0.04429114982485771    Test Loss :0.1869039237499237\n",
      "Epoch :0.075    Train Loss :0.04276416078209877    Test Loss :0.11557542532682419\n",
      "Epoch :0.0875    Train Loss :0.029515201225876808    Test Loss :0.05246329680085182\n",
      "Epoch :0.1    Train Loss :0.014026314951479435    Test Loss :0.027788136154413223\n",
      "Epoch :0.1125    Train Loss :0.013021453283727169    Test Loss :0.02297300100326538\n",
      "Epoch :0.125    Train Loss :0.011465710587799549    Test Loss :0.048903223127126694\n",
      "Epoch :0.1375    Train Loss :0.010115408338606358    Test Loss :0.013808473013341427\n",
      "Epoch :0.15    Train Loss :0.007930385880172253    Test Loss :0.011081649921834469\n",
      "Epoch :0.1625    Train Loss :0.0068337456323206425    Test Loss :0.017348462715744972\n",
      "Epoch :0.175    Train Loss :0.006850890815258026    Test Loss :0.008956735953688622\n",
      "Epoch :0.1875    Train Loss :0.006974503863602877    Test Loss :0.009588547050952911\n",
      "Epoch :0.2    Train Loss :0.005534523166716099    Test Loss :0.01178393792361021\n",
      "Epoch :0.2125    Train Loss :0.006154168862849474    Test Loss :0.01735861971974373\n",
      "Epoch :0.225    Train Loss :0.006226417142897844    Test Loss :0.008683047257363796\n",
      "Epoch :0.2375    Train Loss :0.006086333189159632    Test Loss :0.01489478163421154\n",
      "Epoch :0.25    Train Loss :0.005449917633086443    Test Loss :0.015735071152448654\n",
      "Epoch :0.2625    Train Loss :0.005754238460212946    Test Loss :0.011484408751130104\n",
      "Epoch :0.275    Train Loss :0.004958292469382286    Test Loss :0.017338868230581284\n",
      "Epoch :0.2875    Train Loss :0.005021023564040661    Test Loss :0.011905859224498272\n",
      "Epoch :0.3    Train Loss :0.004907745402306318    Test Loss :0.012225649319589138\n",
      "Epoch :0.3125    Train Loss :0.0051920670084655285    Test Loss :0.011977522633969784\n",
      "Epoch :0.325    Train Loss :0.005596701055765152    Test Loss :0.010848801583051682\n",
      "Epoch :0.3375    Train Loss :0.00479229586198926    Test Loss :0.008635491132736206\n",
      "Epoch :0.35    Train Loss :0.005577465053647757    Test Loss :0.017565224319696426\n",
      "Epoch :0.3625    Train Loss :0.004436097107827663    Test Loss :0.016571376472711563\n",
      "Epoch :0.375    Train Loss :0.003946186043322086    Test Loss :0.012894134037196636\n",
      "Epoch :0.3875    Train Loss :0.004414954222738743    Test Loss :0.01442019734531641\n",
      "Epoch :0.4    Train Loss :0.003409651340916753    Test Loss :0.011368653737008572\n",
      "Epoch :0.4125    Train Loss :0.0035005356185138226    Test Loss :0.00993678905069828\n",
      "Epoch :0.425    Train Loss :0.00492485286667943    Test Loss :0.012629084289073944\n",
      "Epoch :0.4375    Train Loss :0.004032684490084648    Test Loss :0.014560641720890999\n",
      "Epoch :0.45    Train Loss :0.004082919098436832    Test Loss :0.017997173592448235\n",
      "Epoch :0.4625    Train Loss :0.0038835767190903425    Test Loss :0.011410176753997803\n",
      "Epoch :0.475    Train Loss :0.00363141018897295    Test Loss :0.005921341013163328\n",
      "Epoch :0.4875    Train Loss :0.003554948838427663    Test Loss :0.007241690065711737\n",
      "Epoch :0.5    Train Loss :0.0030818884260952473    Test Loss :0.00920923613011837\n",
      "Epoch :0.5125    Train Loss :0.003217461984604597    Test Loss :0.011749468743801117\n",
      "Epoch :0.525    Train Loss :0.002998525509610772    Test Loss :0.0072311339899897575\n",
      "Epoch :0.5375    Train Loss :0.0029979508835822344    Test Loss :0.007198980078101158\n",
      "Epoch :0.55    Train Loss :0.0029184422455728054    Test Loss :0.007561841048300266\n",
      "Epoch :0.5625    Train Loss :0.003299101023003459    Test Loss :0.021146221086382866\n",
      "Epoch :0.575    Train Loss :0.0028642164543271065    Test Loss :0.008233827538788319\n",
      "Epoch :0.5875    Train Loss :0.0032792268320918083    Test Loss :0.011231528595089912\n",
      "Epoch :0.6    Train Loss :0.0034608650021255016    Test Loss :0.010095981881022453\n",
      "Epoch :0.6125    Train Loss :0.0028092998545616865    Test Loss :0.013179552741348743\n",
      "Epoch :0.625    Train Loss :0.0030795668717473745    Test Loss :0.01414533145725727\n",
      "Epoch :0.6375    Train Loss :0.0039063552394509315    Test Loss :0.008497662842273712\n",
      "Epoch :0.65    Train Loss :0.003059736452996731    Test Loss :0.017744211480021477\n",
      "Epoch :0.6625    Train Loss :0.003467160975560546    Test Loss :0.012542322278022766\n",
      "Epoch :0.675    Train Loss :0.0033973127137869596    Test Loss :0.01381751149892807\n",
      "Epoch :0.6875    Train Loss :0.0029994596261531115    Test Loss :0.01099343877285719\n",
      "Epoch :0.7    Train Loss :0.0033013666979968548    Test Loss :0.014171479269862175\n",
      "Epoch :0.7125    Train Loss :0.0026772215496748686    Test Loss :0.015443476848304272\n",
      "Epoch :0.725    Train Loss :0.003199430648237467    Test Loss :0.01899527572095394\n",
      "Epoch :0.7375    Train Loss :0.00280241877771914    Test Loss :0.017525123432278633\n",
      "Epoch :0.75    Train Loss :0.0030021972488611937    Test Loss :0.016945626586675644\n",
      "Epoch :0.7625    Train Loss :0.003179093124344945    Test Loss :0.00916275568306446\n",
      "Epoch :0.775    Train Loss :0.0027541283052414656    Test Loss :0.007685489486902952\n",
      "Epoch :0.7875    Train Loss :0.0023268035147339106    Test Loss :0.009642096236348152\n",
      "Epoch :0.8    Train Loss :0.0029342228081077337    Test Loss :0.009664542973041534\n",
      "Epoch :0.8125    Train Loss :0.002851693658158183    Test Loss :0.016465533524751663\n",
      "Epoch :0.825    Train Loss :0.002486405661329627    Test Loss :0.01514502614736557\n",
      "Epoch :0.8375    Train Loss :0.0023907122667878866    Test Loss :0.019150760024785995\n",
      "Epoch :0.85    Train Loss :0.0026293883565813303    Test Loss :0.02161588706076145\n",
      "Epoch :0.8625    Train Loss :0.002933691954240203    Test Loss :0.016306672245264053\n",
      "Epoch :0.875    Train Loss :0.0029311871621757746    Test Loss :0.013291324488818645\n",
      "Epoch :0.8875    Train Loss :0.0025345853064209223    Test Loss :0.013165030628442764\n",
      "Epoch :0.9    Train Loss :0.0022006630897521973    Test Loss :0.013612189330160618\n",
      "Epoch :0.9125    Train Loss :0.0026578730903565884    Test Loss :0.023682545870542526\n",
      "Epoch :0.925    Train Loss :0.0024736563209444284    Test Loss :0.010510335676372051\n",
      "Epoch :0.9375    Train Loss :0.002910380018875003    Test Loss :0.027437705546617508\n",
      "Epoch :0.95    Train Loss :0.0025513553991913795    Test Loss :0.022742388769984245\n",
      "Epoch :0.9625    Train Loss :0.0024759278167039156    Test Loss :0.024588273838162422\n",
      "Epoch :0.975    Train Loss :0.002556433202698827    Test Loss :0.02325655147433281\n",
      "Epoch :0.9875    Train Loss :0.0022425546776503325    Test Loss :0.01772937923669815\n",
      "Epoch :1.0    Train Loss :0.0024061715230345726    Test Loss :0.019421415403485298\n",
      "RMSE: 17.109127058406102\n",
      "MAE: 14.057656838498293\n",
      "MAPE: 12.091251333256638%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 50, 'lr': 0.02, 'num_layers': 2}\n",
      "total:  9.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.05199199914932251    Test Loss :0.2853277027606964\n",
      "Epoch :0.025    Train Loss :0.05311862751841545    Test Loss :0.17523357272148132\n",
      "Epoch :0.0375    Train Loss :0.05836329236626625    Test Loss :0.2828928232192993\n",
      "Epoch :0.05    Train Loss :0.05083814635872841    Test Loss :0.26011335849761963\n",
      "Epoch :0.0625    Train Loss :0.051293499767780304    Test Loss :0.21590667963027954\n",
      "Epoch :0.075    Train Loss :0.05134906619787216    Test Loss :0.2475423663854599\n",
      "Epoch :0.0875    Train Loss :0.05065809190273285    Test Loss :0.22067733108997345\n",
      "Epoch :0.1    Train Loss :0.05242030322551727    Test Loss :0.24209225177764893\n",
      "Epoch :0.1125    Train Loss :0.0514778196811676    Test Loss :0.22729811072349548\n",
      "Epoch :0.125    Train Loss :0.052388474345207214    Test Loss :0.24537023901939392\n",
      "Epoch :0.1375    Train Loss :0.05232171714305878    Test Loss :0.22340571880340576\n",
      "Epoch :0.15    Train Loss :0.0516757071018219    Test Loss :0.23480038344860077\n",
      "Epoch :0.1625    Train Loss :0.05003669112920761    Test Loss :0.22063741087913513\n",
      "Epoch :0.175    Train Loss :0.02850654348731041    Test Loss :0.07568272203207016\n",
      "Epoch :0.1875    Train Loss :0.040724337100982666    Test Loss :0.1478142887353897\n",
      "Epoch :0.2    Train Loss :0.015809815376996994    Test Loss :0.041480034589767456\n",
      "Epoch :0.2125    Train Loss :0.015425194054841995    Test Loss :0.03453969210386276\n",
      "Epoch :0.225    Train Loss :0.013758046552538872    Test Loss :0.0341891348361969\n",
      "Epoch :0.2375    Train Loss :0.0124640678986907    Test Loss :0.027930011972784996\n",
      "Epoch :0.25    Train Loss :0.01306090783327818    Test Loss :0.030846230685710907\n",
      "Epoch :0.2625    Train Loss :0.01440108846873045    Test Loss :0.024212490767240524\n",
      "Epoch :0.275    Train Loss :0.013215688057243824    Test Loss :0.026093631982803345\n",
      "Epoch :0.2875    Train Loss :0.013260657899081707    Test Loss :0.03280485048890114\n",
      "Epoch :0.3    Train Loss :0.011951396241784096    Test Loss :0.03224651515483856\n",
      "Epoch :0.3125    Train Loss :0.011218689382076263    Test Loss :0.02171439491212368\n",
      "Epoch :0.325    Train Loss :0.011577651835978031    Test Loss :0.027004197239875793\n",
      "Epoch :0.3375    Train Loss :0.012028113007545471    Test Loss :0.02305283583700657\n",
      "Epoch :0.35    Train Loss :0.01033718977123499    Test Loss :0.018324190750718117\n",
      "Epoch :0.3625    Train Loss :0.010891108773648739    Test Loss :0.01578620821237564\n",
      "Epoch :0.375    Train Loss :0.011649910360574722    Test Loss :0.027882421389222145\n",
      "Epoch :0.3875    Train Loss :0.010659333318471909    Test Loss :0.02147633209824562\n",
      "Epoch :0.4    Train Loss :0.010069572366774082    Test Loss :0.02230769582092762\n",
      "Epoch :0.4125    Train Loss :0.009562370367348194    Test Loss :0.022273777052760124\n",
      "Epoch :0.425    Train Loss :0.008985591121017933    Test Loss :0.018828336149454117\n",
      "Epoch :0.4375    Train Loss :0.009070277214050293    Test Loss :0.021269425749778748\n",
      "Epoch :0.45    Train Loss :0.009018906392157078    Test Loss :0.020212195813655853\n",
      "Epoch :0.4625    Train Loss :0.01062544621527195    Test Loss :0.0236100722104311\n",
      "Epoch :0.475    Train Loss :0.008410031907260418    Test Loss :0.020301299169659615\n",
      "Epoch :0.4875    Train Loss :0.009919512085616589    Test Loss :0.028114495798945427\n",
      "Epoch :0.5    Train Loss :0.008562697097659111    Test Loss :0.02110658958554268\n",
      "Epoch :0.5125    Train Loss :0.00943788606673479    Test Loss :0.01811942085623741\n",
      "Epoch :0.525    Train Loss :0.008246484212577343    Test Loss :0.020636869594454765\n",
      "Epoch :0.5375    Train Loss :0.008984306827187538    Test Loss :0.021135345101356506\n",
      "Epoch :0.55    Train Loss :0.007703892420977354    Test Loss :0.017711328342556953\n",
      "Epoch :0.5625    Train Loss :0.00876571238040924    Test Loss :0.017746835947036743\n",
      "Epoch :0.575    Train Loss :0.008185560815036297    Test Loss :0.02207895554602146\n",
      "Epoch :0.5875    Train Loss :0.007502302993088961    Test Loss :0.019114967435598373\n",
      "Epoch :0.6    Train Loss :0.008217680267989635    Test Loss :0.02104802243411541\n",
      "Epoch :0.6125    Train Loss :0.008085448294878006    Test Loss :0.017266597598791122\n",
      "Epoch :0.625    Train Loss :0.00887813325971365    Test Loss :0.025757145136594772\n",
      "Epoch :0.6375    Train Loss :0.007312081288546324    Test Loss :0.022718699648976326\n",
      "Epoch :0.65    Train Loss :0.007739866152405739    Test Loss :0.01808604598045349\n",
      "Epoch :0.6625    Train Loss :0.007112302351742983    Test Loss :0.025269459933042526\n",
      "Epoch :0.675    Train Loss :0.006784094963222742    Test Loss :0.01252408605068922\n",
      "Epoch :0.6875    Train Loss :0.0070662652142345905    Test Loss :0.0238473042845726\n",
      "Epoch :0.7    Train Loss :0.006970558315515518    Test Loss :0.016857242211699486\n",
      "Epoch :0.7125    Train Loss :0.006895023863762617    Test Loss :0.015993064269423485\n",
      "Epoch :0.725    Train Loss :0.006726863328367472    Test Loss :0.016826046630740166\n",
      "Epoch :0.7375    Train Loss :0.006311460398137569    Test Loss :0.016275743022561073\n",
      "Epoch :0.75    Train Loss :0.006730772089213133    Test Loss :0.020205987617373466\n",
      "Epoch :0.7625    Train Loss :0.0054189772345125675    Test Loss :0.015491081401705742\n",
      "Epoch :0.775    Train Loss :0.005641593132168055    Test Loss :0.019144730642437935\n",
      "Epoch :0.7875    Train Loss :0.005495223216712475    Test Loss :0.01973314955830574\n",
      "Epoch :0.8    Train Loss :0.0053850207477808    Test Loss :0.01610715687274933\n",
      "Epoch :0.8125    Train Loss :0.005041840486228466    Test Loss :0.017993813380599022\n",
      "Epoch :0.825    Train Loss :0.005025387741625309    Test Loss :0.015647705644369125\n",
      "Epoch :0.8375    Train Loss :0.0045890407636761665    Test Loss :0.014623216353356838\n",
      "Epoch :0.85    Train Loss :0.005482175853103399    Test Loss :0.0156052615493536\n",
      "Epoch :0.8625    Train Loss :0.004827331285923719    Test Loss :0.01642605848610401\n",
      "Epoch :0.875    Train Loss :0.004664943087846041    Test Loss :0.015170201659202576\n",
      "Epoch :0.8875    Train Loss :0.0049880752339959145    Test Loss :0.017259741201996803\n",
      "Epoch :0.9    Train Loss :0.004977955017238855    Test Loss :0.01739380694925785\n",
      "Epoch :0.9125    Train Loss :0.004988448228687048    Test Loss :0.017153486609458923\n",
      "Epoch :0.925    Train Loss :0.004477143287658691    Test Loss :0.013862641528248787\n",
      "Epoch :0.9375    Train Loss :0.005055216606706381    Test Loss :0.0177492443472147\n",
      "Epoch :0.95    Train Loss :0.004512747284024954    Test Loss :0.019479433074593544\n",
      "Epoch :0.9625    Train Loss :0.004906964022666216    Test Loss :0.012297015637159348\n",
      "Epoch :0.975    Train Loss :0.004255218897014856    Test Loss :0.017096979543566704\n",
      "Epoch :0.9875    Train Loss :0.004902407061308622    Test Loss :0.017902106046676636\n",
      "Epoch :1.0    Train Loss :0.004379144869744778    Test Loss :0.016498923301696777\n",
      "RMSE: 11.536043972818774\n",
      "MAE: 9.473716206084744\n",
      "MAPE: 8.086527691796753%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 50, 'lr': 0.02, 'num_layers': 5}\n",
      "total:  12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.09629829227924347    Test Loss :0.5583103895187378\n",
      "Epoch :0.025    Train Loss :0.10525558143854141    Test Loss :0.4410208761692047\n",
      "Epoch :0.0375    Train Loss :0.06783805042505264    Test Loss :0.11266287416219711\n",
      "Epoch :0.05    Train Loss :0.047489017248153687    Test Loss :0.25130796432495117\n",
      "Epoch :0.0625    Train Loss :0.028482399880886078    Test Loss :0.02252383716404438\n",
      "Epoch :0.075    Train Loss :0.01946159265935421    Test Loss :0.020139561966061592\n",
      "Epoch :0.0875    Train Loss :0.01192039716988802    Test Loss :0.018891334533691406\n",
      "Epoch :0.1    Train Loss :0.011134328320622444    Test Loss :0.013884605839848518\n",
      "Epoch :0.1125    Train Loss :0.009321499615907669    Test Loss :0.024984385818243027\n",
      "Epoch :0.125    Train Loss :0.009094287641346455    Test Loss :0.013698790222406387\n",
      "Epoch :0.1375    Train Loss :0.008886842988431454    Test Loss :0.01778506301343441\n",
      "Epoch :0.15    Train Loss :0.0076447599567472935    Test Loss :0.01936444826424122\n",
      "Epoch :0.1625    Train Loss :0.006440514698624611    Test Loss :0.014191118068993092\n",
      "Epoch :0.175    Train Loss :0.006578520871698856    Test Loss :0.009626607410609722\n",
      "Epoch :0.1875    Train Loss :0.0065446458756923676    Test Loss :0.011614970862865448\n",
      "Epoch :0.2    Train Loss :0.006606067530810833    Test Loss :0.019539522007107735\n",
      "Epoch :0.2125    Train Loss :0.0058743953704833984    Test Loss :0.015327166765928268\n",
      "Epoch :0.225    Train Loss :0.00622990308329463    Test Loss :0.0181253794580698\n",
      "Epoch :0.2375    Train Loss :0.0063031623139977455    Test Loss :0.023962248116731644\n",
      "Epoch :0.25    Train Loss :0.005837813951075077    Test Loss :0.018272416666150093\n",
      "Epoch :0.2625    Train Loss :0.006084769498556852    Test Loss :0.020466312766075134\n",
      "Epoch :0.275    Train Loss :0.005252912174910307    Test Loss :0.03880338370800018\n",
      "Epoch :0.2875    Train Loss :0.0051746671088039875    Test Loss :0.03131778538227081\n",
      "Epoch :0.3    Train Loss :0.005180866923183203    Test Loss :0.051579736173152924\n",
      "Epoch :0.3125    Train Loss :0.00447972072288394    Test Loss :0.04977294057607651\n",
      "Epoch :0.325    Train Loss :0.004744737409055233    Test Loss :0.05812391638755798\n",
      "Epoch :0.3375    Train Loss :0.004186332691460848    Test Loss :0.09522130340337753\n",
      "Epoch :0.35    Train Loss :0.004231695085763931    Test Loss :0.0819351077079773\n",
      "Epoch :0.3625    Train Loss :0.004347036127001047    Test Loss :0.09462496638298035\n",
      "Epoch :0.375    Train Loss :0.0050535439513623714    Test Loss :0.07056061923503876\n",
      "Epoch :0.3875    Train Loss :0.004599664360284805    Test Loss :0.09495112299919128\n",
      "Epoch :0.4    Train Loss :0.00451771030202508    Test Loss :0.1058897078037262\n",
      "Epoch :0.4125    Train Loss :0.004074256867170334    Test Loss :0.09397224336862564\n",
      "Epoch :0.425    Train Loss :0.003974314779043198    Test Loss :0.10956135392189026\n",
      "Epoch :0.4375    Train Loss :0.0041940961964428425    Test Loss :0.12706200778484344\n",
      "Epoch :0.45    Train Loss :0.00381568749435246    Test Loss :0.14888261258602142\n",
      "Epoch :0.4625    Train Loss :0.004065543878823519    Test Loss :0.0925011858344078\n",
      "Epoch :0.475    Train Loss :0.0035331444814801216    Test Loss :0.10948843508958817\n",
      "Epoch :0.4875    Train Loss :0.003961923532187939    Test Loss :0.15320946276187897\n",
      "Epoch :0.5    Train Loss :0.0038148623425513506    Test Loss :0.10797157883644104\n",
      "Epoch :0.5125    Train Loss :0.004318310413509607    Test Loss :0.07426605373620987\n",
      "Epoch :0.525    Train Loss :0.004098910838365555    Test Loss :0.09105663746595383\n",
      "Epoch :0.5375    Train Loss :0.0032557060476392508    Test Loss :0.1296389400959015\n",
      "Epoch :0.55    Train Loss :0.00384029489941895    Test Loss :0.09742331504821777\n",
      "Epoch :0.5625    Train Loss :0.004460855852812529    Test Loss :0.11280782520771027\n",
      "Epoch :0.575    Train Loss :0.0033690771088004112    Test Loss :0.155536949634552\n",
      "Epoch :0.5875    Train Loss :0.002907334826886654    Test Loss :0.1212652251124382\n",
      "Epoch :0.6    Train Loss :0.003530470421537757    Test Loss :0.11555006355047226\n",
      "Epoch :0.6125    Train Loss :0.0028731022030115128    Test Loss :0.1299554854631424\n",
      "Epoch :0.625    Train Loss :0.0026640461292117834    Test Loss :0.1633252501487732\n",
      "Epoch :0.6375    Train Loss :0.003721043234691024    Test Loss :0.1541522741317749\n",
      "Epoch :0.65    Train Loss :0.0035067484714090824    Test Loss :0.05869651585817337\n",
      "Epoch :0.6625    Train Loss :0.0034022454638034105    Test Loss :0.0522586889564991\n",
      "Epoch :0.675    Train Loss :0.002875467063859105    Test Loss :0.1096625104546547\n",
      "Epoch :0.6875    Train Loss :0.0035562512930482626    Test Loss :0.062005799263715744\n",
      "Epoch :0.7    Train Loss :0.0032001652289181948    Test Loss :0.06788678467273712\n",
      "Epoch :0.7125    Train Loss :0.0026869659777730703    Test Loss :0.1085209771990776\n",
      "Epoch :0.725    Train Loss :0.0029501200187951326    Test Loss :0.041687846183776855\n",
      "Epoch :0.7375    Train Loss :0.0030761545058339834    Test Loss :0.09474800527095795\n",
      "Epoch :0.75    Train Loss :0.00238803680986166    Test Loss :0.06887893378734589\n",
      "Epoch :0.7625    Train Loss :0.0030877881217747927    Test Loss :0.07156980782747269\n",
      "Epoch :0.775    Train Loss :0.0028963396325707436    Test Loss :0.12581464648246765\n",
      "Epoch :0.7875    Train Loss :0.0025928979739546776    Test Loss :0.0914890319108963\n",
      "Epoch :0.8    Train Loss :0.002618884202092886    Test Loss :0.08743470162153244\n",
      "Epoch :0.8125    Train Loss :0.0025949697010219097    Test Loss :0.0844465047121048\n",
      "Epoch :0.825    Train Loss :0.0025679334066808224    Test Loss :0.1319369077682495\n",
      "Epoch :0.8375    Train Loss :0.0027262368239462376    Test Loss :0.11065696179866791\n",
      "Epoch :0.85    Train Loss :0.0026798099279403687    Test Loss :0.15426087379455566\n",
      "Epoch :0.8625    Train Loss :0.0028716286178678274    Test Loss :0.1361684650182724\n",
      "Epoch :0.875    Train Loss :0.002418478950858116    Test Loss :0.053546465933322906\n",
      "Epoch :0.8875    Train Loss :0.002683355240151286    Test Loss :0.09824291616678238\n",
      "Epoch :0.9    Train Loss :0.002836428349837661    Test Loss :0.1061284989118576\n",
      "Epoch :0.9125    Train Loss :0.0025950183626264334    Test Loss :0.10056436061859131\n",
      "Epoch :0.925    Train Loss :0.00237108557485044    Test Loss :0.07839901745319366\n",
      "Epoch :0.9375    Train Loss :0.0022743712179362774    Test Loss :0.07281774282455444\n",
      "Epoch :0.95    Train Loss :0.0029105213470757008    Test Loss :0.11621655523777008\n",
      "Epoch :0.9625    Train Loss :0.0026048889849334955    Test Loss :0.13325701653957367\n",
      "Epoch :0.975    Train Loss :0.0027432944625616074    Test Loss :0.07623140513896942\n",
      "Epoch :0.9875    Train Loss :0.002952223177999258    Test Loss :0.1281043440103531\n",
      "Epoch :1.0    Train Loss :0.0023666899651288986    Test Loss :0.11456575244665146\n",
      "RMSE: 20.480032726069574\n",
      "MAE: 16.869027159774067\n",
      "MAPE: 14.517200414998593%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 100, 'lr': 0.01, 'num_layers': 2}\n",
      "total:  16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.05797808989882469    Test Loss :0.2600799798965454\n",
      "Epoch :0.025    Train Loss :0.05197589471936226    Test Loss :0.22340667247772217\n",
      "Epoch :0.0375    Train Loss :0.04358863830566406    Test Loss :0.012913674116134644\n",
      "Epoch :0.05    Train Loss :0.05593900755047798    Test Loss :0.31629759073257446\n",
      "Epoch :0.0625    Train Loss :0.03870761767029762    Test Loss :0.16601252555847168\n",
      "Epoch :0.075    Train Loss :0.019800914451479912    Test Loss :0.008880703710019588\n",
      "Epoch :0.0875    Train Loss :0.015168406069278717    Test Loss :0.059084124863147736\n",
      "Epoch :0.1    Train Loss :0.014463910833001137    Test Loss :0.0386289581656456\n",
      "Epoch :0.1125    Train Loss :0.011721531860530376    Test Loss :0.01278950646519661\n",
      "Epoch :0.125    Train Loss :0.010112878866493702    Test Loss :0.018586434423923492\n",
      "Epoch :0.1375    Train Loss :0.008706685155630112    Test Loss :0.022910861298441887\n",
      "Epoch :0.15    Train Loss :0.008072867058217525    Test Loss :0.019930316135287285\n",
      "Epoch :0.1625    Train Loss :0.007191119249910116    Test Loss :0.016992837190628052\n",
      "Epoch :0.175    Train Loss :0.00802509393543005    Test Loss :0.017715996131300926\n",
      "Epoch :0.1875    Train Loss :0.007309080101549625    Test Loss :0.014988216571509838\n",
      "Epoch :0.2    Train Loss :0.007372349034994841    Test Loss :0.01748112216591835\n",
      "Epoch :0.2125    Train Loss :0.006959188729524612    Test Loss :0.017878061160445213\n",
      "Epoch :0.225    Train Loss :0.007660270668566227    Test Loss :0.017350785434246063\n",
      "Epoch :0.2375    Train Loss :0.007113634143024683    Test Loss :0.016926130279898643\n",
      "Epoch :0.25    Train Loss :0.006449117790907621    Test Loss :0.019908929243683815\n",
      "Epoch :0.2625    Train Loss :0.006037862040102482    Test Loss :0.017621176317334175\n",
      "Epoch :0.275    Train Loss :0.006370483431965113    Test Loss :0.030457593500614166\n",
      "Epoch :0.2875    Train Loss :0.006986530497670174    Test Loss :0.031030405312776566\n",
      "Epoch :0.3    Train Loss :0.006177728064358234    Test Loss :0.029183529317378998\n",
      "Epoch :0.3125    Train Loss :0.007136447820812464    Test Loss :0.02848714590072632\n",
      "Epoch :0.325    Train Loss :0.007958927191793919    Test Loss :0.03515821322798729\n",
      "Epoch :0.3375    Train Loss :0.00613546185195446    Test Loss :0.027151064947247505\n",
      "Epoch :0.35    Train Loss :0.005345637444406748    Test Loss :0.03332432359457016\n",
      "Epoch :0.3625    Train Loss :0.0055285124108195305    Test Loss :0.03626089543104172\n",
      "Epoch :0.375    Train Loss :0.004597864579409361    Test Loss :0.05883156508207321\n",
      "Epoch :0.3875    Train Loss :0.005203723441809416    Test Loss :0.029090410098433495\n",
      "Epoch :0.4    Train Loss :0.005899739917367697    Test Loss :0.059624772518873215\n",
      "Epoch :0.4125    Train Loss :0.005866745486855507    Test Loss :0.034564390778541565\n",
      "Epoch :0.425    Train Loss :0.005296351388096809    Test Loss :0.05258258059620857\n",
      "Epoch :0.4375    Train Loss :0.00433348910883069    Test Loss :0.06029956042766571\n",
      "Epoch :0.45    Train Loss :0.0045478553511202335    Test Loss :0.03354994207620621\n",
      "Epoch :0.4625    Train Loss :0.00466888677328825    Test Loss :0.06685876101255417\n",
      "Epoch :0.475    Train Loss :0.003910827450454235    Test Loss :0.067121222615242\n",
      "Epoch :0.4875    Train Loss :0.005624115467071533    Test Loss :0.04818304628133774\n",
      "Epoch :0.5    Train Loss :0.004002475645393133    Test Loss :0.04985585808753967\n",
      "Epoch :0.5125    Train Loss :0.0057556829415261745    Test Loss :0.07048753648996353\n",
      "Epoch :0.525    Train Loss :0.0049087791703641415    Test Loss :0.1154102236032486\n",
      "Epoch :0.5375    Train Loss :0.00578431598842144    Test Loss :0.011401467956602573\n",
      "Epoch :0.55    Train Loss :0.005257802549749613    Test Loss :0.024565892294049263\n",
      "Epoch :0.5625    Train Loss :0.004547020420432091    Test Loss :0.02397269196808338\n",
      "Epoch :0.575    Train Loss :0.004779262468218803    Test Loss :0.027889147400856018\n",
      "Epoch :0.5875    Train Loss :0.004014915786683559    Test Loss :0.036345355212688446\n",
      "Epoch :0.6    Train Loss :0.003970605321228504    Test Loss :0.0509585365653038\n",
      "Epoch :0.6125    Train Loss :0.004155196715146303    Test Loss :0.05372503399848938\n",
      "Epoch :0.625    Train Loss :0.0033843573182821274    Test Loss :0.03945903107523918\n",
      "Epoch :0.6375    Train Loss :0.00433266069740057    Test Loss :0.04566121846437454\n",
      "Epoch :0.65    Train Loss :0.004131101071834564    Test Loss :0.06370902806520462\n",
      "Epoch :0.6625    Train Loss :0.0042262086644768715    Test Loss :0.028725959360599518\n",
      "Epoch :0.675    Train Loss :0.0036399520467966795    Test Loss :0.04470430314540863\n",
      "Epoch :0.6875    Train Loss :0.0034377488773316145    Test Loss :0.06544335931539536\n",
      "Epoch :0.7    Train Loss :0.003659069538116455    Test Loss :0.05682583525776863\n",
      "Epoch :0.7125    Train Loss :0.005199211183935404    Test Loss :0.02654559537768364\n",
      "Epoch :0.725    Train Loss :0.004739001858979464    Test Loss :0.06181251257658005\n",
      "Epoch :0.7375    Train Loss :0.004450035281479359    Test Loss :0.02216189354658127\n",
      "Epoch :0.75    Train Loss :0.0037529354449361563    Test Loss :0.06801319122314453\n",
      "Epoch :0.7625    Train Loss :0.0038286037743091583    Test Loss :0.019299332052469254\n",
      "Epoch :0.775    Train Loss :0.00472332164645195    Test Loss :0.06050168350338936\n",
      "Epoch :0.7875    Train Loss :0.0038470362778753042    Test Loss :0.024396220222115517\n",
      "Epoch :0.8    Train Loss :0.0030197305604815483    Test Loss :0.03820502758026123\n",
      "Epoch :0.8125    Train Loss :0.004023175220936537    Test Loss :0.04098198190331459\n",
      "Epoch :0.825    Train Loss :0.0031338727567344904    Test Loss :0.0513090156018734\n",
      "Epoch :0.8375    Train Loss :0.0042287372052669525    Test Loss :0.07116790860891342\n",
      "Epoch :0.85    Train Loss :0.003307816106826067    Test Loss :0.04871320724487305\n",
      "Epoch :0.8625    Train Loss :0.0025880192406475544    Test Loss :0.08138760924339294\n",
      "Epoch :0.875    Train Loss :0.00379589619114995    Test Loss :0.07205252349376678\n",
      "Epoch :0.8875    Train Loss :0.0035748158115893602    Test Loss :0.06591029465198517\n",
      "Epoch :0.9    Train Loss :0.0033088272903114557    Test Loss :0.0447685606777668\n",
      "Epoch :0.9125    Train Loss :0.0032364539802074432    Test Loss :0.0457746721804142\n",
      "Epoch :0.925    Train Loss :0.0031687752343714237    Test Loss :0.027944011613726616\n",
      "Epoch :0.9375    Train Loss :0.0032784361392259598    Test Loss :0.026106588542461395\n",
      "Epoch :0.95    Train Loss :0.0036158557049930096    Test Loss :0.035085175186395645\n",
      "Epoch :0.9625    Train Loss :0.0024482670705765486    Test Loss :0.029044393450021744\n",
      "Epoch :0.975    Train Loss :0.0029338670428842306    Test Loss :0.059318531304597855\n",
      "Epoch :0.9875    Train Loss :0.0027570417150855064    Test Loss :0.04314444214105606\n",
      "Epoch :1.0    Train Loss :0.0030552365351468325    Test Loss :0.033083636313676834\n",
      "RMSE: 12.623615859841939\n",
      "MAE: 10.29386329573364\n",
      "MAPE: 8.842657709904554%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 100, 'lr': 0.01, 'num_layers': 5}\n",
      "total:  19.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.14112162590026855    Test Loss :0.3168758451938629\n",
      "Epoch :0.025    Train Loss :0.0441119559109211    Test Loss :0.132778599858284\n",
      "Epoch :0.0375    Train Loss :0.016709161922335625    Test Loss :0.032931849360466\n",
      "Epoch :0.05    Train Loss :0.012410739436745644    Test Loss :0.011080864816904068\n",
      "Epoch :0.0625    Train Loss :0.06549804657697678    Test Loss :0.24606676399707794\n",
      "Epoch :0.075    Train Loss :0.03037559613585472    Test Loss :0.00720853079110384\n",
      "Epoch :0.0875    Train Loss :0.017121726647019386    Test Loss :0.05284225568175316\n",
      "Epoch :0.1    Train Loss :0.015248955227434635    Test Loss :0.12808984518051147\n",
      "Epoch :0.1125    Train Loss :0.008717743679881096    Test Loss :0.013916782103478909\n",
      "Epoch :0.125    Train Loss :0.007406908553093672    Test Loss :0.010553976520895958\n",
      "Epoch :0.1375    Train Loss :0.006525877863168716    Test Loss :0.010934052057564259\n",
      "Epoch :0.15    Train Loss :0.00606171041727066    Test Loss :0.008444143459200859\n",
      "Epoch :0.1625    Train Loss :0.005984635557979345    Test Loss :0.010447110049426556\n",
      "Epoch :0.175    Train Loss :0.0056634028442204    Test Loss :0.00877085980027914\n",
      "Epoch :0.1875    Train Loss :0.0046884785406291485    Test Loss :0.010122322477400303\n",
      "Epoch :0.2    Train Loss :0.00559544051066041    Test Loss :0.0071841999888420105\n",
      "Epoch :0.2125    Train Loss :0.004728354513645172    Test Loss :0.007513097953051329\n",
      "Epoch :0.225    Train Loss :0.005154224578291178    Test Loss :0.006744321435689926\n",
      "Epoch :0.2375    Train Loss :0.0052602640353143215    Test Loss :0.007574174553155899\n",
      "Epoch :0.25    Train Loss :0.0049909078516066074    Test Loss :0.006567345932126045\n",
      "Epoch :0.2625    Train Loss :0.0047217668034136295    Test Loss :0.007623476907610893\n",
      "Epoch :0.275    Train Loss :0.005230991169810295    Test Loss :0.009298435412347317\n",
      "Epoch :0.2875    Train Loss :0.004370441194623709    Test Loss :0.007750900462269783\n",
      "Epoch :0.3    Train Loss :0.004757107235491276    Test Loss :0.008023290894925594\n",
      "Epoch :0.3125    Train Loss :0.004604069050401449    Test Loss :0.0066604819148778915\n",
      "Epoch :0.325    Train Loss :0.004194878041744232    Test Loss :0.008004511706531048\n",
      "Epoch :0.3375    Train Loss :0.004119201097637415    Test Loss :0.0069032032042741776\n",
      "Epoch :0.35    Train Loss :0.003953712061047554    Test Loss :0.007403617259114981\n",
      "Epoch :0.3625    Train Loss :0.004182605072855949    Test Loss :0.006083172746002674\n",
      "Epoch :0.375    Train Loss :0.003642758121713996    Test Loss :0.007736383005976677\n",
      "Epoch :0.3875    Train Loss :0.004193189553916454    Test Loss :0.007117729168385267\n",
      "Epoch :0.4    Train Loss :0.00434755627065897    Test Loss :0.004502828698605299\n",
      "Epoch :0.4125    Train Loss :0.003730853321030736    Test Loss :0.0062879230827093124\n",
      "Epoch :0.425    Train Loss :0.0035128924064338207    Test Loss :0.0093439482152462\n",
      "Epoch :0.4375    Train Loss :0.0034082222264260054    Test Loss :0.008727285079658031\n",
      "Epoch :0.45    Train Loss :0.003694540821015835    Test Loss :0.005801816005259752\n",
      "Epoch :0.4625    Train Loss :0.003948859870433807    Test Loss :0.006129107438027859\n",
      "Epoch :0.475    Train Loss :0.0036180727183818817    Test Loss :0.005793198011815548\n",
      "Epoch :0.4875    Train Loss :0.0031499024480581284    Test Loss :0.006358030252158642\n",
      "Epoch :0.5    Train Loss :0.0034865776542574167    Test Loss :0.009932063519954681\n",
      "Epoch :0.5125    Train Loss :0.0032446617260575294    Test Loss :0.010080860927700996\n",
      "Epoch :0.525    Train Loss :0.0035474668256938457    Test Loss :0.0042827073484659195\n",
      "Epoch :0.5375    Train Loss :0.003021352458745241    Test Loss :0.006464882753789425\n",
      "Epoch :0.55    Train Loss :0.0032418393529951572    Test Loss :0.00647829519584775\n",
      "Epoch :0.5625    Train Loss :0.003069796832278371    Test Loss :0.006034956779330969\n",
      "Epoch :0.575    Train Loss :0.003352564759552479    Test Loss :0.008263379335403442\n",
      "Epoch :0.5875    Train Loss :0.0027539092116057873    Test Loss :0.004359123762696981\n",
      "Epoch :0.6    Train Loss :0.003599027404561639    Test Loss :0.005449902266263962\n",
      "Epoch :0.6125    Train Loss :0.003214654978364706    Test Loss :0.005369063466787338\n",
      "Epoch :0.625    Train Loss :0.003320906776934862    Test Loss :0.0045113833621144295\n",
      "Epoch :0.6375    Train Loss :0.0025635522324591875    Test Loss :0.004821786191314459\n",
      "Epoch :0.65    Train Loss :0.0026324798818677664    Test Loss :0.005591412074863911\n",
      "Epoch :0.6625    Train Loss :0.002644206862896681    Test Loss :0.007658899761736393\n",
      "Epoch :0.675    Train Loss :0.0028108609840273857    Test Loss :0.005620019976049662\n",
      "Epoch :0.6875    Train Loss :0.0028008450753986835    Test Loss :0.00513995997607708\n",
      "Epoch :0.7    Train Loss :0.003129239659756422    Test Loss :0.005010646302253008\n",
      "Epoch :0.7125    Train Loss :0.003133039455860853    Test Loss :0.004410090856254101\n",
      "Epoch :0.725    Train Loss :0.0026253433898091316    Test Loss :0.008682800456881523\n",
      "Epoch :0.7375    Train Loss :0.0029148918110877275    Test Loss :0.010082801803946495\n",
      "Epoch :0.75    Train Loss :0.0022657830268144608    Test Loss :0.005899128038436174\n",
      "Epoch :0.7625    Train Loss :0.002652128227055073    Test Loss :0.004528204910457134\n",
      "Epoch :0.775    Train Loss :0.0029140517581254244    Test Loss :0.005911201238632202\n",
      "Epoch :0.7875    Train Loss :0.0026343418285250664    Test Loss :0.0056228032335639\n",
      "Epoch :0.8    Train Loss :0.002703755395486951    Test Loss :0.004379308316856623\n",
      "Epoch :0.8125    Train Loss :0.002669491106644273    Test Loss :0.009721899405121803\n",
      "Epoch :0.825    Train Loss :0.0026004414539784193    Test Loss :0.006933283526450396\n",
      "Epoch :0.8375    Train Loss :0.0023291739635169506    Test Loss :0.008523715659976006\n",
      "Epoch :0.85    Train Loss :0.002746290760114789    Test Loss :0.004100280348211527\n",
      "Epoch :0.8625    Train Loss :0.002649391535669565    Test Loss :0.005423413589596748\n",
      "Epoch :0.875    Train Loss :0.0030834823846817017    Test Loss :0.004926734138280153\n",
      "Epoch :0.8875    Train Loss :0.0024117622524499893    Test Loss :0.008171839639544487\n",
      "Epoch :0.9    Train Loss :0.002403711434453726    Test Loss :0.007565654814243317\n",
      "Epoch :0.9125    Train Loss :0.0028659962117671967    Test Loss :0.004684409126639366\n",
      "Epoch :0.925    Train Loss :0.0023502230178564787    Test Loss :0.0058917086571455\n",
      "Epoch :0.9375    Train Loss :0.002010960131883621    Test Loss :0.0053163738921284676\n",
      "Epoch :0.95    Train Loss :0.0027444446459412575    Test Loss :0.007963970303535461\n",
      "Epoch :0.9625    Train Loss :0.0029172832146286964    Test Loss :0.003268826985731721\n",
      "Epoch :0.975    Train Loss :0.0024996413849294186    Test Loss :0.003459752071648836\n",
      "Epoch :0.9875    Train Loss :0.0023001732770353556    Test Loss :0.008178101852536201\n",
      "Epoch :1.0    Train Loss :0.00282976683229208    Test Loss :0.0044481088407337666\n",
      "RMSE: 9.326625729650555\n",
      "MAE: 7.036730136911734\n",
      "MAPE: 6.0227364034298585%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 100, 'lr': 0.02, 'num_layers': 2}\n",
      "total:  22.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.07152517139911652    Test Loss :0.8426835536956787\n",
      "Epoch :0.025    Train Loss :0.24516014754772186    Test Loss :0.35747405886650085\n",
      "Epoch :0.0375    Train Loss :0.06990707665681839    Test Loss :0.10632964223623276\n",
      "Epoch :0.05    Train Loss :0.0644214078783989    Test Loss :0.36240091919898987\n",
      "Epoch :0.0625    Train Loss :0.05905773490667343    Test Loss :0.1588522344827652\n",
      "Epoch :0.075    Train Loss :0.05533752590417862    Test Loss :0.30465075373649597\n",
      "Epoch :0.0875    Train Loss :0.053149573504924774    Test Loss :0.18687120079994202\n",
      "Epoch :0.1    Train Loss :0.0529310368001461    Test Loss :0.27616870403289795\n",
      "Epoch :0.1125    Train Loss :0.05181172862648964    Test Loss :0.20319749414920807\n",
      "Epoch :0.125    Train Loss :0.05229896679520607    Test Loss :0.256806880235672\n",
      "Epoch :0.1375    Train Loss :0.05189664289355278    Test Loss :0.21707427501678467\n",
      "Epoch :0.15    Train Loss :0.05201819911599159    Test Loss :0.24905075132846832\n",
      "Epoch :0.1625    Train Loss :0.052000559866428375    Test Loss :0.22563287615776062\n",
      "Epoch :0.175    Train Loss :0.0515899732708931    Test Loss :0.24357452988624573\n",
      "Epoch :0.1875    Train Loss :0.051583532243967056    Test Loss :0.22621385753154755\n",
      "Epoch :0.2    Train Loss :0.0517113134264946    Test Loss :0.23919792473316193\n",
      "Epoch :0.2125    Train Loss :0.05186779424548149    Test Loss :0.23247717320919037\n",
      "Epoch :0.225    Train Loss :0.051412902772426605    Test Loss :0.234555184841156\n",
      "Epoch :0.2375    Train Loss :0.05158022418618202    Test Loss :0.235701784491539\n",
      "Epoch :0.25    Train Loss :0.05170067399740219    Test Loss :0.23190222680568695\n",
      "Epoch :0.2625    Train Loss :0.05161936953663826    Test Loss :0.23710453510284424\n",
      "Epoch :0.275    Train Loss :0.05173037573695183    Test Loss :0.23093554377555847\n",
      "Epoch :0.2875    Train Loss :0.05169851332902908    Test Loss :0.23212981224060059\n",
      "Epoch :0.3    Train Loss :0.051638804376125336    Test Loss :0.23831114172935486\n",
      "Epoch :0.3125    Train Loss :0.05169427767395973    Test Loss :0.23333032429218292\n",
      "Epoch :0.325    Train Loss :0.051797401160001755    Test Loss :0.23281729221343994\n",
      "Epoch :0.3375    Train Loss :0.05146602541208267    Test Loss :0.23595106601715088\n",
      "Epoch :0.35    Train Loss :0.051387619227170944    Test Loss :0.2346542477607727\n",
      "Epoch :0.3625    Train Loss :0.05147220566868782    Test Loss :0.2364303469657898\n",
      "Epoch :0.375    Train Loss :0.05144483968615532    Test Loss :0.23599955439567566\n",
      "Epoch :0.3875    Train Loss :0.05182007700204849    Test Loss :0.23208670318126678\n",
      "Epoch :0.4    Train Loss :0.05153939500451088    Test Loss :0.23553317785263062\n",
      "Epoch :0.4125    Train Loss :0.05126121640205383    Test Loss :0.23368777334690094\n",
      "Epoch :0.425    Train Loss :0.05121419578790665    Test Loss :0.23426315188407898\n",
      "Epoch :0.4375    Train Loss :0.05072956159710884    Test Loss :0.22636376321315765\n",
      "Epoch :0.45    Train Loss :0.051932092756032944    Test Loss :0.22356025874614716\n",
      "Epoch :0.4625    Train Loss :0.046828217804431915    Test Loss :0.22487059235572815\n",
      "Epoch :0.475    Train Loss :0.03948433697223663    Test Loss :0.13667431473731995\n",
      "Epoch :0.4875    Train Loss :0.02621840313076973    Test Loss :0.07975112646818161\n",
      "Epoch :0.5    Train Loss :0.06418482214212418    Test Loss :0.1010814905166626\n",
      "Epoch :0.5125    Train Loss :0.034127164632081985    Test Loss :0.03474496304988861\n",
      "Epoch :0.525    Train Loss :0.0214817076921463    Test Loss :0.03089224360883236\n",
      "Epoch :0.5375    Train Loss :0.020888080820441246    Test Loss :0.08350908011198044\n",
      "Epoch :0.55    Train Loss :0.043842777609825134    Test Loss :0.15882866084575653\n",
      "Epoch :0.5625    Train Loss :0.027079612016677856    Test Loss :0.08038153499364853\n",
      "Epoch :0.575    Train Loss :0.016734199598431587    Test Loss :0.035562995821237564\n",
      "Epoch :0.5875    Train Loss :0.025874171406030655    Test Loss :0.06622467935085297\n",
      "Epoch :0.6    Train Loss :0.018152959644794464    Test Loss :0.015231973491609097\n",
      "Epoch :0.6125    Train Loss :0.014740167185664177    Test Loss :0.02286597155034542\n",
      "Epoch :0.625    Train Loss :0.03202258050441742    Test Loss :0.0956006646156311\n",
      "Epoch :0.6375    Train Loss :0.028873952105641365    Test Loss :0.13112469017505646\n",
      "Epoch :0.65    Train Loss :0.024954920634627342    Test Loss :0.052132006734609604\n",
      "Epoch :0.6625    Train Loss :0.01701544225215912    Test Loss :0.04788478836417198\n",
      "Epoch :0.675    Train Loss :0.028342196717858315    Test Loss :0.02853318303823471\n",
      "Epoch :0.6875    Train Loss :0.014707906171679497    Test Loss :0.01504881214350462\n",
      "Epoch :0.7    Train Loss :0.013783860020339489    Test Loss :0.028090663254261017\n",
      "Epoch :0.7125    Train Loss :0.014180394820868969    Test Loss :0.023472776636481285\n",
      "Epoch :0.725    Train Loss :0.02295471914112568    Test Loss :0.03206140920519829\n",
      "Epoch :0.7375    Train Loss :0.020373264327645302    Test Loss :0.04628559947013855\n",
      "Epoch :0.75    Train Loss :0.020043589174747467    Test Loss :0.02610805444419384\n",
      "Epoch :0.7625    Train Loss :0.01654502935707569    Test Loss :0.033175479620695114\n",
      "Epoch :0.775    Train Loss :0.014367244206368923    Test Loss :0.01776093989610672\n",
      "Epoch :0.7875    Train Loss :0.014351326040923595    Test Loss :0.031760621815919876\n",
      "Epoch :0.8    Train Loss :0.014365747570991516    Test Loss :0.02099073864519596\n",
      "Epoch :0.8125    Train Loss :0.013269062153995037    Test Loss :0.022381171584129333\n",
      "Epoch :0.825    Train Loss :0.012562439776957035    Test Loss :0.019896719604730606\n",
      "Epoch :0.8375    Train Loss :0.01374670397490263    Test Loss :0.02698792703449726\n",
      "Epoch :0.85    Train Loss :0.013803157955408096    Test Loss :0.02223171666264534\n",
      "Epoch :0.8625    Train Loss :0.012662860564887524    Test Loss :0.02245735004544258\n",
      "Epoch :0.875    Train Loss :0.013482782989740372    Test Loss :0.022804254665970802\n",
      "Epoch :0.8875    Train Loss :0.015820257365703583    Test Loss :0.02770695835351944\n",
      "Epoch :0.9    Train Loss :0.013273697346448898    Test Loss :0.02349134534597397\n",
      "Epoch :0.9125    Train Loss :0.013501448556780815    Test Loss :0.02566699869930744\n",
      "Epoch :0.925    Train Loss :0.014235851354897022    Test Loss :0.02726617082953453\n",
      "Epoch :0.9375    Train Loss :0.01358197070658207    Test Loss :0.02571866475045681\n",
      "Epoch :0.95    Train Loss :0.012660685926675797    Test Loss :0.026446953415870667\n",
      "Epoch :0.9625    Train Loss :0.01328770536929369    Test Loss :0.02348577231168747\n",
      "Epoch :0.975    Train Loss :0.014420718885958195    Test Loss :0.020338943228125572\n",
      "Epoch :0.9875    Train Loss :0.012595761567354202    Test Loss :0.028588997200131416\n",
      "Epoch :1.0    Train Loss :0.012647255323827267    Test Loss :0.020339928567409515\n",
      "RMSE: 12.003628105673027\n",
      "MAE: 9.993450696558105\n",
      "MAPE: 8.558822899894947%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 100, 'lr': 0.02, 'num_layers': 5}\n",
      "total:  25.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.05272752419114113    Test Loss :0.3359259366989136\n",
      "Epoch :0.016666666666666666    Train Loss :0.04666532576084137    Test Loss :0.19524164497852325\n",
      "Epoch :0.025    Train Loss :0.04056806489825249    Test Loss :0.09923284500837326\n",
      "Epoch :0.03333333333333333    Train Loss :0.01231340877711773    Test Loss :0.06288198381662369\n",
      "Epoch :0.041666666666666664    Train Loss :0.009445738047361374    Test Loss :0.014032043516635895\n",
      "Epoch :0.05    Train Loss :0.010792332701385021    Test Loss :0.025108717381954193\n",
      "Epoch :0.058333333333333334    Train Loss :0.008943025954067707    Test Loss :0.01532173901796341\n",
      "Epoch :0.06666666666666667    Train Loss :0.005515499506145716    Test Loss :0.009695002809166908\n",
      "Epoch :0.075    Train Loss :0.008123473264276981    Test Loss :0.007913046516478062\n",
      "Epoch :0.08333333333333333    Train Loss :0.006655040662735701    Test Loss :0.007247358560562134\n",
      "Epoch :0.09166666666666666    Train Loss :0.006369946990162134    Test Loss :0.007924688048660755\n",
      "Epoch :0.1    Train Loss :0.005938445217907429    Test Loss :0.010064591653645039\n",
      "Epoch :0.10833333333333334    Train Loss :0.0059388913214206696    Test Loss :0.00884094089269638\n",
      "Epoch :0.11666666666666667    Train Loss :0.006147392094135284    Test Loss :0.008137796074151993\n",
      "Epoch :0.125    Train Loss :0.005226660519838333    Test Loss :0.006350763142108917\n",
      "Epoch :0.13333333333333333    Train Loss :0.005752908997237682    Test Loss :0.006038390565663576\n",
      "Epoch :0.14166666666666666    Train Loss :0.005766731686890125    Test Loss :0.007339200004935265\n",
      "Epoch :0.15    Train Loss :0.004684722051024437    Test Loss :0.007751462981104851\n",
      "Epoch :0.15833333333333333    Train Loss :0.004835829604417086    Test Loss :0.006803859956562519\n",
      "Epoch :0.16666666666666666    Train Loss :0.0042754835449159145    Test Loss :0.0058411466889083385\n",
      "Epoch :0.175    Train Loss :0.004395161755383015    Test Loss :0.006264262832701206\n",
      "Epoch :0.18333333333333332    Train Loss :0.004973608069121838    Test Loss :0.00763906491920352\n",
      "Epoch :0.19166666666666668    Train Loss :0.004267089534550905    Test Loss :0.008784160017967224\n",
      "Epoch :0.2    Train Loss :0.00450562359765172    Test Loss :0.007409011945128441\n",
      "Epoch :0.20833333333333334    Train Loss :0.0035210547503083944    Test Loss :0.008195760659873486\n",
      "Epoch :0.21666666666666667    Train Loss :0.004092920571565628    Test Loss :0.006846579723060131\n",
      "Epoch :0.225    Train Loss :0.0036480696871876717    Test Loss :0.006455008871853352\n",
      "Epoch :0.23333333333333334    Train Loss :0.0033133651595562696    Test Loss :0.006732685957103968\n",
      "Epoch :0.24166666666666667    Train Loss :0.0033199728932231665    Test Loss :0.007177838124334812\n",
      "Epoch :0.25    Train Loss :0.003022894961759448    Test Loss :0.008206833153963089\n",
      "Epoch :0.25833333333333336    Train Loss :0.0030798304360359907    Test Loss :0.005720062181353569\n",
      "Epoch :0.26666666666666666    Train Loss :0.0030007418245077133    Test Loss :0.008656153455376625\n",
      "Epoch :0.275    Train Loss :0.002871911507099867    Test Loss :0.006139467936009169\n",
      "Epoch :0.2833333333333333    Train Loss :0.0032205055467784405    Test Loss :0.008098065853118896\n",
      "Epoch :0.2916666666666667    Train Loss :0.0030418469104915857    Test Loss :0.005376188084483147\n",
      "Epoch :0.3    Train Loss :0.0028751313220709562    Test Loss :0.00913467537611723\n",
      "Epoch :0.30833333333333335    Train Loss :0.0024574403651058674    Test Loss :0.006934895180165768\n",
      "Epoch :0.31666666666666665    Train Loss :0.0024754684418439865    Test Loss :0.0053197103552520275\n",
      "Epoch :0.325    Train Loss :0.0033993793185800314    Test Loss :0.005638256203383207\n",
      "Epoch :0.3333333333333333    Train Loss :0.002466907724738121    Test Loss :0.007570041809231043\n",
      "Epoch :0.3416666666666667    Train Loss :0.0027263390365988016    Test Loss :0.00688911322504282\n",
      "Epoch :0.35    Train Loss :0.002631291514262557    Test Loss :0.006991549860686064\n",
      "Epoch :0.35833333333333334    Train Loss :0.0026273278053849936    Test Loss :0.005011951085180044\n",
      "Epoch :0.36666666666666664    Train Loss :0.0022986936382949352    Test Loss :0.00622282549738884\n",
      "Epoch :0.375    Train Loss :0.0022785465698689222    Test Loss :0.004895020741969347\n",
      "Epoch :0.38333333333333336    Train Loss :0.002214533044025302    Test Loss :0.006069009192287922\n",
      "Epoch :0.39166666666666666    Train Loss :0.0025469984393566847    Test Loss :0.007763190194964409\n",
      "Epoch :0.4    Train Loss :0.003205489134415984    Test Loss :0.006113394629210234\n",
      "Epoch :0.4083333333333333    Train Loss :0.002835238818079233    Test Loss :0.009437710046768188\n",
      "Epoch :0.4166666666666667    Train Loss :0.002903092885389924    Test Loss :0.004707279149442911\n",
      "Epoch :0.425    Train Loss :0.002401884878054261    Test Loss :0.0043922546319663525\n",
      "Epoch :0.43333333333333335    Train Loss :0.002972704591229558    Test Loss :0.005876145791262388\n",
      "Epoch :0.44166666666666665    Train Loss :0.0024757604114711285    Test Loss :0.006287462078034878\n",
      "Epoch :0.45    Train Loss :0.002636184450238943    Test Loss :0.0061200144700706005\n",
      "Epoch :0.4583333333333333    Train Loss :0.002296495484188199    Test Loss :0.006946846842765808\n",
      "Epoch :0.4666666666666667    Train Loss :0.002333233831450343    Test Loss :0.006100375670939684\n",
      "Epoch :0.475    Train Loss :0.0025614253245294094    Test Loss :0.006271157879382372\n",
      "Epoch :0.48333333333333334    Train Loss :0.0020940368995070457    Test Loss :0.00969077367335558\n",
      "Epoch :0.49166666666666664    Train Loss :0.0027825019787997007    Test Loss :0.006363614462316036\n",
      "Epoch :0.5    Train Loss :0.002208456862717867    Test Loss :0.007948411628603935\n",
      "Epoch :0.5083333333333333    Train Loss :0.0022201782558113337    Test Loss :0.006715052295476198\n",
      "Epoch :0.5166666666666667    Train Loss :0.002808299846947193    Test Loss :0.004835720639675856\n",
      "Epoch :0.525    Train Loss :0.0026059381198138    Test Loss :0.003937628120183945\n",
      "Epoch :0.5333333333333333    Train Loss :0.0021262497175484896    Test Loss :0.0057706222869455814\n",
      "Epoch :0.5416666666666666    Train Loss :0.0022794194519519806    Test Loss :0.00477985292673111\n",
      "Epoch :0.55    Train Loss :0.002493674401193857    Test Loss :0.006852555554360151\n",
      "Epoch :0.5583333333333333    Train Loss :0.0023241674061864614    Test Loss :0.004909179173409939\n",
      "Epoch :0.5666666666666667    Train Loss :0.0025894432328641415    Test Loss :0.009178537875413895\n",
      "Epoch :0.575    Train Loss :0.002247641794383526    Test Loss :0.005014999769628048\n",
      "Epoch :0.5833333333333334    Train Loss :0.001898978021927178    Test Loss :0.007365029770880938\n",
      "Epoch :0.5916666666666667    Train Loss :0.002473191823810339    Test Loss :0.004368262365460396\n",
      "Epoch :0.6    Train Loss :0.002391805872321129    Test Loss :0.007139825262129307\n",
      "Epoch :0.6083333333333333    Train Loss :0.0023757710587233305    Test Loss :0.006755358539521694\n",
      "Epoch :0.6166666666666667    Train Loss :0.002438789699226618    Test Loss :0.003955481573939323\n",
      "Epoch :0.625    Train Loss :0.0024803662672638893    Test Loss :0.005386686883866787\n",
      "Epoch :0.6333333333333333    Train Loss :0.0023889129515737295    Test Loss :0.004165904596447945\n",
      "Epoch :0.6416666666666667    Train Loss :0.00221026549115777    Test Loss :0.005423580761998892\n",
      "Epoch :0.65    Train Loss :0.0029320758767426014    Test Loss :0.007699126377701759\n",
      "Epoch :0.6583333333333333    Train Loss :0.0022893992718309164    Test Loss :0.007141191512346268\n",
      "Epoch :0.6666666666666666    Train Loss :0.002007728209719062    Test Loss :0.009220932610332966\n",
      "Epoch :0.675    Train Loss :0.0023399905767291784    Test Loss :0.005704128183424473\n",
      "Epoch :0.6833333333333333    Train Loss :0.00233774003572762    Test Loss :0.004577776417136192\n",
      "Epoch :0.6916666666666667    Train Loss :0.0022892639972269535    Test Loss :0.003096373751759529\n",
      "Epoch :0.7    Train Loss :0.0019250913755968213    Test Loss :0.007191952783614397\n",
      "Epoch :0.7083333333333334    Train Loss :0.00240444322116673    Test Loss :0.0052969008684158325\n",
      "Epoch :0.7166666666666667    Train Loss :0.002482870826497674    Test Loss :0.0051404377445578575\n",
      "Epoch :0.725    Train Loss :0.0017294385470449924    Test Loss :0.005386965349316597\n",
      "Epoch :0.7333333333333333    Train Loss :0.00207752943970263    Test Loss :0.006947185844182968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7416666666666667    Train Loss :0.0022510853596031666    Test Loss :0.00713809859007597\n",
      "Epoch :0.75    Train Loss :0.0020186766050755978    Test Loss :0.006274553947150707\n",
      "Epoch :0.7583333333333333    Train Loss :0.002129650441929698    Test Loss :0.006391939707100391\n",
      "Epoch :0.7666666666666667    Train Loss :0.0022923166397958994    Test Loss :0.008898281492292881\n",
      "Epoch :0.775    Train Loss :0.0023514768108725548    Test Loss :0.010011760517954826\n",
      "Epoch :0.7833333333333333    Train Loss :0.002059229416772723    Test Loss :0.007926695048809052\n",
      "Epoch :0.7916666666666666    Train Loss :0.002159136114642024    Test Loss :0.012191333808004856\n",
      "Epoch :0.8    Train Loss :0.0022421982139348984    Test Loss :0.009004920721054077\n",
      "Epoch :0.8083333333333333    Train Loss :0.0029871617443859577    Test Loss :0.011099152266979218\n",
      "Epoch :0.8166666666666667    Train Loss :0.0023330398835241795    Test Loss :0.0049849869683384895\n",
      "Epoch :0.825    Train Loss :0.0022821456659585238    Test Loss :0.007198057137429714\n",
      "Epoch :0.8333333333333334    Train Loss :0.0021833297796547413    Test Loss :0.011816881597042084\n",
      "Epoch :0.8416666666666667    Train Loss :0.002639029175043106    Test Loss :0.007150040473788977\n",
      "Epoch :0.85    Train Loss :0.0019567846320569515    Test Loss :0.007886243984103203\n",
      "Epoch :0.8583333333333333    Train Loss :0.0022558134514838457    Test Loss :0.004684965591877699\n",
      "Epoch :0.8666666666666667    Train Loss :0.0022869217209517956    Test Loss :0.006382705643773079\n",
      "Epoch :0.875    Train Loss :0.002283930778503418    Test Loss :0.00741826044395566\n",
      "Epoch :0.8833333333333333    Train Loss :0.002232321072369814    Test Loss :0.009777246974408627\n",
      "Epoch :0.8916666666666667    Train Loss :0.002244005212560296    Test Loss :0.012560639530420303\n",
      "Epoch :0.9    Train Loss :0.002142990240827203    Test Loss :0.012128938920795918\n",
      "Epoch :0.9083333333333333    Train Loss :0.0015956240240484476    Test Loss :0.026435334235429764\n",
      "Epoch :0.9166666666666666    Train Loss :0.002245544921606779    Test Loss :0.027285369113087654\n",
      "Epoch :0.925    Train Loss :0.002353534335270524    Test Loss :0.01400345005095005\n",
      "Epoch :0.9333333333333333    Train Loss :0.0023155270610004663    Test Loss :0.012964573688805103\n",
      "Epoch :0.9416666666666667    Train Loss :0.0019345119362697005    Test Loss :0.008797545917332172\n",
      "Epoch :0.95    Train Loss :0.001860740827396512    Test Loss :0.01856931298971176\n",
      "Epoch :0.9583333333333334    Train Loss :0.002605772577226162    Test Loss :0.014374620281159878\n",
      "Epoch :0.9666666666666667    Train Loss :0.002008849987760186    Test Loss :0.009544072672724724\n",
      "Epoch :0.975    Train Loss :0.0022643709089607    Test Loss :0.012694149278104305\n",
      "Epoch :0.9833333333333333    Train Loss :0.0023384452797472477    Test Loss :0.04525670409202576\n",
      "Epoch :0.9916666666666667    Train Loss :0.0021078602876514196    Test Loss :0.015255601145327091\n",
      "Epoch :1.0    Train Loss :0.0021682907827198505    Test Loss :0.01075119711458683\n",
      "RMSE: 14.230731110888053\n",
      "MAE: 11.020516976076042\n",
      "MAPE: 9.427903625296006%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 600, 'hid_size': 50, 'lr': 0.01, 'num_layers': 2}\n",
      "total:  28.000000000000004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.05962767079472542    Test Loss :0.38276296854019165\n",
      "Epoch :0.016666666666666666    Train Loss :0.0610518753528595    Test Loss :0.28905847668647766\n",
      "Epoch :0.025    Train Loss :0.05207357555627823    Test Loss :0.24605274200439453\n",
      "Epoch :0.03333333333333333    Train Loss :0.05109187588095665    Test Loss :0.24276475608348846\n",
      "Epoch :0.041666666666666664    Train Loss :0.04587632790207863    Test Loss :0.16581763327121735\n",
      "Epoch :0.05    Train Loss :0.018476303666830063    Test Loss :0.019246995449066162\n",
      "Epoch :0.058333333333333334    Train Loss :0.018350880593061447    Test Loss :0.027259308844804764\n",
      "Epoch :0.06666666666666667    Train Loss :0.015231906436383724    Test Loss :0.04914238303899765\n",
      "Epoch :0.075    Train Loss :0.014245002530515194    Test Loss :0.014923368580639362\n",
      "Epoch :0.08333333333333333    Train Loss :0.017105644568800926    Test Loss :0.02034025453031063\n",
      "Epoch :0.09166666666666666    Train Loss :0.019958967342972755    Test Loss :0.07542949169874191\n",
      "Epoch :0.1    Train Loss :0.015016712248325348    Test Loss :0.04662482440471649\n",
      "Epoch :0.10833333333333334    Train Loss :0.013536421582102776    Test Loss :0.00729399174451828\n",
      "Epoch :0.11666666666666667    Train Loss :0.010297034867107868    Test Loss :0.014961181208491325\n",
      "Epoch :0.125    Train Loss :0.01090316567569971    Test Loss :0.026969531551003456\n",
      "Epoch :0.13333333333333333    Train Loss :0.008777055889368057    Test Loss :0.02408311516046524\n",
      "Epoch :0.14166666666666666    Train Loss :0.008967720903456211    Test Loss :0.013970117084681988\n",
      "Epoch :0.15    Train Loss :0.008049068041145802    Test Loss :0.01533906813710928\n",
      "Epoch :0.15833333333333333    Train Loss :0.0077661070972681046    Test Loss :0.017841145396232605\n",
      "Epoch :0.16666666666666666    Train Loss :0.006836641114205122    Test Loss :0.017703469842672348\n",
      "Epoch :0.175    Train Loss :0.006806273479014635    Test Loss :0.016280509531497955\n",
      "Epoch :0.18333333333333332    Train Loss :0.0071957954205572605    Test Loss :0.015888230875134468\n",
      "Epoch :0.19166666666666668    Train Loss :0.0067235431633889675    Test Loss :0.01636740192770958\n",
      "Epoch :0.2    Train Loss :0.006696133408695459    Test Loss :0.018324164673686028\n",
      "Epoch :0.20833333333333334    Train Loss :0.006248020566999912    Test Loss :0.015959419310092926\n",
      "Epoch :0.21666666666666667    Train Loss :0.006268205586820841    Test Loss :0.016719471663236618\n",
      "Epoch :0.225    Train Loss :0.0065006990917027    Test Loss :0.01798729971051216\n",
      "Epoch :0.23333333333333334    Train Loss :0.006606976501643658    Test Loss :0.016641486436128616\n",
      "Epoch :0.24166666666666667    Train Loss :0.006790669169276953    Test Loss :0.018475191667675972\n",
      "Epoch :0.25    Train Loss :0.005842339713126421    Test Loss :0.018137505277991295\n",
      "Epoch :0.25833333333333336    Train Loss :0.005677490495145321    Test Loss :0.018008612096309662\n",
      "Epoch :0.26666666666666666    Train Loss :0.005955385975539684    Test Loss :0.01642009988427162\n",
      "Epoch :0.275    Train Loss :0.0050761462189257145    Test Loss :0.019486581906676292\n",
      "Epoch :0.2833333333333333    Train Loss :0.005600045435130596    Test Loss :0.01959485560655594\n",
      "Epoch :0.2916666666666667    Train Loss :0.005285946186631918    Test Loss :0.018650643527507782\n",
      "Epoch :0.3    Train Loss :0.00531607074663043    Test Loss :0.02824857085943222\n",
      "Epoch :0.30833333333333335    Train Loss :0.004987814463675022    Test Loss :0.03799014538526535\n",
      "Epoch :0.31666666666666665    Train Loss :0.005186994094401598    Test Loss :0.02575763128697872\n",
      "Epoch :0.325    Train Loss :0.0055123260244727135    Test Loss :0.033781521022319794\n",
      "Epoch :0.3333333333333333    Train Loss :0.005384440533816814    Test Loss :0.019715063273906708\n",
      "Epoch :0.3416666666666667    Train Loss :0.004619993735104799    Test Loss :0.037221673876047134\n",
      "Epoch :0.35    Train Loss :0.004515659064054489    Test Loss :0.04624536260962486\n",
      "Epoch :0.35833333333333334    Train Loss :0.004441681317985058    Test Loss :0.039840567857027054\n",
      "Epoch :0.36666666666666664    Train Loss :0.0038226929027587175    Test Loss :0.038583140820264816\n",
      "Epoch :0.375    Train Loss :0.006315083242952824    Test Loss :0.04421636462211609\n",
      "Epoch :0.38333333333333336    Train Loss :0.006025512237101793    Test Loss :0.013878071680665016\n",
      "Epoch :0.39166666666666666    Train Loss :0.0049230484291911125    Test Loss :0.022461429238319397\n",
      "Epoch :0.4    Train Loss :0.004377644043415785    Test Loss :0.03668949380517006\n",
      "Epoch :0.4083333333333333    Train Loss :0.0035789639223366976    Test Loss :0.04864155873656273\n",
      "Epoch :0.4166666666666667    Train Loss :0.004531887825578451    Test Loss :0.049358002841472626\n",
      "Epoch :0.425    Train Loss :0.0039849188178777695    Test Loss :0.04871654137969017\n",
      "Epoch :0.43333333333333335    Train Loss :0.00403843866661191    Test Loss :0.041241470724344254\n",
      "Epoch :0.44166666666666665    Train Loss :0.0034670978784561157    Test Loss :0.0473477877676487\n",
      "Epoch :0.45    Train Loss :0.005169142968952656    Test Loss :0.039542995393276215\n",
      "Epoch :0.4583333333333333    Train Loss :0.0032693364191800356    Test Loss :0.036174263805150986\n",
      "Epoch :0.4666666666666667    Train Loss :0.003575265407562256    Test Loss :0.03934084624052048\n",
      "Epoch :0.475    Train Loss :0.0033832727931439877    Test Loss :0.04579421877861023\n",
      "Epoch :0.48333333333333334    Train Loss :0.0032816813327372074    Test Loss :0.03881703317165375\n",
      "Epoch :0.49166666666666664    Train Loss :0.004197359085083008    Test Loss :0.04039357602596283\n",
      "Epoch :0.5    Train Loss :0.006038280203938484    Test Loss :0.020056406036019325\n",
      "Epoch :0.5083333333333333    Train Loss :0.005286567844450474    Test Loss :0.0373908095061779\n",
      "Epoch :0.5166666666666667    Train Loss :0.003730115480720997    Test Loss :0.05652257055044174\n",
      "Epoch :0.525    Train Loss :0.004749030340462923    Test Loss :0.06722917407751083\n",
      "Epoch :0.5333333333333333    Train Loss :0.0037182702217251062    Test Loss :0.05018320679664612\n",
      "Epoch :0.5416666666666666    Train Loss :0.004187570419162512    Test Loss :0.044632621109485626\n",
      "Epoch :0.55    Train Loss :0.0033546099439263344    Test Loss :0.03841687738895416\n",
      "Epoch :0.5583333333333333    Train Loss :0.003799998667091131    Test Loss :0.05181209370493889\n",
      "Epoch :0.5666666666666667    Train Loss :0.0032274392433464527    Test Loss :0.036121852695941925\n",
      "Epoch :0.575    Train Loss :0.003085910342633724    Test Loss :0.03507012873888016\n",
      "Epoch :0.5833333333333334    Train Loss :0.004285147879272699    Test Loss :0.04430769756436348\n",
      "Epoch :0.5916666666666667    Train Loss :0.0034645202104002237    Test Loss :0.043707944452762604\n",
      "Epoch :0.6    Train Loss :0.003511667950078845    Test Loss :0.04656702280044556\n",
      "Epoch :0.6083333333333333    Train Loss :0.0054305256344377995    Test Loss :0.049546051770448685\n",
      "Epoch :0.6166666666666667    Train Loss :0.004589755088090897    Test Loss :0.01855642721056938\n",
      "Epoch :0.625    Train Loss :0.0040077636949718    Test Loss :0.03126658871769905\n",
      "Epoch :0.6333333333333333    Train Loss :0.0038925651460886    Test Loss :0.04004507139325142\n",
      "Epoch :0.6416666666666667    Train Loss :0.003432840807363391    Test Loss :0.044945087283849716\n",
      "Epoch :0.65    Train Loss :0.0032223931048065424    Test Loss :0.04805995896458626\n",
      "Epoch :0.6583333333333333    Train Loss :0.0036807742435485125    Test Loss :0.06162085756659508\n",
      "Epoch :0.6666666666666666    Train Loss :0.004600677639245987    Test Loss :0.05133308470249176\n",
      "Epoch :0.675    Train Loss :0.006286394316703081    Test Loss :0.04607801511883736\n",
      "Epoch :0.6833333333333333    Train Loss :0.004357817582786083    Test Loss :0.011916117742657661\n",
      "Epoch :0.6916666666666667    Train Loss :0.00481981597840786    Test Loss :0.013073285110294819\n",
      "Epoch :0.7    Train Loss :0.004260668531060219    Test Loss :0.04346020147204399\n",
      "Epoch :0.7083333333333334    Train Loss :0.0038676313124597073    Test Loss :0.04942633584141731\n",
      "Epoch :0.7166666666666667    Train Loss :0.003123384201899171    Test Loss :0.05608067661523819\n",
      "Epoch :0.725    Train Loss :0.004107299726456404    Test Loss :0.0693102777004242\n",
      "Epoch :0.7333333333333333    Train Loss :0.0036710472777485847    Test Loss :0.05030909180641174\n",
      "Epoch :0.7416666666666667    Train Loss :0.0036941280122846365    Test Loss :0.031155843287706375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.75    Train Loss :0.0037197645287960768    Test Loss :0.037081122398376465\n",
      "Epoch :0.7583333333333333    Train Loss :0.003414029022678733    Test Loss :0.040227338671684265\n",
      "Epoch :0.7666666666666667    Train Loss :0.002784201642498374    Test Loss :0.05420948937535286\n",
      "Epoch :0.775    Train Loss :0.004131841007620096    Test Loss :0.06223400682210922\n",
      "Epoch :0.7833333333333333    Train Loss :0.004718045704066753    Test Loss :0.04277074709534645\n",
      "Epoch :0.7916666666666666    Train Loss :0.003769459668546915    Test Loss :0.02868911437690258\n",
      "Epoch :0.8    Train Loss :0.004622337408363819    Test Loss :0.030711200088262558\n",
      "Epoch :0.8083333333333333    Train Loss :0.003600829979404807    Test Loss :0.03316523879766464\n",
      "Epoch :0.8166666666666667    Train Loss :0.0032956621143966913    Test Loss :0.04760634899139404\n",
      "Epoch :0.825    Train Loss :0.0034761305432766676    Test Loss :0.05065629258751869\n",
      "Epoch :0.8333333333333334    Train Loss :0.0032009619753807783    Test Loss :0.05360296741127968\n",
      "Epoch :0.8416666666666667    Train Loss :0.0027295046020299196    Test Loss :0.03703857585787773\n",
      "Epoch :0.85    Train Loss :0.004224419593811035    Test Loss :0.044643912464380264\n",
      "Epoch :0.8583333333333333    Train Loss :0.0037660771049559116    Test Loss :0.03900191932916641\n",
      "Epoch :0.8666666666666667    Train Loss :0.003791822586208582    Test Loss :0.04957441985607147\n",
      "Epoch :0.875    Train Loss :0.0038225322496145964    Test Loss :0.048165563493967056\n",
      "Epoch :0.8833333333333333    Train Loss :0.0039199343882501125    Test Loss :0.042077984660863876\n",
      "Epoch :0.8916666666666667    Train Loss :0.0031770893838256598    Test Loss :0.040062323212623596\n",
      "Epoch :0.9    Train Loss :0.0034113526344299316    Test Loss :0.042425744235515594\n",
      "Epoch :0.9083333333333333    Train Loss :0.002892895136028528    Test Loss :0.05327626317739487\n",
      "Epoch :0.9166666666666666    Train Loss :0.002669624052941799    Test Loss :0.05593225359916687\n",
      "Epoch :0.925    Train Loss :0.0025888208765536547    Test Loss :0.05392109602689743\n",
      "Epoch :0.9333333333333333    Train Loss :0.0028107110410928726    Test Loss :0.0492890402674675\n",
      "Epoch :0.9416666666666667    Train Loss :0.002782700816169381    Test Loss :0.0385519377887249\n",
      "Epoch :0.95    Train Loss :0.0030724278185516596    Test Loss :0.03907672315835953\n",
      "Epoch :0.9583333333333334    Train Loss :0.003196615492925048    Test Loss :0.04863026365637779\n",
      "Epoch :0.9666666666666667    Train Loss :0.0023910836316645145    Test Loss :0.0603988878428936\n",
      "Epoch :0.975    Train Loss :0.0036824895069003105    Test Loss :0.07042744755744934\n",
      "Epoch :0.9833333333333333    Train Loss :0.0028833262622356415    Test Loss :0.04577001556754112\n",
      "Epoch :0.9916666666666667    Train Loss :0.0032477802596986294    Test Loss :0.03948517516255379\n",
      "Epoch :1.0    Train Loss :0.0030135521665215492    Test Loss :0.03792555257678032\n",
      "RMSE: 14.741425118029206\n",
      "MAE: 12.234199410251183\n",
      "MAPE: 10.591038847473662%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 600, 'hid_size': 50, 'lr': 0.01, 'num_layers': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  31.0\n",
      "Epoch :0.008333333333333333    Train Loss :0.05885861814022064    Test Loss :0.28884586691856384\n",
      "Epoch :0.016666666666666666    Train Loss :0.04994971677660942    Test Loss :0.1354595124721527\n",
      "Epoch :0.025    Train Loss :0.014861484989523888    Test Loss :0.14606347680091858\n",
      "Epoch :0.03333333333333333    Train Loss :0.010160774923861027    Test Loss :0.013932968489825726\n",
      "Epoch :0.041666666666666664    Train Loss :0.01304565742611885    Test Loss :0.046952418982982635\n",
      "Epoch :0.05    Train Loss :0.009766549803316593    Test Loss :0.011465808376669884\n",
      "Epoch :0.058333333333333334    Train Loss :0.008167710155248642    Test Loss :0.035410378128290176\n",
      "Epoch :0.06666666666666667    Train Loss :0.008043525740504265    Test Loss :0.017704317346215248\n",
      "Epoch :0.075    Train Loss :0.006319887936115265    Test Loss :0.011253239586949348\n",
      "Epoch :0.08333333333333333    Train Loss :0.006352752912789583    Test Loss :0.009547117166221142\n",
      "Epoch :0.09166666666666666    Train Loss :0.006146772298961878    Test Loss :0.014960819855332375\n",
      "Epoch :0.1    Train Loss :0.005841223523020744    Test Loss :0.010805110447108746\n",
      "Epoch :0.10833333333333334    Train Loss :0.005460676737129688    Test Loss :0.013460787944495678\n",
      "Epoch :0.11666666666666667    Train Loss :0.00550107192248106    Test Loss :0.01064862310886383\n",
      "Epoch :0.125    Train Loss :0.0053633819334208965    Test Loss :0.011385860852897167\n",
      "Epoch :0.13333333333333333    Train Loss :0.005250830668956041    Test Loss :0.010343179106712341\n",
      "Epoch :0.14166666666666666    Train Loss :0.004332620184868574    Test Loss :0.010939061641693115\n",
      "Epoch :0.15    Train Loss :0.005354027263820171    Test Loss :0.01517106406390667\n",
      "Epoch :0.15833333333333333    Train Loss :0.004875408485531807    Test Loss :0.009010166861116886\n",
      "Epoch :0.16666666666666666    Train Loss :0.004168233368545771    Test Loss :0.007452826946973801\n",
      "Epoch :0.175    Train Loss :0.004171282984316349    Test Loss :0.008395896293222904\n",
      "Epoch :0.18333333333333332    Train Loss :0.0037160359788686037    Test Loss :0.0065397415310144424\n",
      "Epoch :0.19166666666666668    Train Loss :0.003162542823702097    Test Loss :0.006411334499716759\n",
      "Epoch :0.2    Train Loss :0.0034203354734927416    Test Loss :0.007324662059545517\n",
      "Epoch :0.20833333333333334    Train Loss :0.00325284362770617    Test Loss :0.004736412316560745\n",
      "Epoch :0.21666666666666667    Train Loss :0.003620950737968087    Test Loss :0.005571998190134764\n",
      "Epoch :0.225    Train Loss :0.003689176170155406    Test Loss :0.0058011990040540695\n",
      "Epoch :0.23333333333333334    Train Loss :0.0035255257971584797    Test Loss :0.006555239204317331\n",
      "Epoch :0.24166666666666667    Train Loss :0.0029390291310846806    Test Loss :0.008387263864278793\n",
      "Epoch :0.25    Train Loss :0.002964674262329936    Test Loss :0.007525951601564884\n",
      "Epoch :0.25833333333333336    Train Loss :0.002902283798903227    Test Loss :0.006990720983594656\n",
      "Epoch :0.26666666666666666    Train Loss :0.003052309388294816    Test Loss :0.006600184366106987\n",
      "Epoch :0.275    Train Loss :0.002623386913910508    Test Loss :0.006326464936137199\n",
      "Epoch :0.2833333333333333    Train Loss :0.0032097608782351017    Test Loss :0.00923022162169218\n",
      "Epoch :0.2916666666666667    Train Loss :0.002151692286133766    Test Loss :0.011181866750121117\n",
      "Epoch :0.3    Train Loss :0.0028517907485365868    Test Loss :0.006244364660233259\n",
      "Epoch :0.30833333333333335    Train Loss :0.002345046028494835    Test Loss :0.006999338511377573\n",
      "Epoch :0.31666666666666665    Train Loss :0.002397821517661214    Test Loss :0.007906675338745117\n",
      "Epoch :0.325    Train Loss :0.002442088443785906    Test Loss :0.00609480869024992\n",
      "Epoch :0.3333333333333333    Train Loss :0.0026511407922953367    Test Loss :0.006848336663097143\n",
      "Epoch :0.3416666666666667    Train Loss :0.0027432020287960768    Test Loss :0.0046213362365961075\n",
      "Epoch :0.35    Train Loss :0.0033832546323537827    Test Loss :0.0066663832403719425\n",
      "Epoch :0.35833333333333334    Train Loss :0.003059015143662691    Test Loss :0.010435196571052074\n",
      "Epoch :0.36666666666666664    Train Loss :0.0028109729755669832    Test Loss :0.009454614482820034\n",
      "Epoch :0.375    Train Loss :0.0023823201190680265    Test Loss :0.007971702143549919\n",
      "Epoch :0.38333333333333336    Train Loss :0.0028516168240457773    Test Loss :0.006851758807897568\n",
      "Epoch :0.39166666666666666    Train Loss :0.002251385012641549    Test Loss :0.003411422483623028\n",
      "Epoch :0.4    Train Loss :0.002260803710669279    Test Loss :0.006532181985676289\n",
      "Epoch :0.4083333333333333    Train Loss :0.0023438178468495607    Test Loss :0.00773916020989418\n",
      "Epoch :0.4166666666666667    Train Loss :0.0022253242786973715    Test Loss :0.005467798560857773\n",
      "Epoch :0.425    Train Loss :0.0023716981522738934    Test Loss :0.0042625670321285725\n",
      "Epoch :0.43333333333333335    Train Loss :0.0023260442540049553    Test Loss :0.005088629201054573\n",
      "Epoch :0.44166666666666665    Train Loss :0.002261147368699312    Test Loss :0.006126703228801489\n",
      "Epoch :0.45    Train Loss :0.0024728430435061455    Test Loss :0.005803276784718037\n",
      "Epoch :0.4583333333333333    Train Loss :0.0021030099596828222    Test Loss :0.007405240088701248\n",
      "Epoch :0.4666666666666667    Train Loss :0.0021745357662439346    Test Loss :0.007411212660372257\n",
      "Epoch :0.475    Train Loss :0.002788421930745244    Test Loss :0.007022432517260313\n",
      "Epoch :0.48333333333333334    Train Loss :0.0024852014612406492    Test Loss :0.007002696394920349\n",
      "Epoch :0.49166666666666664    Train Loss :0.002097232034429908    Test Loss :0.00511478167027235\n",
      "Epoch :0.5    Train Loss :0.0021198629401624203    Test Loss :0.008028862066566944\n",
      "Epoch :0.5083333333333333    Train Loss :0.002155603840947151    Test Loss :0.00844411551952362\n",
      "Epoch :0.5166666666666667    Train Loss :0.0022498343605548143    Test Loss :0.008359246887266636\n",
      "Epoch :0.525    Train Loss :0.0021107166539877653    Test Loss :0.007093366235494614\n",
      "Epoch :0.5333333333333333    Train Loss :0.0020733033306896687    Test Loss :0.009832940995693207\n",
      "Epoch :0.5416666666666666    Train Loss :0.0019770481158047915    Test Loss :0.009776169434189796\n",
      "Epoch :0.55    Train Loss :0.001999440835788846    Test Loss :0.006263745483011007\n",
      "Epoch :0.5583333333333333    Train Loss :0.001651262049563229    Test Loss :0.012134501710534096\n",
      "Epoch :0.5666666666666667    Train Loss :0.0022940970957279205    Test Loss :0.008767686784267426\n",
      "Epoch :0.575    Train Loss :0.0018457822734490037    Test Loss :0.013811539858579636\n",
      "Epoch :0.5833333333333334    Train Loss :0.002274339785799384    Test Loss :0.016283836215734482\n",
      "Epoch :0.5916666666666667    Train Loss :0.0026052212342619896    Test Loss :0.009714365936815739\n",
      "Epoch :0.6    Train Loss :0.0023416029289364815    Test Loss :0.006013200618326664\n",
      "Epoch :0.6083333333333333    Train Loss :0.0024991612881422043    Test Loss :0.014455358497798443\n",
      "Epoch :0.6166666666666667    Train Loss :0.002001306740567088    Test Loss :0.020009052008390427\n",
      "Epoch :0.625    Train Loss :0.001832726993598044    Test Loss :0.022285226732492447\n",
      "Epoch :0.6333333333333333    Train Loss :0.002212278777733445    Test Loss :0.02087246999144554\n",
      "Epoch :0.6416666666666667    Train Loss :0.002386941807344556    Test Loss :0.03458277881145477\n",
      "Epoch :0.65    Train Loss :0.0017195312539115548    Test Loss :0.016374191269278526\n",
      "Epoch :0.6583333333333333    Train Loss :0.001912289997562766    Test Loss :0.02830135077238083\n",
      "Epoch :0.6666666666666666    Train Loss :0.0020384490489959717    Test Loss :0.024374540895223618\n",
      "Epoch :0.675    Train Loss :0.0017965161241590977    Test Loss :0.04163334146142006\n",
      "Epoch :0.6833333333333333    Train Loss :0.0018785338615998626    Test Loss :0.0473647341132164\n",
      "Epoch :0.6916666666666667    Train Loss :0.0022087234538048506    Test Loss :0.0719107836484909\n",
      "Epoch :0.7    Train Loss :0.0023699714802205563    Test Loss :0.013357063755393028\n",
      "Epoch :0.7083333333333334    Train Loss :0.001410041586495936    Test Loss :0.06419570744037628\n",
      "Epoch :0.7166666666666667    Train Loss :0.002209776546806097    Test Loss :0.006195832975208759\n",
      "Epoch :0.725    Train Loss :0.0021792021580040455    Test Loss :0.0034224740229547024\n",
      "Epoch :0.7333333333333333    Train Loss :0.0020582834258675575    Test Loss :0.004474382847547531\n",
      "Epoch :0.7416666666666667    Train Loss :0.002392177702859044    Test Loss :0.005419711582362652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.75    Train Loss :0.0026242544408887625    Test Loss :0.0068062711507081985\n",
      "Epoch :0.7583333333333333    Train Loss :0.0019630505703389645    Test Loss :0.014763204380869865\n",
      "Epoch :0.7666666666666667    Train Loss :0.0015416305977851152    Test Loss :0.030296461656689644\n",
      "Epoch :0.775    Train Loss :0.002446261467412114    Test Loss :0.015596719458699226\n",
      "Epoch :0.7833333333333333    Train Loss :0.0019280391279608011    Test Loss :0.01341945119202137\n",
      "Epoch :0.7916666666666666    Train Loss :0.0023135081864893436    Test Loss :0.02402566373348236\n",
      "Epoch :0.8    Train Loss :0.0017138049006462097    Test Loss :0.009477846324443817\n",
      "Epoch :0.8083333333333333    Train Loss :0.0014630433870479465    Test Loss :0.01295256707817316\n",
      "Epoch :0.8166666666666667    Train Loss :0.002038929844275117    Test Loss :0.011577591300010681\n",
      "Epoch :0.825    Train Loss :0.0017149619525298476    Test Loss :0.012376622296869755\n",
      "Epoch :0.8333333333333334    Train Loss :0.002080533653497696    Test Loss :0.015537803061306477\n",
      "Epoch :0.8416666666666667    Train Loss :0.0017132214270532131    Test Loss :0.010616239160299301\n",
      "Epoch :0.85    Train Loss :0.001643530442379415    Test Loss :0.018342528492212296\n",
      "Epoch :0.8583333333333333    Train Loss :0.002401265548542142    Test Loss :0.015396200120449066\n",
      "Epoch :0.8666666666666667    Train Loss :0.0013454757863655686    Test Loss :0.013124959543347359\n",
      "Epoch :0.875    Train Loss :0.0012892886297777295    Test Loss :0.017997315153479576\n",
      "Epoch :0.8833333333333333    Train Loss :0.001383153721690178    Test Loss :0.011626675724983215\n",
      "Epoch :0.8916666666666667    Train Loss :0.0014650230295956135    Test Loss :0.017434675246477127\n",
      "Epoch :0.9    Train Loss :0.0017594574019312859    Test Loss :0.019013112410902977\n",
      "Epoch :0.9083333333333333    Train Loss :0.00134410394821316    Test Loss :0.0164813119918108\n",
      "Epoch :0.9166666666666666    Train Loss :0.0014058526139706373    Test Loss :0.018634887412190437\n",
      "Epoch :0.925    Train Loss :0.0014464312698692083    Test Loss :0.019225863739848137\n",
      "Epoch :0.9333333333333333    Train Loss :0.0011709227692335844    Test Loss :0.01846325770020485\n",
      "Epoch :0.9416666666666667    Train Loss :0.0013033952564001083    Test Loss :0.019376127049326897\n",
      "Epoch :0.95    Train Loss :0.0014161793515086174    Test Loss :0.01809561252593994\n",
      "Epoch :0.9583333333333334    Train Loss :0.0012534885900095105    Test Loss :0.025663936510682106\n",
      "Epoch :0.9666666666666667    Train Loss :0.0010571697494015098    Test Loss :0.021328048780560493\n",
      "Epoch :0.975    Train Loss :0.0013055333402007818    Test Loss :0.01892620325088501\n",
      "Epoch :0.9833333333333333    Train Loss :0.0013317622942849994    Test Loss :0.026965180411934853\n",
      "Epoch :0.9916666666666667    Train Loss :0.001356951193884015    Test Loss :0.020941616967320442\n",
      "Epoch :1.0    Train Loss :0.0016230620676651597    Test Loss :0.01855824515223503\n",
      "RMSE: 18.44972785840713\n",
      "MAE: 15.060411501508465\n",
      "MAPE: 13.13146538241412%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 600, 'hid_size': 50, 'lr': 0.02, 'num_layers': 2}\n",
      "total:  34.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.08480500429868698    Test Loss :0.24023005366325378\n",
      "Epoch :0.016666666666666666    Train Loss :0.060859497636556625    Test Loss :0.27577829360961914\n",
      "Epoch :0.025    Train Loss :0.05305058881640434    Test Loss :0.21375106275081635\n",
      "Epoch :0.03333333333333333    Train Loss :0.05280826985836029    Test Loss :0.2585030496120453\n",
      "Epoch :0.041666666666666664    Train Loss :0.05192611739039421    Test Loss :0.2123754918575287\n",
      "Epoch :0.05    Train Loss :0.051310550421476364    Test Loss :0.23789219558238983\n",
      "Epoch :0.058333333333333334    Train Loss :0.048959240317344666    Test Loss :0.2266780436038971\n",
      "Epoch :0.06666666666666667    Train Loss :0.028188753873109818    Test Loss :0.09040602296590805\n",
      "Epoch :0.075    Train Loss :0.014413044787943363    Test Loss :0.015499972738325596\n",
      "Epoch :0.08333333333333333    Train Loss :0.013204537332057953    Test Loss :0.04188136011362076\n",
      "Epoch :0.09166666666666666    Train Loss :0.012078063562512398    Test Loss :0.021862855181097984\n",
      "Epoch :0.1    Train Loss :0.013063361868262291    Test Loss :0.02316514030098915\n",
      "Epoch :0.10833333333333334    Train Loss :0.012929250486195087    Test Loss :0.02869873121380806\n",
      "Epoch :0.11666666666666667    Train Loss :0.010880708694458008    Test Loss :0.031591080129146576\n",
      "Epoch :0.125    Train Loss :0.011624560691416264    Test Loss :0.017548715695738792\n",
      "Epoch :0.13333333333333333    Train Loss :0.008989007212221622    Test Loss :0.01818535104393959\n",
      "Epoch :0.14166666666666666    Train Loss :0.00902347732335329    Test Loss :0.025781704112887383\n",
      "Epoch :0.15    Train Loss :0.009118042886257172    Test Loss :0.020046161487698555\n",
      "Epoch :0.15833333333333333    Train Loss :0.009214588440954685    Test Loss :0.018104447051882744\n",
      "Epoch :0.16666666666666666    Train Loss :0.008731428533792496    Test Loss :0.018422238528728485\n",
      "Epoch :0.175    Train Loss :0.009486476890742779    Test Loss :0.02358284778892994\n",
      "Epoch :0.18333333333333332    Train Loss :0.00863620825111866    Test Loss :0.016839899122714996\n",
      "Epoch :0.19166666666666668    Train Loss :0.007818185724318027    Test Loss :0.022874820977449417\n",
      "Epoch :0.2    Train Loss :0.0070556350983679295    Test Loss :0.015601628459990025\n",
      "Epoch :0.20833333333333334    Train Loss :0.009127162396907806    Test Loss :0.02115499973297119\n",
      "Epoch :0.21666666666666667    Train Loss :0.007136127911508083    Test Loss :0.018188809975981712\n",
      "Epoch :0.225    Train Loss :0.006978407967835665    Test Loss :0.02048850618302822\n",
      "Epoch :0.23333333333333334    Train Loss :0.0075021712109446526    Test Loss :0.018199358135461807\n",
      "Epoch :0.24166666666666667    Train Loss :0.006942132022231817    Test Loss :0.01616601273417473\n",
      "Epoch :0.25    Train Loss :0.006704367231577635    Test Loss :0.019378462806344032\n",
      "Epoch :0.25833333333333336    Train Loss :0.0065856515429914    Test Loss :0.017788968980312347\n",
      "Epoch :0.26666666666666666    Train Loss :0.006712016183882952    Test Loss :0.019258670508861542\n",
      "Epoch :0.275    Train Loss :0.007256804965436459    Test Loss :0.018956191837787628\n",
      "Epoch :0.2833333333333333    Train Loss :0.006332584656774998    Test Loss :0.014127157628536224\n",
      "Epoch :0.2916666666666667    Train Loss :0.006688904948532581    Test Loss :0.01961185783147812\n",
      "Epoch :0.3    Train Loss :0.006710043177008629    Test Loss :0.01794923096895218\n",
      "Epoch :0.30833333333333335    Train Loss :0.0063233631663024426    Test Loss :0.018663857132196426\n",
      "Epoch :0.31666666666666665    Train Loss :0.005706870928406715    Test Loss :0.014543155208230019\n",
      "Epoch :0.325    Train Loss :0.006755421869456768    Test Loss :0.019673069939017296\n",
      "Epoch :0.3333333333333333    Train Loss :0.006185819394886494    Test Loss :0.019130079075694084\n",
      "Epoch :0.3416666666666667    Train Loss :0.005940926261246204    Test Loss :0.01904682070016861\n",
      "Epoch :0.35    Train Loss :0.006016315892338753    Test Loss :0.01633179374039173\n",
      "Epoch :0.35833333333333334    Train Loss :0.005986048839986324    Test Loss :0.0178376454859972\n",
      "Epoch :0.36666666666666664    Train Loss :0.005791398696601391    Test Loss :0.019365305081009865\n",
      "Epoch :0.375    Train Loss :0.005968455225229263    Test Loss :0.021359296515583992\n",
      "Epoch :0.38333333333333336    Train Loss :0.006060018669813871    Test Loss :0.01799521967768669\n",
      "Epoch :0.39166666666666666    Train Loss :0.006488707847893238    Test Loss :0.017554571852087975\n",
      "Epoch :0.4    Train Loss :0.005461292807012796    Test Loss :0.015890460461378098\n",
      "Epoch :0.4083333333333333    Train Loss :0.0056399754248559475    Test Loss :0.020604846999049187\n",
      "Epoch :0.4166666666666667    Train Loss :0.005622895900160074    Test Loss :0.015489721670746803\n",
      "Epoch :0.425    Train Loss :0.0051048751920461655    Test Loss :0.020821548998355865\n",
      "Epoch :0.43333333333333335    Train Loss :0.0050476109609007835    Test Loss :0.015482250601053238\n",
      "Epoch :0.44166666666666665    Train Loss :0.004850953351706266    Test Loss :0.02694261074066162\n",
      "Epoch :0.45    Train Loss :0.005105159245431423    Test Loss :0.025667555630207062\n",
      "Epoch :0.4583333333333333    Train Loss :0.0056624398566782475    Test Loss :0.027613654732704163\n",
      "Epoch :0.4666666666666667    Train Loss :0.005538242403417826    Test Loss :0.04213332384824753\n",
      "Epoch :0.475    Train Loss :0.004776825197041035    Test Loss :0.03892451897263527\n",
      "Epoch :0.48333333333333334    Train Loss :0.004945867229253054    Test Loss :0.0236319862306118\n",
      "Epoch :0.49166666666666664    Train Loss :0.004466793034225702    Test Loss :0.025149093940854073\n",
      "Epoch :0.5    Train Loss :0.004191726911813021    Test Loss :0.02745303511619568\n",
      "Epoch :0.5083333333333333    Train Loss :0.0051949904300272465    Test Loss :0.026793505996465683\n",
      "Epoch :0.5166666666666667    Train Loss :0.004555174149572849    Test Loss :0.02303386479616165\n",
      "Epoch :0.525    Train Loss :0.005884127225726843    Test Loss :0.03275526314973831\n",
      "Epoch :0.5333333333333333    Train Loss :0.004669351037591696    Test Loss :0.028230421245098114\n",
      "Epoch :0.5416666666666666    Train Loss :0.005013403482735157    Test Loss :0.0184533279389143\n",
      "Epoch :0.55    Train Loss :0.004089732654392719    Test Loss :0.022134611383080482\n",
      "Epoch :0.5583333333333333    Train Loss :0.0044875615276396275    Test Loss :0.028656145557761192\n",
      "Epoch :0.5666666666666667    Train Loss :0.003802717197686434    Test Loss :0.048235081136226654\n",
      "Epoch :0.575    Train Loss :0.004945049528032541    Test Loss :0.05360876023769379\n",
      "Epoch :0.5833333333333334    Train Loss :0.004554147366434336    Test Loss :0.02701704017817974\n",
      "Epoch :0.5916666666666667    Train Loss :0.004421773366630077    Test Loss :0.021649308502674103\n",
      "Epoch :0.6    Train Loss :0.004880967084318399    Test Loss :0.01839783415198326\n",
      "Epoch :0.6083333333333333    Train Loss :0.00433545745909214    Test Loss :0.025439806282520294\n",
      "Epoch :0.6166666666666667    Train Loss :0.004269941709935665    Test Loss :0.03703879937529564\n",
      "Epoch :0.625    Train Loss :0.003498765639960766    Test Loss :0.04722848907113075\n",
      "Epoch :0.6333333333333333    Train Loss :0.0036234664730727673    Test Loss :0.05377351865172386\n",
      "Epoch :0.6416666666666667    Train Loss :0.0040799821726977825    Test Loss :0.052230238914489746\n",
      "Epoch :0.65    Train Loss :0.005134556442499161    Test Loss :0.03954160958528519\n",
      "Epoch :0.6583333333333333    Train Loss :0.004433330148458481    Test Loss :0.03043338470160961\n",
      "Epoch :0.6666666666666666    Train Loss :0.003876106347888708    Test Loss :0.03272426500916481\n",
      "Epoch :0.675    Train Loss :0.003807495813816786    Test Loss :0.025384800508618355\n",
      "Epoch :0.6833333333333333    Train Loss :0.003774723969399929    Test Loss :0.0424615852534771\n",
      "Epoch :0.6916666666666667    Train Loss :0.0032542585395276546    Test Loss :0.0492221936583519\n",
      "Epoch :0.7    Train Loss :0.003123390953987837    Test Loss :0.06057463213801384\n",
      "Epoch :0.7083333333333334    Train Loss :0.003890740219503641    Test Loss :0.04780655726790428\n",
      "Epoch :0.7166666666666667    Train Loss :0.003349433885887265    Test Loss :0.03569226711988449\n",
      "Epoch :0.725    Train Loss :0.003538216697052121    Test Loss :0.03149673715233803\n",
      "Epoch :0.7333333333333333    Train Loss :0.0035150342155247927    Test Loss :0.040604788810014725\n",
      "Epoch :0.7416666666666667    Train Loss :0.0032043519895523787    Test Loss :0.05026914179325104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.75    Train Loss :0.0037362193688750267    Test Loss :0.054886944591999054\n",
      "Epoch :0.7583333333333333    Train Loss :0.004589195363223553    Test Loss :0.038189906626939774\n",
      "Epoch :0.7666666666666667    Train Loss :0.004610861651599407    Test Loss :0.03600824624300003\n",
      "Epoch :0.775    Train Loss :0.004362460691481829    Test Loss :0.026188576593995094\n",
      "Epoch :0.7833333333333333    Train Loss :0.0034701181575655937    Test Loss :0.03387153521180153\n",
      "Epoch :0.7916666666666666    Train Loss :0.0033445751760154963    Test Loss :0.059997979551553726\n",
      "Epoch :0.8    Train Loss :0.0031707000453025103    Test Loss :0.054770197719335556\n",
      "Epoch :0.8083333333333333    Train Loss :0.0036032453645020723    Test Loss :0.06599247455596924\n",
      "Epoch :0.8166666666666667    Train Loss :0.0034537948668003082    Test Loss :0.047368407249450684\n",
      "Epoch :0.825    Train Loss :0.00402838084846735    Test Loss :0.050492316484451294\n",
      "Epoch :0.8333333333333334    Train Loss :0.0040861680172383785    Test Loss :0.03584583103656769\n",
      "Epoch :0.8416666666666667    Train Loss :0.004034074489027262    Test Loss :0.02991640567779541\n",
      "Epoch :0.85    Train Loss :0.0034217017237097025    Test Loss :0.017215048894286156\n",
      "Epoch :0.8583333333333333    Train Loss :0.0037046438083052635    Test Loss :0.008772684261202812\n",
      "Epoch :0.8666666666666667    Train Loss :0.0023311420809477568    Test Loss :0.017444461584091187\n",
      "Epoch :0.875    Train Loss :0.0031436262652277946    Test Loss :0.01305251382291317\n",
      "Epoch :0.8833333333333333    Train Loss :0.003662614617496729    Test Loss :0.015448224730789661\n",
      "Epoch :0.8916666666666667    Train Loss :0.0027452718932181597    Test Loss :0.011786769144237041\n",
      "Epoch :0.9    Train Loss :0.002746633952483535    Test Loss :0.01917226053774357\n",
      "Epoch :0.9083333333333333    Train Loss :0.0027060415595769882    Test Loss :0.026791203767061234\n",
      "Epoch :0.9166666666666666    Train Loss :0.002590575721114874    Test Loss :0.019697275012731552\n",
      "Epoch :0.925    Train Loss :0.0027105179615318775    Test Loss :0.02005910314619541\n",
      "Epoch :0.9333333333333333    Train Loss :0.0027168316300958395    Test Loss :0.02490592747926712\n",
      "Epoch :0.9416666666666667    Train Loss :0.0027975761331617832    Test Loss :0.013250071555376053\n",
      "Epoch :0.95    Train Loss :0.00264337076805532    Test Loss :0.018856994807720184\n",
      "Epoch :0.9583333333333334    Train Loss :0.0027968711219727993    Test Loss :0.02463592030107975\n",
      "Epoch :0.9666666666666667    Train Loss :0.0027630894910544157    Test Loss :0.03034755028784275\n",
      "Epoch :0.975    Train Loss :0.002057046163827181    Test Loss :0.018178021535277367\n",
      "Epoch :0.9833333333333333    Train Loss :0.0024480537977069616    Test Loss :0.027132023125886917\n",
      "Epoch :0.9916666666666667    Train Loss :0.0033210348337888718    Test Loss :0.02029317617416382\n",
      "Epoch :1.0    Train Loss :0.0031929295510053635    Test Loss :0.02724316716194153\n",
      "RMSE: 15.834751752054357\n",
      "MAE: 13.952306779747701\n",
      "MAPE: 12.168721206436716%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 600, 'hid_size': 50, 'lr': 0.02, 'num_layers': 5}\n",
      "total:  38.0\n",
      "Epoch :0.008333333333333333    Train Loss :0.10791092365980148    Test Loss :0.5801051259040833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.016666666666666666    Train Loss :0.10267744213342667    Test Loss :0.4373232126235962\n",
      "Epoch :0.025    Train Loss :0.06691175699234009    Test Loss :0.14833906292915344\n",
      "Epoch :0.03333333333333333    Train Loss :0.05146145448088646    Test Loss :0.2636381983757019\n",
      "Epoch :0.041666666666666664    Train Loss :0.03319500759243965    Test Loss :0.06288144737482071\n",
      "Epoch :0.05    Train Loss :0.020354827865958214    Test Loss :0.047027625143527985\n",
      "Epoch :0.058333333333333334    Train Loss :0.014863457530736923    Test Loss :0.04168461263179779\n",
      "Epoch :0.06666666666666667    Train Loss :0.008589430712163448    Test Loss :0.017455192282795906\n",
      "Epoch :0.075    Train Loss :0.008928443305194378    Test Loss :0.012823202647268772\n",
      "Epoch :0.08333333333333333    Train Loss :0.006845636758953333    Test Loss :0.01391039602458477\n",
      "Epoch :0.09166666666666666    Train Loss :0.008291317149996758    Test Loss :0.0109165720641613\n",
      "Epoch :0.1    Train Loss :0.007071228232234716    Test Loss :0.016353068873286247\n",
      "Epoch :0.10833333333333334    Train Loss :0.007012826856225729    Test Loss :0.012852168641984463\n",
      "Epoch :0.11666666666666667    Train Loss :0.005992104299366474    Test Loss :0.009933573193848133\n",
      "Epoch :0.125    Train Loss :0.00572414044290781    Test Loss :0.01017625629901886\n",
      "Epoch :0.13333333333333333    Train Loss :0.006181242875754833    Test Loss :0.00978879351168871\n",
      "Epoch :0.14166666666666666    Train Loss :0.005641424097120762    Test Loss :0.010299227200448513\n",
      "Epoch :0.15    Train Loss :0.005293365102261305    Test Loss :0.013331139460206032\n",
      "Epoch :0.15833333333333333    Train Loss :0.005050342064350843    Test Loss :0.010761222802102566\n",
      "Epoch :0.16666666666666666    Train Loss :0.004765668883919716    Test Loss :0.012770681641995907\n",
      "Epoch :0.175    Train Loss :0.0055327992886304855    Test Loss :0.019149169325828552\n",
      "Epoch :0.18333333333333332    Train Loss :0.0049437265843153    Test Loss :0.01901819184422493\n",
      "Epoch :0.19166666666666668    Train Loss :0.004540552385151386    Test Loss :0.014540042728185654\n",
      "Epoch :0.2    Train Loss :0.004860696382820606    Test Loss :0.015365909785032272\n",
      "Epoch :0.20833333333333334    Train Loss :0.004617899190634489    Test Loss :0.017747392877936363\n",
      "Epoch :0.21666666666666667    Train Loss :0.004317663609981537    Test Loss :0.02139219269156456\n",
      "Epoch :0.225    Train Loss :0.00458091264590621    Test Loss :0.02363959699869156\n",
      "Epoch :0.23333333333333334    Train Loss :0.004134379327297211    Test Loss :0.017196424305438995\n",
      "Epoch :0.24166666666666667    Train Loss :0.003928087186068296    Test Loss :0.01806977577507496\n",
      "Epoch :0.25    Train Loss :0.003749014111235738    Test Loss :0.02301286906003952\n",
      "Epoch :0.25833333333333336    Train Loss :0.004053866490721703    Test Loss :0.01566331833600998\n",
      "Epoch :0.26666666666666666    Train Loss :0.0036273631267249584    Test Loss :0.015068250708281994\n",
      "Epoch :0.275    Train Loss :0.003142797853797674    Test Loss :0.014070292934775352\n",
      "Epoch :0.2833333333333333    Train Loss :0.003222713014110923    Test Loss :0.01878623478114605\n",
      "Epoch :0.2916666666666667    Train Loss :0.0037632116582244635    Test Loss :0.021206697449088097\n",
      "Epoch :0.3    Train Loss :0.0031187254935503006    Test Loss :0.009246841073036194\n",
      "Epoch :0.30833333333333335    Train Loss :0.0030648745596408844    Test Loss :0.014290455728769302\n",
      "Epoch :0.31666666666666665    Train Loss :0.0033256940077990294    Test Loss :0.01854582317173481\n",
      "Epoch :0.325    Train Loss :0.0029292688705027103    Test Loss :0.023736558854579926\n",
      "Epoch :0.3333333333333333    Train Loss :0.003385148011147976    Test Loss :0.019992850720882416\n",
      "Epoch :0.3416666666666667    Train Loss :0.0025824052281677723    Test Loss :0.015421646647155285\n",
      "Epoch :0.35    Train Loss :0.003520433558151126    Test Loss :0.014700699597597122\n",
      "Epoch :0.35833333333333334    Train Loss :0.002807311248034239    Test Loss :0.029112104326486588\n",
      "Epoch :0.36666666666666664    Train Loss :0.002667865250259638    Test Loss :0.032704614102840424\n",
      "Epoch :0.375    Train Loss :0.003127926029264927    Test Loss :0.026338234543800354\n",
      "Epoch :0.38333333333333336    Train Loss :0.00248853862285614    Test Loss :0.04529724642634392\n",
      "Epoch :0.39166666666666666    Train Loss :0.0031597870402038097    Test Loss :0.01314719207584858\n",
      "Epoch :0.4    Train Loss :0.0027729200664907694    Test Loss :0.02668045461177826\n",
      "Epoch :0.4083333333333333    Train Loss :0.0030939122661948204    Test Loss :0.018671156838536263\n",
      "Epoch :0.4166666666666667    Train Loss :0.002537171123549342    Test Loss :0.02525196596980095\n",
      "Epoch :0.425    Train Loss :0.0031181699596345425    Test Loss :0.02453477308154106\n",
      "Epoch :0.43333333333333335    Train Loss :0.0024539309088140726    Test Loss :0.012383966706693172\n",
      "Epoch :0.44166666666666665    Train Loss :0.0027453869115561247    Test Loss :0.02677255868911743\n",
      "Epoch :0.45    Train Loss :0.0027548950165510178    Test Loss :0.024614231660962105\n",
      "Epoch :0.4583333333333333    Train Loss :0.0022165030241012573    Test Loss :0.029866259545087814\n",
      "Epoch :0.4666666666666667    Train Loss :0.003036810550838709    Test Loss :0.025332732126116753\n",
      "Epoch :0.475    Train Loss :0.002694057999178767    Test Loss :0.02359769307076931\n",
      "Epoch :0.48333333333333334    Train Loss :0.0023234274704009295    Test Loss :0.02705041877925396\n",
      "Epoch :0.49166666666666664    Train Loss :0.0025351003278046846    Test Loss :0.03667278215289116\n",
      "Epoch :0.5    Train Loss :0.0029000008944422007    Test Loss :0.02503024786710739\n",
      "Epoch :0.5083333333333333    Train Loss :0.0026884847320616245    Test Loss :0.028076665475964546\n",
      "Epoch :0.5166666666666667    Train Loss :0.0027841327246278524    Test Loss :0.028728008270263672\n",
      "Epoch :0.525    Train Loss :0.0025671834591776133    Test Loss :0.01932746171951294\n",
      "Epoch :0.5333333333333333    Train Loss :0.0021321983076632023    Test Loss :0.025578975677490234\n",
      "Epoch :0.5416666666666666    Train Loss :0.0019531287252902985    Test Loss :0.024704651907086372\n",
      "Epoch :0.55    Train Loss :0.002417856128886342    Test Loss :0.023276887834072113\n",
      "Epoch :0.5583333333333333    Train Loss :0.002308277180418372    Test Loss :0.02953888103365898\n",
      "Epoch :0.5666666666666667    Train Loss :0.0027983300387859344    Test Loss :0.03563486784696579\n",
      "Epoch :0.575    Train Loss :0.0026752608828246593    Test Loss :0.023542040959000587\n",
      "Epoch :0.5833333333333334    Train Loss :0.0024628834798932076    Test Loss :0.030371591448783875\n",
      "Epoch :0.5916666666666667    Train Loss :0.0031121813226491213    Test Loss :0.028158970177173615\n",
      "Epoch :0.6    Train Loss :0.0026864034589380026    Test Loss :0.030299292877316475\n",
      "Epoch :0.6083333333333333    Train Loss :0.0024022713769227266    Test Loss :0.023393824696540833\n",
      "Epoch :0.6166666666666667    Train Loss :0.0022474743891507387    Test Loss :0.04206482321023941\n",
      "Epoch :0.625    Train Loss :0.002464616671204567    Test Loss :0.0210527665913105\n",
      "Epoch :0.6333333333333333    Train Loss :0.0024693747982382774    Test Loss :0.034171320497989655\n",
      "Epoch :0.6416666666666667    Train Loss :0.0023454995825886726    Test Loss :0.03674822673201561\n",
      "Epoch :0.65    Train Loss :0.002227908931672573    Test Loss :0.05687694624066353\n",
      "Epoch :0.6583333333333333    Train Loss :0.002132437890395522    Test Loss :0.034649964421987534\n",
      "Epoch :0.6666666666666666    Train Loss :0.002110298490151763    Test Loss :0.042891621589660645\n",
      "Epoch :0.675    Train Loss :0.002425720216706395    Test Loss :0.05163968354463577\n",
      "Epoch :0.6833333333333333    Train Loss :0.002135206013917923    Test Loss :0.035409245640039444\n",
      "Epoch :0.6916666666666667    Train Loss :0.002346266992390156    Test Loss :0.03505785018205643\n",
      "Epoch :0.7    Train Loss :0.002384362043812871    Test Loss :0.024764012545347214\n",
      "Epoch :0.7083333333333334    Train Loss :0.0021280182991176844    Test Loss :0.042369622737169266\n",
      "Epoch :0.7166666666666667    Train Loss :0.0019220017129555345    Test Loss :0.03788996860384941\n",
      "Epoch :0.725    Train Loss :0.002128250664100051    Test Loss :0.032429587095975876\n",
      "Epoch :0.7333333333333333    Train Loss :0.002361514838412404    Test Loss :0.04591759294271469\n",
      "Epoch :0.7416666666666667    Train Loss :0.0017534431535750628    Test Loss :0.058365918695926666\n",
      "Epoch :0.75    Train Loss :0.0018038597190752625    Test Loss :0.025875966995954514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7583333333333333    Train Loss :0.002380503574386239    Test Loss :0.05390902981162071\n",
      "Epoch :0.7666666666666667    Train Loss :0.0025632958859205246    Test Loss :0.0366988480091095\n",
      "Epoch :0.775    Train Loss :0.0018407887546345592    Test Loss :0.02779201604425907\n",
      "Epoch :0.7833333333333333    Train Loss :0.002575924387201667    Test Loss :0.04343923181295395\n",
      "Epoch :0.7916666666666666    Train Loss :0.0021815160289406776    Test Loss :0.01930426061153412\n",
      "Epoch :0.8    Train Loss :0.0023727365769445896    Test Loss :0.04674006253480911\n",
      "Epoch :0.8083333333333333    Train Loss :0.0020728677045553923    Test Loss :0.028301902115345\n",
      "Epoch :0.8166666666666667    Train Loss :0.0021538506262004375    Test Loss :0.024462280794978142\n",
      "Epoch :0.825    Train Loss :0.0020523350685834885    Test Loss :0.046436987817287445\n",
      "Epoch :0.8333333333333334    Train Loss :0.001734731369651854    Test Loss :0.02555627003312111\n",
      "Epoch :0.8416666666666667    Train Loss :0.0022105760872364044    Test Loss :0.03460519015789032\n",
      "Epoch :0.85    Train Loss :0.0022178031504154205    Test Loss :0.01994502730667591\n",
      "Epoch :0.8583333333333333    Train Loss :0.0019397406140342355    Test Loss :0.03097202256321907\n",
      "Epoch :0.8666666666666667    Train Loss :0.002556127728894353    Test Loss :0.042466167360544205\n",
      "Epoch :0.875    Train Loss :0.0022566555999219418    Test Loss :0.02798185870051384\n",
      "Epoch :0.8833333333333333    Train Loss :0.0018357491353526711    Test Loss :0.03046971932053566\n",
      "Epoch :0.8916666666666667    Train Loss :0.002726200968027115    Test Loss :0.05887410417199135\n",
      "Epoch :0.9    Train Loss :0.0017558345571160316    Test Loss :0.045375414192676544\n",
      "Epoch :0.9083333333333333    Train Loss :0.002164047211408615    Test Loss :0.030169520527124405\n",
      "Epoch :0.9166666666666666    Train Loss :0.002261264016851783    Test Loss :0.04958365857601166\n",
      "Epoch :0.925    Train Loss :0.0023021060042083263    Test Loss :0.059109870344400406\n",
      "Epoch :0.9333333333333333    Train Loss :0.002223038813099265    Test Loss :0.04219256341457367\n",
      "Epoch :0.9416666666666667    Train Loss :0.002689626067876816    Test Loss :0.05166735500097275\n",
      "Epoch :0.95    Train Loss :0.002194963628426194    Test Loss :0.0548403263092041\n",
      "Epoch :0.9583333333333334    Train Loss :0.0020548454485833645    Test Loss :0.04714170843362808\n",
      "Epoch :0.9666666666666667    Train Loss :0.0021390486508607864    Test Loss :0.043000783771276474\n",
      "Epoch :0.975    Train Loss :0.0016869177343323827    Test Loss :0.06151067465543747\n",
      "Epoch :0.9833333333333333    Train Loss :0.001754269702360034    Test Loss :0.06086403876543045\n",
      "Epoch :0.9916666666666667    Train Loss :0.0019410711247473955    Test Loss :0.05707439407706261\n",
      "Epoch :1.0    Train Loss :0.002160242758691311    Test Loss :0.04693479463458061\n",
      "RMSE: 20.542067980948524\n",
      "MAE: 16.935182377824873\n",
      "MAPE: 14.928561936295662%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 600, 'hid_size': 100, 'lr': 0.01, 'num_layers': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  41.0\n",
      "Epoch :0.008333333333333333    Train Loss :0.06504999846220016    Test Loss :0.29674291610717773\n",
      "Epoch :0.016666666666666666    Train Loss :0.05188377946615219    Test Loss :0.22438944876194\n",
      "Epoch :0.025    Train Loss :0.05176118016242981    Test Loss :0.25387728214263916\n",
      "Epoch :0.03333333333333333    Train Loss :0.042228396981954575    Test Loss :0.054227009415626526\n",
      "Epoch :0.041666666666666664    Train Loss :0.027941500768065453    Test Loss :0.09676018357276917\n",
      "Epoch :0.05    Train Loss :0.01684562861919403    Test Loss :0.10981911420822144\n",
      "Epoch :0.058333333333333334    Train Loss :0.015095394104719162    Test Loss :0.09594108909368515\n",
      "Epoch :0.06666666666666667    Train Loss :0.011866729706525803    Test Loss :0.010272792540490627\n",
      "Epoch :0.075    Train Loss :0.011328490450978279    Test Loss :0.02299346961081028\n",
      "Epoch :0.08333333333333333    Train Loss :0.008901075460016727    Test Loss :0.024090008810162544\n",
      "Epoch :0.09166666666666666    Train Loss :0.008938373997807503    Test Loss :0.01112765446305275\n",
      "Epoch :0.1    Train Loss :0.007569204084575176    Test Loss :0.02778264880180359\n",
      "Epoch :0.10833333333333334    Train Loss :0.008162093348801136    Test Loss :0.014674452133476734\n",
      "Epoch :0.11666666666666667    Train Loss :0.007241482846438885    Test Loss :0.01791723258793354\n",
      "Epoch :0.125    Train Loss :0.007021396420896053    Test Loss :0.022819997742772102\n",
      "Epoch :0.13333333333333333    Train Loss :0.005924107972532511    Test Loss :0.026574356481432915\n",
      "Epoch :0.14166666666666666    Train Loss :0.0060424418188631535    Test Loss :0.02060924656689167\n",
      "Epoch :0.15    Train Loss :0.006299890112131834    Test Loss :0.02307453751564026\n",
      "Epoch :0.15833333333333333    Train Loss :0.006666441448032856    Test Loss :0.028944436460733414\n",
      "Epoch :0.16666666666666666    Train Loss :0.006330625154078007    Test Loss :0.04362044483423233\n",
      "Epoch :0.175    Train Loss :0.005294961389154196    Test Loss :0.03652144595980644\n",
      "Epoch :0.18333333333333332    Train Loss :0.005286973435431719    Test Loss :0.031628359109163284\n",
      "Epoch :0.19166666666666668    Train Loss :0.006815946660935879    Test Loss :0.03830491751432419\n",
      "Epoch :0.2    Train Loss :0.005557747557759285    Test Loss :0.03731122985482216\n",
      "Epoch :0.20833333333333334    Train Loss :0.0053406679071486    Test Loss :0.027013413608074188\n",
      "Epoch :0.21666666666666667    Train Loss :0.005063517950475216    Test Loss :0.043365899473428726\n",
      "Epoch :0.225    Train Loss :0.004660078324377537    Test Loss :0.041613463312387466\n",
      "Epoch :0.23333333333333334    Train Loss :0.004562439862638712    Test Loss :0.0274495966732502\n",
      "Epoch :0.24166666666666667    Train Loss :0.004830173682421446    Test Loss :0.041681088507175446\n",
      "Epoch :0.25    Train Loss :0.004762665834277868    Test Loss :0.05256419628858566\n",
      "Epoch :0.25833333333333336    Train Loss :0.004628545138984919    Test Loss :0.05781355872750282\n",
      "Epoch :0.26666666666666666    Train Loss :0.00425027497112751    Test Loss :0.041864749044179916\n",
      "Epoch :0.275    Train Loss :0.00430805841460824    Test Loss :0.03541630133986473\n",
      "Epoch :0.2833333333333333    Train Loss :0.0048312111757695675    Test Loss :0.03344344347715378\n",
      "Epoch :0.2916666666666667    Train Loss :0.004474496468901634    Test Loss :0.02643570490181446\n",
      "Epoch :0.3    Train Loss :0.004924639593809843    Test Loss :0.04350126534700394\n",
      "Epoch :0.30833333333333335    Train Loss :0.004759561270475388    Test Loss :0.04392628371715546\n",
      "Epoch :0.31666666666666665    Train Loss :0.0038350028917193413    Test Loss :0.03661797195672989\n",
      "Epoch :0.325    Train Loss :0.004010164178907871    Test Loss :0.043521929532289505\n",
      "Epoch :0.3333333333333333    Train Loss :0.003910738509148359    Test Loss :0.047148238867521286\n",
      "Epoch :0.3416666666666667    Train Loss :0.004062898922711611    Test Loss :0.03101525455713272\n",
      "Epoch :0.35    Train Loss :0.0035072716418653727    Test Loss :0.06791674345731735\n",
      "Epoch :0.35833333333333334    Train Loss :0.0037629695143550634    Test Loss :0.05466437339782715\n",
      "Epoch :0.36666666666666664    Train Loss :0.0037388172931969166    Test Loss :0.027277804911136627\n",
      "Epoch :0.375    Train Loss :0.00408686650916934    Test Loss :0.04822220280766487\n",
      "Epoch :0.38333333333333336    Train Loss :0.0033742517698556185    Test Loss :0.0501028411090374\n",
      "Epoch :0.39166666666666666    Train Loss :0.0038147082086652517    Test Loss :0.049576494842767715\n",
      "Epoch :0.4    Train Loss :0.003757752012461424    Test Loss :0.0585339218378067\n",
      "Epoch :0.4083333333333333    Train Loss :0.003526517655700445    Test Loss :0.048177242279052734\n",
      "Epoch :0.4166666666666667    Train Loss :0.003459977451711893    Test Loss :0.04530591890215874\n",
      "Epoch :0.425    Train Loss :0.0033221638295799494    Test Loss :0.054029252380132675\n",
      "Epoch :0.43333333333333335    Train Loss :0.003039114410057664    Test Loss :0.04654334858059883\n",
      "Epoch :0.44166666666666665    Train Loss :0.004565476905554533    Test Loss :0.016973525285720825\n",
      "Epoch :0.45    Train Loss :0.003416333580389619    Test Loss :0.02925267070531845\n",
      "Epoch :0.4583333333333333    Train Loss :0.003395277773961425    Test Loss :0.0235181525349617\n",
      "Epoch :0.4666666666666667    Train Loss :0.0027553134132176638    Test Loss :0.02466150000691414\n",
      "Epoch :0.475    Train Loss :0.00279233418405056    Test Loss :0.016903268173336983\n",
      "Epoch :0.48333333333333334    Train Loss :0.0035260673612356186    Test Loss :0.028593959286808968\n",
      "Epoch :0.49166666666666664    Train Loss :0.002884289715439081    Test Loss :0.03746466711163521\n",
      "Epoch :0.5    Train Loss :0.002845802577212453    Test Loss :0.029055695980787277\n",
      "Epoch :0.5083333333333333    Train Loss :0.0029896923806518316    Test Loss :0.036247286945581436\n",
      "Epoch :0.5166666666666667    Train Loss :0.004324307199567556    Test Loss :0.03425055742263794\n",
      "Epoch :0.525    Train Loss :0.0024638178292661905    Test Loss :0.02034367434680462\n",
      "Epoch :0.5333333333333333    Train Loss :0.0032010304275900126    Test Loss :0.04461506009101868\n",
      "Epoch :0.5416666666666666    Train Loss :0.0034947937820106745    Test Loss :0.027265513315796852\n",
      "Epoch :0.55    Train Loss :0.003687625052407384    Test Loss :0.02557247504591942\n",
      "Epoch :0.5583333333333333    Train Loss :0.0027225916273891926    Test Loss :0.0284188874065876\n",
      "Epoch :0.5666666666666667    Train Loss :0.002818566281348467    Test Loss :0.023560618981719017\n",
      "Epoch :0.575    Train Loss :0.0025205924175679684    Test Loss :0.026797235012054443\n",
      "Epoch :0.5833333333333334    Train Loss :0.0024000494740903378    Test Loss :0.0372568778693676\n",
      "Epoch :0.5916666666666667    Train Loss :0.00265699764713645    Test Loss :0.04102512076497078\n",
      "Epoch :0.6    Train Loss :0.003003393067047    Test Loss :0.021966343745589256\n",
      "Epoch :0.6083333333333333    Train Loss :0.0032842557411640882    Test Loss :0.03000960871577263\n",
      "Epoch :0.6166666666666667    Train Loss :0.002334226155653596    Test Loss :0.05411273241043091\n",
      "Epoch :0.625    Train Loss :0.003329615807160735    Test Loss :0.02990884520113468\n",
      "Epoch :0.6333333333333333    Train Loss :0.002743031596764922    Test Loss :0.026203060522675514\n",
      "Epoch :0.6416666666666667    Train Loss :0.0032580955885350704    Test Loss :0.028287287801504135\n",
      "Epoch :0.65    Train Loss :0.002498014597222209    Test Loss :0.03030667081475258\n",
      "Epoch :0.6583333333333333    Train Loss :0.0025331757497042418    Test Loss :0.0327145978808403\n",
      "Epoch :0.6666666666666666    Train Loss :0.0025108549743890762    Test Loss :0.031258437782526016\n",
      "Epoch :0.675    Train Loss :0.0019773771055042744    Test Loss :0.028012849390506744\n",
      "Epoch :0.6833333333333333    Train Loss :0.002282319124788046    Test Loss :0.021930133923888206\n",
      "Epoch :0.6916666666666667    Train Loss :0.0028840682934969664    Test Loss :0.03769359365105629\n",
      "Epoch :0.7    Train Loss :0.002445558086037636    Test Loss :0.027321182191371918\n",
      "Epoch :0.7083333333333334    Train Loss :0.0021894890815019608    Test Loss :0.03325733542442322\n",
      "Epoch :0.7166666666666667    Train Loss :0.0028559898491948843    Test Loss :0.035551007837057114\n",
      "Epoch :0.725    Train Loss :0.002468804595991969    Test Loss :0.021626248955726624\n",
      "Epoch :0.7333333333333333    Train Loss :0.002325726905837655    Test Loss :0.024616075679659843\n",
      "Epoch :0.7416666666666667    Train Loss :0.002674542833119631    Test Loss :0.03578154742717743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.75    Train Loss :0.0026688186917454004    Test Loss :0.027735795825719833\n",
      "Epoch :0.7583333333333333    Train Loss :0.0021395853254944086    Test Loss :0.03702761232852936\n",
      "Epoch :0.7666666666666667    Train Loss :0.0023023311514407396    Test Loss :0.02746272273361683\n",
      "Epoch :0.775    Train Loss :0.002241662936285138    Test Loss :0.02906329371035099\n",
      "Epoch :0.7833333333333333    Train Loss :0.0025109725538641214    Test Loss :0.043797776103019714\n",
      "Epoch :0.7916666666666666    Train Loss :0.002034282311797142    Test Loss :0.040246445685625076\n",
      "Epoch :0.8    Train Loss :0.002088968176394701    Test Loss :0.02753041684627533\n",
      "Epoch :0.8083333333333333    Train Loss :0.0016727044712752104    Test Loss :0.03558550775051117\n",
      "Epoch :0.8166666666666667    Train Loss :0.0035334688145667315    Test Loss :0.02204037271440029\n",
      "Epoch :0.825    Train Loss :0.0025673347990959883    Test Loss :0.04696041718125343\n",
      "Epoch :0.8333333333333334    Train Loss :0.0020866591949015856    Test Loss :0.052902430295944214\n",
      "Epoch :0.8416666666666667    Train Loss :0.003272747853770852    Test Loss :0.024639766663312912\n",
      "Epoch :0.85    Train Loss :0.0025663869455456734    Test Loss :0.04559788480401039\n",
      "Epoch :0.8583333333333333    Train Loss :0.00343532208353281    Test Loss :0.023565517738461494\n",
      "Epoch :0.8666666666666667    Train Loss :0.0026169728953391314    Test Loss :0.024876657873392105\n",
      "Epoch :0.875    Train Loss :0.002154630608856678    Test Loss :0.02312915213406086\n",
      "Epoch :0.8833333333333333    Train Loss :0.0022309673950076103    Test Loss :0.06659185141324997\n",
      "Epoch :0.8916666666666667    Train Loss :0.0020950643811374903    Test Loss :0.06861326098442078\n",
      "Epoch :0.9    Train Loss :0.0016887433594092727    Test Loss :0.026149576529860497\n",
      "Epoch :0.9083333333333333    Train Loss :0.0023295844439417124    Test Loss :0.030984103679656982\n",
      "Epoch :0.9166666666666666    Train Loss :0.002089150482788682    Test Loss :0.014883722178637981\n",
      "Epoch :0.925    Train Loss :0.0016330622602254152    Test Loss :0.03934426233172417\n",
      "Epoch :0.9333333333333333    Train Loss :0.0020124022848904133    Test Loss :0.02624855749309063\n",
      "Epoch :0.9416666666666667    Train Loss :0.0021688758861273527    Test Loss :0.03033193200826645\n",
      "Epoch :0.95    Train Loss :0.0019316294929012656    Test Loss :0.03315816819667816\n",
      "Epoch :0.9583333333333334    Train Loss :0.001971430378034711    Test Loss :0.022788602858781815\n",
      "Epoch :0.9666666666666667    Train Loss :0.00172606750857085    Test Loss :0.037079498171806335\n",
      "Epoch :0.975    Train Loss :0.0017985575832426548    Test Loss :0.04947056621313095\n",
      "Epoch :0.9833333333333333    Train Loss :0.0017661647871136665    Test Loss :0.0475577712059021\n",
      "Epoch :0.9916666666666667    Train Loss :0.0015285135013982654    Test Loss :0.04560991749167442\n",
      "Epoch :1.0    Train Loss :0.001995261525735259    Test Loss :0.03790340572595596\n",
      "RMSE: 15.004569235960217\n",
      "MAE: 12.67666960600118\n",
      "MAPE: 11.120416195258787%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 600, 'hid_size': 100, 'lr': 0.01, 'num_layers': 5}\n",
      "total:  44.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.11501780897378922    Test Loss :0.3827786445617676\n",
      "Epoch :0.016666666666666666    Train Loss :0.03984390199184418    Test Loss :0.08311374485492706\n",
      "Epoch :0.025    Train Loss :0.05404181033372879    Test Loss :0.027365993708372116\n",
      "Epoch :0.03333333333333333    Train Loss :0.061530448496341705    Test Loss :0.3302667438983917\n",
      "Epoch :0.041666666666666664    Train Loss :0.04529186338186264    Test Loss :0.18106722831726074\n",
      "Epoch :0.05    Train Loss :0.04895051196217537    Test Loss :0.13393983244895935\n",
      "Epoch :0.058333333333333334    Train Loss :0.04151495546102524    Test Loss :0.17778995633125305\n",
      "Epoch :0.06666666666666667    Train Loss :0.03506290167570114    Test Loss :0.1510552167892456\n",
      "Epoch :0.075    Train Loss :0.022741490975022316    Test Loss :0.04100577533245087\n",
      "Epoch :0.08333333333333333    Train Loss :0.011191564612090588    Test Loss :0.023097068071365356\n",
      "Epoch :0.09166666666666666    Train Loss :0.012358477339148521    Test Loss :0.04877875745296478\n",
      "Epoch :0.1    Train Loss :0.008097311481833458    Test Loss :0.013268502429127693\n",
      "Epoch :0.10833333333333334    Train Loss :0.008731497451663017    Test Loss :0.02553146332502365\n",
      "Epoch :0.11666666666666667    Train Loss :0.006877421867102385    Test Loss :0.008334360085427761\n",
      "Epoch :0.125    Train Loss :0.0073142582550644875    Test Loss :0.007612923160195351\n",
      "Epoch :0.13333333333333333    Train Loss :0.007257436402142048    Test Loss :0.009449834004044533\n",
      "Epoch :0.14166666666666666    Train Loss :0.007508441340178251    Test Loss :0.018451852723956108\n",
      "Epoch :0.15    Train Loss :0.005968688055872917    Test Loss :0.008729573339223862\n",
      "Epoch :0.15833333333333333    Train Loss :0.006568568758666515    Test Loss :0.0096080731600523\n",
      "Epoch :0.16666666666666666    Train Loss :0.006857795175164938    Test Loss :0.011829935014247894\n",
      "Epoch :0.175    Train Loss :0.006041941698640585    Test Loss :0.009390115737915039\n",
      "Epoch :0.18333333333333332    Train Loss :0.00553623354062438    Test Loss :0.0057733263820409775\n",
      "Epoch :0.19166666666666668    Train Loss :0.005702163092792034    Test Loss :0.01043091993778944\n",
      "Epoch :0.2    Train Loss :0.005551752168685198    Test Loss :0.01025026198476553\n",
      "Epoch :0.20833333333333334    Train Loss :0.004675130359828472    Test Loss :0.011580816470086575\n",
      "Epoch :0.21666666666666667    Train Loss :0.005291512701660395    Test Loss :0.014453629963099957\n",
      "Epoch :0.225    Train Loss :0.004705218132585287    Test Loss :0.010787297040224075\n",
      "Epoch :0.23333333333333334    Train Loss :0.00517937121912837    Test Loss :0.011946331709623337\n",
      "Epoch :0.24166666666666667    Train Loss :0.00533472141250968    Test Loss :0.01195866521447897\n",
      "Epoch :0.25    Train Loss :0.005208110436797142    Test Loss :0.014233380556106567\n",
      "Epoch :0.25833333333333336    Train Loss :0.004729040898382664    Test Loss :0.012078562751412392\n",
      "Epoch :0.26666666666666666    Train Loss :0.004656536038964987    Test Loss :0.015503548085689545\n",
      "Epoch :0.275    Train Loss :0.005047979764640331    Test Loss :0.012421325780451298\n",
      "Epoch :0.2833333333333333    Train Loss :0.004561819136142731    Test Loss :0.014868492260575294\n",
      "Epoch :0.2916666666666667    Train Loss :0.004919238388538361    Test Loss :0.014607112854719162\n",
      "Epoch :0.3    Train Loss :0.004758845083415508    Test Loss :0.023344680666923523\n",
      "Epoch :0.30833333333333335    Train Loss :0.003970201127231121    Test Loss :0.013621748425066471\n",
      "Epoch :0.31666666666666665    Train Loss :0.004004732705652714    Test Loss :0.016686372458934784\n",
      "Epoch :0.325    Train Loss :0.004081934690475464    Test Loss :0.018999965861439705\n",
      "Epoch :0.3333333333333333    Train Loss :0.004423075821250677    Test Loss :0.02065999247133732\n",
      "Epoch :0.3416666666666667    Train Loss :0.004137314856052399    Test Loss :0.026968535035848618\n",
      "Epoch :0.35    Train Loss :0.004532290622591972    Test Loss :0.024195032194256783\n",
      "Epoch :0.35833333333333334    Train Loss :0.004059644415974617    Test Loss :0.024053283035755157\n",
      "Epoch :0.36666666666666664    Train Loss :0.004252512939274311    Test Loss :0.04024224728345871\n",
      "Epoch :0.375    Train Loss :0.004342806991189718    Test Loss :0.019794220104813576\n",
      "Epoch :0.38333333333333336    Train Loss :0.004563583992421627    Test Loss :0.03186798095703125\n",
      "Epoch :0.39166666666666666    Train Loss :0.004242854192852974    Test Loss :0.027044713497161865\n",
      "Epoch :0.4    Train Loss :0.004216635134071112    Test Loss :0.03785296529531479\n",
      "Epoch :0.4083333333333333    Train Loss :0.004189592320472002    Test Loss :0.03431074321269989\n",
      "Epoch :0.4166666666666667    Train Loss :0.003302023746073246    Test Loss :0.0324249304831028\n",
      "Epoch :0.425    Train Loss :0.0037711409386247396    Test Loss :0.03330207243561745\n",
      "Epoch :0.43333333333333335    Train Loss :0.003677968168631196    Test Loss :0.03371218219399452\n",
      "Epoch :0.44166666666666665    Train Loss :0.003469108836725354    Test Loss :0.040001433342695236\n",
      "Epoch :0.45    Train Loss :0.0039027950260788202    Test Loss :0.03616120293736458\n",
      "Epoch :0.4583333333333333    Train Loss :0.0036070644855499268    Test Loss :0.03815009072422981\n",
      "Epoch :0.4666666666666667    Train Loss :0.004227495286613703    Test Loss :0.03874453529715538\n",
      "Epoch :0.475    Train Loss :0.0032555784564465284    Test Loss :0.040600698441267014\n",
      "Epoch :0.48333333333333334    Train Loss :0.003945507574826479    Test Loss :0.040564317256212234\n",
      "Epoch :0.49166666666666664    Train Loss :0.0031739005353301764    Test Loss :0.03086753748357296\n",
      "Epoch :0.5    Train Loss :0.004097691271454096    Test Loss :0.04419536888599396\n",
      "Epoch :0.5083333333333333    Train Loss :0.003390341764315963    Test Loss :0.044928062707185745\n",
      "Epoch :0.5166666666666667    Train Loss :0.003471747972071171    Test Loss :0.052199315279722214\n",
      "Epoch :0.525    Train Loss :0.002735408255830407    Test Loss :0.03984402120113373\n",
      "Epoch :0.5333333333333333    Train Loss :0.002935949247330427    Test Loss :0.05869613215327263\n",
      "Epoch :0.5416666666666666    Train Loss :0.0031501937191933393    Test Loss :0.039687737822532654\n",
      "Epoch :0.55    Train Loss :0.0030432073399424553    Test Loss :0.05361524224281311\n",
      "Epoch :0.5583333333333333    Train Loss :0.0030119901057332754    Test Loss :0.058397434651851654\n",
      "Epoch :0.5666666666666667    Train Loss :0.0035996194928884506    Test Loss :0.054891977459192276\n",
      "Epoch :0.575    Train Loss :0.003106069518253207    Test Loss :0.05572777986526489\n",
      "Epoch :0.5833333333333334    Train Loss :0.002943963510915637    Test Loss :0.06087799742817879\n",
      "Epoch :0.5916666666666667    Train Loss :0.0030323124956339598    Test Loss :0.037671200931072235\n",
      "Epoch :0.6    Train Loss :0.0029709695372730494    Test Loss :0.03762199729681015\n",
      "Epoch :0.6083333333333333    Train Loss :0.0030953106470406055    Test Loss :0.03885624557733536\n",
      "Epoch :0.6166666666666667    Train Loss :0.0029486764688044786    Test Loss :0.04325782507658005\n",
      "Epoch :0.625    Train Loss :0.0030284863896667957    Test Loss :0.043420616537332535\n",
      "Epoch :0.6333333333333333    Train Loss :0.002746269106864929    Test Loss :0.041721004992723465\n",
      "Epoch :0.6416666666666667    Train Loss :0.0031736474484205246    Test Loss :0.05297456681728363\n",
      "Epoch :0.65    Train Loss :0.0030781179666519165    Test Loss :0.030429746955633163\n",
      "Epoch :0.6583333333333333    Train Loss :0.0026450480800122023    Test Loss :0.04479062929749489\n",
      "Epoch :0.6666666666666666    Train Loss :0.002858741907402873    Test Loss :0.03469788655638695\n",
      "Epoch :0.675    Train Loss :0.0027631816919893026    Test Loss :0.04778368026018143\n",
      "Epoch :0.6833333333333333    Train Loss :0.002823465969413519    Test Loss :0.040645159780979156\n",
      "Epoch :0.6916666666666667    Train Loss :0.0026154527440667152    Test Loss :0.06515860557556152\n",
      "Epoch :0.7    Train Loss :0.00272607640363276    Test Loss :0.05643605440855026\n",
      "Epoch :0.7083333333333334    Train Loss :0.0027309057768434286    Test Loss :0.05631788447499275\n",
      "Epoch :0.7166666666666667    Train Loss :0.0028188175056129694    Test Loss :0.07596283406019211\n",
      "Epoch :0.725    Train Loss :0.0027324638795107603    Test Loss :0.04656433314085007\n",
      "Epoch :0.7333333333333333    Train Loss :0.0028565346729010344    Test Loss :0.08714418858289719\n",
      "Epoch :0.7416666666666667    Train Loss :0.0026194266974925995    Test Loss :0.06325952708721161\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.75    Train Loss :0.0027921341825276613    Test Loss :0.07982262223958969\n",
      "Epoch :0.7583333333333333    Train Loss :0.0024174563586711884    Test Loss :0.07501097768545151\n",
      "Epoch :0.7666666666666667    Train Loss :0.002777701010927558    Test Loss :0.07463070750236511\n",
      "Epoch :0.775    Train Loss :0.002603940898552537    Test Loss :0.07434716075658798\n",
      "Epoch :0.7833333333333333    Train Loss :0.0024321109522134066    Test Loss :0.086442731320858\n",
      "Epoch :0.7916666666666666    Train Loss :0.002302545355632901    Test Loss :0.10359664261341095\n",
      "Epoch :0.8    Train Loss :0.00302149448543787    Test Loss :0.09860631823539734\n",
      "Epoch :0.8083333333333333    Train Loss :0.00271666725166142    Test Loss :0.10603748261928558\n",
      "Epoch :0.8166666666666667    Train Loss :0.00257867225445807    Test Loss :0.08634992688894272\n",
      "Epoch :0.825    Train Loss :0.002908755326643586    Test Loss :0.05779290571808815\n",
      "Epoch :0.8333333333333334    Train Loss :0.002590070478618145    Test Loss :0.07657822221517563\n",
      "Epoch :0.8416666666666667    Train Loss :0.002529764547944069    Test Loss :0.10163695365190506\n",
      "Epoch :0.85    Train Loss :0.0028146523982286453    Test Loss :0.10109613090753555\n",
      "Epoch :0.8583333333333333    Train Loss :0.002542199334129691    Test Loss :0.08732057362794876\n",
      "Epoch :0.8666666666666667    Train Loss :0.0023318964522331953    Test Loss :0.060426924377679825\n",
      "Epoch :0.875    Train Loss :0.0029035587795078754    Test Loss :0.061619069427251816\n",
      "Epoch :0.8833333333333333    Train Loss :0.002326062647625804    Test Loss :0.0682719498872757\n",
      "Epoch :0.8916666666666667    Train Loss :0.0028014974668622017    Test Loss :0.09010428935289383\n",
      "Epoch :0.9    Train Loss :0.0023250693920999765    Test Loss :0.08737358450889587\n",
      "Epoch :0.9083333333333333    Train Loss :0.0026200294960290194    Test Loss :0.09287045896053314\n",
      "Epoch :0.9166666666666666    Train Loss :0.0023805159144103527    Test Loss :0.07973737269639969\n",
      "Epoch :0.925    Train Loss :0.002686202060431242    Test Loss :0.09092709422111511\n",
      "Epoch :0.9333333333333333    Train Loss :0.0025033720303326845    Test Loss :0.07004031538963318\n",
      "Epoch :0.9416666666666667    Train Loss :0.0027868759352713823    Test Loss :0.08584822714328766\n",
      "Epoch :0.95    Train Loss :0.002263362053781748    Test Loss :0.07176034152507782\n",
      "Epoch :0.9583333333333334    Train Loss :0.0026581340935081244    Test Loss :0.08952659368515015\n",
      "Epoch :0.9666666666666667    Train Loss :0.0023408543784171343    Test Loss :0.09717196226119995\n",
      "Epoch :0.975    Train Loss :0.0029292271938174963    Test Loss :0.053551461547613144\n",
      "Epoch :0.9833333333333333    Train Loss :0.002195963403210044    Test Loss :0.09159750491380692\n",
      "Epoch :0.9916666666666667    Train Loss :0.002597580896690488    Test Loss :0.08630441129207611\n",
      "Epoch :1.0    Train Loss :0.002112375572323799    Test Loss :0.08408223092556\n",
      "RMSE: 20.23497956445277\n",
      "MAE: 16.90771153753734\n",
      "MAPE: 14.67273175775889%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 600, 'hid_size': 100, 'lr': 0.02, 'num_layers': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  47.0\n",
      "Epoch :0.008333333333333333    Train Loss :0.1768544614315033    Test Loss :1.273419737815857\n",
      "Epoch :0.016666666666666666    Train Loss :0.06902888417243958    Test Loss :0.3012799620628357\n",
      "Epoch :0.025    Train Loss :0.05384109169244766    Test Loss :0.13341309130191803\n",
      "Epoch :0.03333333333333333    Train Loss :0.05971753969788551    Test Loss :0.3310369849205017\n",
      "Epoch :0.041666666666666664    Train Loss :0.05762379616498947    Test Loss :0.19109052419662476\n",
      "Epoch :0.05    Train Loss :0.05343814194202423    Test Loss :0.23478691279888153\n",
      "Epoch :0.058333333333333334    Train Loss :0.051555220037698746    Test Loss :0.26621103286743164\n",
      "Epoch :0.06666666666666667    Train Loss :0.052417583763599396    Test Loss :0.20179873704910278\n",
      "Epoch :0.075    Train Loss :0.05237280949950218    Test Loss :0.24576815962791443\n",
      "Epoch :0.08333333333333333    Train Loss :0.05165647715330124    Test Loss :0.24825039505958557\n",
      "Epoch :0.09166666666666666    Train Loss :0.05180070921778679    Test Loss :0.2195274978876114\n",
      "Epoch :0.1    Train Loss :0.0516509972512722    Test Loss :0.2322504073381424\n",
      "Epoch :0.10833333333333334    Train Loss :0.05161378160119057    Test Loss :0.24415531754493713\n",
      "Epoch :0.11666666666666667    Train Loss :0.05170930549502373    Test Loss :0.23437929153442383\n",
      "Epoch :0.125    Train Loss :0.051565077155828476    Test Loss :0.2283667027950287\n",
      "Epoch :0.13333333333333333    Train Loss :0.05157831311225891    Test Loss :0.23330309987068176\n",
      "Epoch :0.14166666666666666    Train Loss :0.05157918110489845    Test Loss :0.23774445056915283\n",
      "Epoch :0.15    Train Loss :0.051581136882305145    Test Loss :0.2363787144422531\n",
      "Epoch :0.15833333333333333    Train Loss :0.05156068503856659    Test Loss :0.2337353527545929\n",
      "Epoch :0.16666666666666666    Train Loss :0.05153898894786835    Test Loss :0.23260781168937683\n",
      "Epoch :0.175    Train Loss :0.051576390862464905    Test Loss :0.23340371251106262\n",
      "Epoch :0.18333333333333332    Train Loss :0.05155228078365326    Test Loss :0.2346280813217163\n",
      "Epoch :0.19166666666666668    Train Loss :0.05158645287156105    Test Loss :0.2352059781551361\n",
      "Epoch :0.2    Train Loss :0.051590513437986374    Test Loss :0.23496820032596588\n",
      "Epoch :0.20833333333333334    Train Loss :0.05157756060361862    Test Loss :0.23486977815628052\n",
      "Epoch :0.21666666666666667    Train Loss :0.05159856751561165    Test Loss :0.2345106154680252\n",
      "Epoch :0.225    Train Loss :0.05157449468970299    Test Loss :0.23421281576156616\n",
      "Epoch :0.23333333333333334    Train Loss :0.05154610052704811    Test Loss :0.2340625673532486\n",
      "Epoch :0.24166666666666667    Train Loss :0.051612164825201035    Test Loss :0.23403796553611755\n",
      "Epoch :0.25    Train Loss :0.05157218500971794    Test Loss :0.23420588672161102\n",
      "Epoch :0.25833333333333336    Train Loss :0.0515722893178463    Test Loss :0.2344193458557129\n",
      "Epoch :0.26666666666666666    Train Loss :0.05157103016972542    Test Loss :0.2344796359539032\n",
      "Epoch :0.275    Train Loss :0.051566366106271744    Test Loss :0.23445796966552734\n",
      "Epoch :0.2833333333333333    Train Loss :0.05155421048402786    Test Loss :0.2343653291463852\n",
      "Epoch :0.2916666666666667    Train Loss :0.051594167947769165    Test Loss :0.23450693488121033\n",
      "Epoch :0.3    Train Loss :0.05156183987855911    Test Loss :0.2344975620508194\n",
      "Epoch :0.30833333333333335    Train Loss :0.05154450610280037    Test Loss :0.23426246643066406\n",
      "Epoch :0.31666666666666665    Train Loss :0.05158047378063202    Test Loss :0.23435984551906586\n",
      "Epoch :0.325    Train Loss :0.05154428631067276    Test Loss :0.23427867889404297\n",
      "Epoch :0.3333333333333333    Train Loss :0.051579415798187256    Test Loss :0.23429250717163086\n",
      "Epoch :0.3416666666666667    Train Loss :0.051568176597356796    Test Loss :0.23418432474136353\n",
      "Epoch :0.35    Train Loss :0.05155716836452484    Test Loss :0.23439253866672516\n",
      "Epoch :0.35833333333333334    Train Loss :0.05156923457980156    Test Loss :0.23447874188423157\n",
      "Epoch :0.36666666666666664    Train Loss :0.05159510672092438    Test Loss :0.2346397191286087\n",
      "Epoch :0.375    Train Loss :0.05159606784582138    Test Loss :0.23485597968101501\n",
      "Epoch :0.38333333333333336    Train Loss :0.05158047005534172    Test Loss :0.2349165678024292\n",
      "Epoch :0.39166666666666666    Train Loss :0.051573432981967926    Test Loss :0.23477312922477722\n",
      "Epoch :0.4    Train Loss :0.05158751457929611    Test Loss :0.23455646634101868\n",
      "Epoch :0.4083333333333333    Train Loss :0.05157477781176567    Test Loss :0.23419463634490967\n",
      "Epoch :0.4166666666666667    Train Loss :0.05159510299563408    Test Loss :0.2343606948852539\n",
      "Epoch :0.425    Train Loss :0.051576390862464905    Test Loss :0.23430493474006653\n",
      "Epoch :0.43333333333333335    Train Loss :0.051564913243055344    Test Loss :0.2342117726802826\n",
      "Epoch :0.44166666666666665    Train Loss :0.05158526822924614    Test Loss :0.23443007469177246\n",
      "Epoch :0.45    Train Loss :0.05158287659287453    Test Loss :0.2345493882894516\n",
      "Epoch :0.4583333333333333    Train Loss :0.051616083830595016    Test Loss :0.23431722819805145\n",
      "Epoch :0.4666666666666667    Train Loss :0.05156401917338371    Test Loss :0.2342306226491928\n",
      "Epoch :0.475    Train Loss :0.05154908075928688    Test Loss :0.23424458503723145\n",
      "Epoch :0.48333333333333334    Train Loss :0.05157247185707092    Test Loss :0.2343454211950302\n",
      "Epoch :0.49166666666666664    Train Loss :0.05156043916940689    Test Loss :0.23450680077075958\n",
      "Epoch :0.5    Train Loss :0.05157889053225517    Test Loss :0.23450227081775665\n",
      "Epoch :0.5083333333333333    Train Loss :0.05156191438436508    Test Loss :0.2344290167093277\n",
      "Epoch :0.5166666666666667    Train Loss :0.0515645332634449    Test Loss :0.23412473499774933\n",
      "Epoch :0.525    Train Loss :0.05159962922334671    Test Loss :0.2344883233308792\n",
      "Epoch :0.5333333333333333    Train Loss :0.05158521980047226    Test Loss :0.23467332124710083\n",
      "Epoch :0.5416666666666666    Train Loss :0.05156266316771507    Test Loss :0.23426544666290283\n",
      "Epoch :0.55    Train Loss :0.051589008420705795    Test Loss :0.23440837860107422\n",
      "Epoch :0.5583333333333333    Train Loss :0.051584161818027496    Test Loss :0.234280064702034\n",
      "Epoch :0.5666666666666667    Train Loss :0.051533885300159454    Test Loss :0.23433995246887207\n",
      "Epoch :0.575    Train Loss :0.05157831683754921    Test Loss :0.23425334692001343\n",
      "Epoch :0.5833333333333334    Train Loss :0.051571864634752274    Test Loss :0.23439210653305054\n",
      "Epoch :0.5916666666666667    Train Loss :0.05156451091170311    Test Loss :0.23449008166790009\n",
      "Epoch :0.6    Train Loss :0.051560934633016586    Test Loss :0.2345045953989029\n",
      "Epoch :0.6083333333333333    Train Loss :0.0515662282705307    Test Loss :0.23430201411247253\n",
      "Epoch :0.6166666666666667    Train Loss :0.051615726202726364    Test Loss :0.23457176983356476\n",
      "Epoch :0.625    Train Loss :0.05158132314682007    Test Loss :0.23495472967624664\n",
      "Epoch :0.6333333333333333    Train Loss :0.05155131220817566    Test Loss :0.23383963108062744\n",
      "Epoch :0.6416666666666667    Train Loss :0.05155593901872635    Test Loss :0.23448622226715088\n",
      "Epoch :0.65    Train Loss :0.05161682143807411    Test Loss :0.23498684167861938\n",
      "Epoch :0.6583333333333333    Train Loss :0.051560379564762115    Test Loss :0.23365724086761475\n",
      "Epoch :0.6666666666666666    Train Loss :0.051568735390901566    Test Loss :0.23407714068889618\n",
      "Epoch :0.675    Train Loss :0.051569659262895584    Test Loss :0.23482747375965118\n",
      "Epoch :0.6833333333333333    Train Loss :0.05157838389277458    Test Loss :0.23427124321460724\n",
      "Epoch :0.6916666666666667    Train Loss :0.05158103629946709    Test Loss :0.2342691421508789\n",
      "Epoch :0.7    Train Loss :0.051567886024713516    Test Loss :0.2349843978881836\n",
      "Epoch :0.7083333333333334    Train Loss :0.051583435386419296    Test Loss :0.23440119624137878\n",
      "Epoch :0.7166666666666667    Train Loss :0.05157563090324402    Test Loss :0.23439118266105652\n",
      "Epoch :0.725    Train Loss :0.051550284028053284    Test Loss :0.23456308245658875\n",
      "Epoch :0.7333333333333333    Train Loss :0.05158068612217903    Test Loss :0.2345578372478485\n",
      "Epoch :0.7416666666666667    Train Loss :0.05159870907664299    Test Loss :0.23431333899497986\n",
      "Epoch :0.75    Train Loss :0.051591675728559494    Test Loss :0.23425647616386414\n",
      "Epoch :0.7583333333333333    Train Loss :0.051569774746894836    Test Loss :0.23453059792518616\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7666666666666667    Train Loss :0.051567453891038895    Test Loss :0.23422271013259888\n",
      "Epoch :0.775    Train Loss :0.051558755338191986    Test Loss :0.23447945713996887\n",
      "Epoch :0.7833333333333333    Train Loss :0.0515875369310379    Test Loss :0.234466090798378\n",
      "Epoch :0.7916666666666666    Train Loss :0.05153749883174896    Test Loss :0.23443366587162018\n",
      "Epoch :0.8    Train Loss :0.05155196413397789    Test Loss :0.23453080654144287\n",
      "Epoch :0.8083333333333333    Train Loss :0.0515766479074955    Test Loss :0.23442430794239044\n",
      "Epoch :0.8166666666666667    Train Loss :0.05160864442586899    Test Loss :0.23438569903373718\n",
      "Epoch :0.825    Train Loss :0.051556769758462906    Test Loss :0.23451203107833862\n",
      "Epoch :0.8333333333333334    Train Loss :0.05154935270547867    Test Loss :0.23416084051132202\n",
      "Epoch :0.8416666666666667    Train Loss :0.051518164575099945    Test Loss :0.2335193008184433\n",
      "Epoch :0.85    Train Loss :0.05157151073217392    Test Loss :0.2345130741596222\n",
      "Epoch :0.8583333333333333    Train Loss :0.051583804190158844    Test Loss :0.2330566793680191\n",
      "Epoch :0.8666666666666667    Train Loss :0.05154484882950783    Test Loss :0.23667286336421967\n",
      "Epoch :0.875    Train Loss :0.051615625619888306    Test Loss :0.2352645993232727\n",
      "Epoch :0.8833333333333333    Train Loss :0.051612503826618195    Test Loss :0.23350760340690613\n",
      "Epoch :0.8916666666666667    Train Loss :0.05155831575393677    Test Loss :0.2354922592639923\n",
      "Epoch :0.9    Train Loss :0.05188645422458649    Test Loss :0.2389407455921173\n",
      "Epoch :0.9083333333333333    Train Loss :0.0515085831284523    Test Loss :0.23210692405700684\n",
      "Epoch :0.9166666666666666    Train Loss :0.05162019655108452    Test Loss :0.23963047564029694\n",
      "Epoch :0.925    Train Loss :0.05159851908683777    Test Loss :0.23036663234233856\n",
      "Epoch :0.9333333333333333    Train Loss :0.05157138779759407    Test Loss :0.23753628134727478\n",
      "Epoch :0.9416666666666667    Train Loss :0.0515846312046051    Test Loss :0.23208889365196228\n",
      "Epoch :0.95    Train Loss :0.05158178508281708    Test Loss :0.23631244897842407\n",
      "Epoch :0.9583333333333334    Train Loss :0.05158168077468872    Test Loss :0.23294122517108917\n",
      "Epoch :0.9666666666666667    Train Loss :0.05158229172229767    Test Loss :0.2355722039937973\n",
      "Epoch :0.975    Train Loss :0.051586877554655075    Test Loss :0.23365114629268646\n",
      "Epoch :0.9833333333333333    Train Loss :0.05156773328781128    Test Loss :0.23488734662532806\n",
      "Epoch :0.9916666666666667    Train Loss :0.05157468467950821    Test Loss :0.23414888978004456\n",
      "Epoch :1.0    Train Loss :0.05157136917114258    Test Loss :0.234469935297966\n",
      "RMSE: 43.82539111126663\n",
      "MAE: 43.2596309068532\n",
      "MAPE: 38.12435054505041%\n",
      "parametros: {'dropout_rate': 0.5, 'epochs': 600, 'hid_size': 100, 'lr': 0.02, 'num_layers': 5}\n",
      "total:  50.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.04596833884716034    Test Loss :0.2715390920639038\n",
      "Epoch :0.025    Train Loss :0.03722686693072319    Test Loss :0.033931978046894073\n",
      "Epoch :0.0375    Train Loss :0.013910830952227116    Test Loss :0.1512037217617035\n",
      "Epoch :0.05    Train Loss :0.014110527001321316    Test Loss :0.026068709790706635\n",
      "Epoch :0.0625    Train Loss :0.012593396939337254    Test Loss :0.03360184282064438\n",
      "Epoch :0.075    Train Loss :0.009497640654444695    Test Loss :0.017614271491765976\n",
      "Epoch :0.0875    Train Loss :0.009942051954567432    Test Loss :0.012290666811168194\n",
      "Epoch :0.1    Train Loss :0.006724316161125898    Test Loss :0.015365997329354286\n",
      "Epoch :0.1125    Train Loss :0.00841807946562767    Test Loss :0.009735941886901855\n",
      "Epoch :0.125    Train Loss :0.008314862847328186    Test Loss :0.010053090751171112\n",
      "Epoch :0.1375    Train Loss :0.0063958135433495045    Test Loss :0.011825726367533207\n",
      "Epoch :0.15    Train Loss :0.006467533763498068    Test Loss :0.011225215159356594\n",
      "Epoch :0.1625    Train Loss :0.0062088496051728725    Test Loss :0.011183570139110088\n",
      "Epoch :0.175    Train Loss :0.007189034018665552    Test Loss :0.009391793981194496\n",
      "Epoch :0.1875    Train Loss :0.007229748647660017    Test Loss :0.010170763358473778\n",
      "Epoch :0.2    Train Loss :0.00671980157494545    Test Loss :0.01090534869581461\n",
      "Epoch :0.2125    Train Loss :0.006039693020284176    Test Loss :0.009895178489387035\n",
      "Epoch :0.225    Train Loss :0.006481728050857782    Test Loss :0.008571897633373737\n",
      "Epoch :0.2375    Train Loss :0.0056238844990730286    Test Loss :0.009779848158359528\n",
      "Epoch :0.25    Train Loss :0.005059494636952877    Test Loss :0.011243757791817188\n",
      "Epoch :0.2625    Train Loss :0.005057676695287228    Test Loss :0.00947608519345522\n",
      "Epoch :0.275    Train Loss :0.00516149029135704    Test Loss :0.011578371748328209\n",
      "Epoch :0.2875    Train Loss :0.005385140422731638    Test Loss :0.013326306827366352\n",
      "Epoch :0.3    Train Loss :0.004569957032799721    Test Loss :0.013474829494953156\n",
      "Epoch :0.3125    Train Loss :0.004965857602655888    Test Loss :0.009938396513462067\n",
      "Epoch :0.325    Train Loss :0.0048756985925138    Test Loss :0.008814271539449692\n",
      "Epoch :0.3375    Train Loss :0.005062231328338385    Test Loss :0.011395116336643696\n",
      "Epoch :0.35    Train Loss :0.00519283814355731    Test Loss :0.009330440312623978\n",
      "Epoch :0.3625    Train Loss :0.005460998043417931    Test Loss :0.011772722005844116\n",
      "Epoch :0.375    Train Loss :0.004384200554341078    Test Loss :0.010654323734343052\n",
      "Epoch :0.3875    Train Loss :0.004199578892439604    Test Loss :0.013003780506551266\n",
      "Epoch :0.4    Train Loss :0.004477331414818764    Test Loss :0.013127236627042294\n",
      "Epoch :0.4125    Train Loss :0.004151520784944296    Test Loss :0.011104399338364601\n",
      "Epoch :0.425    Train Loss :0.004697099793702364    Test Loss :0.013120262883603573\n",
      "Epoch :0.4375    Train Loss :0.004137201234698296    Test Loss :0.011666061356663704\n",
      "Epoch :0.45    Train Loss :0.0041586412116885185    Test Loss :0.019370991736650467\n",
      "Epoch :0.4625    Train Loss :0.0036491763312369585    Test Loss :0.010207968764007092\n",
      "Epoch :0.475    Train Loss :0.00383069459348917    Test Loss :0.011857489123940468\n",
      "Epoch :0.4875    Train Loss :0.003912440501153469    Test Loss :0.012185745872557163\n",
      "Epoch :0.5    Train Loss :0.004105674102902412    Test Loss :0.013411572203040123\n",
      "Epoch :0.5125    Train Loss :0.004395679105073214    Test Loss :0.009667931124567986\n",
      "Epoch :0.525    Train Loss :0.0035480784717947245    Test Loss :0.011094024404883385\n",
      "Epoch :0.5375    Train Loss :0.003998051397502422    Test Loss :0.006969339679926634\n",
      "Epoch :0.55    Train Loss :0.003494824515655637    Test Loss :0.015188832767307758\n",
      "Epoch :0.5625    Train Loss :0.0036461083218455315    Test Loss :0.008036392740905285\n",
      "Epoch :0.575    Train Loss :0.0042318906635046005    Test Loss :0.009948312304913998\n",
      "Epoch :0.5875    Train Loss :0.004047869239002466    Test Loss :0.006473570596426725\n",
      "Epoch :0.6    Train Loss :0.0041427393443882465    Test Loss :0.018189748749136925\n",
      "Epoch :0.6125    Train Loss :0.0036247679963707924    Test Loss :0.010556248016655445\n",
      "Epoch :0.625    Train Loss :0.003496582619845867    Test Loss :0.014170310460031033\n",
      "Epoch :0.6375    Train Loss :0.004142495803534985    Test Loss :0.011502455919981003\n",
      "Epoch :0.65    Train Loss :0.00392628088593483    Test Loss :0.011611628346145153\n",
      "Epoch :0.6625    Train Loss :0.003380466252565384    Test Loss :0.0095216641202569\n",
      "Epoch :0.675    Train Loss :0.0032356714364141226    Test Loss :0.01269878726452589\n",
      "Epoch :0.6875    Train Loss :0.0032964651472866535    Test Loss :0.014834929257631302\n",
      "Epoch :0.7    Train Loss :0.0034451186656951904    Test Loss :0.010222611017525196\n",
      "Epoch :0.7125    Train Loss :0.0035328564699739218    Test Loss :0.01028135884553194\n",
      "Epoch :0.725    Train Loss :0.0032652546651661396    Test Loss :0.010083569213747978\n",
      "Epoch :0.7375    Train Loss :0.003480758750811219    Test Loss :0.013487321324646473\n",
      "Epoch :0.75    Train Loss :0.004071597009897232    Test Loss :0.009065469726920128\n",
      "Epoch :0.7625    Train Loss :0.003549946704879403    Test Loss :0.010329493321478367\n",
      "Epoch :0.775    Train Loss :0.003645998192951083    Test Loss :0.005415379069745541\n",
      "Epoch :0.7875    Train Loss :0.004346749279648066    Test Loss :0.013759602792561054\n",
      "Epoch :0.8    Train Loss :0.0034074215218424797    Test Loss :0.01076408103108406\n",
      "Epoch :0.8125    Train Loss :0.0038293199613690376    Test Loss :0.022682176902890205\n",
      "Epoch :0.825    Train Loss :0.0027105079498142004    Test Loss :0.01435153093189001\n",
      "Epoch :0.8375    Train Loss :0.003610270796343684    Test Loss :0.011422039940953255\n",
      "Epoch :0.85    Train Loss :0.004010201897472143    Test Loss :0.007023390382528305\n",
      "Epoch :0.8625    Train Loss :0.0029364777728915215    Test Loss :0.010228465311229229\n",
      "Epoch :0.875    Train Loss :0.0036265605594962835    Test Loss :0.008025333285331726\n",
      "Epoch :0.8875    Train Loss :0.004374093376100063    Test Loss :0.010477576404809952\n",
      "Epoch :0.9    Train Loss :0.003228499786928296    Test Loss :0.011995771899819374\n",
      "Epoch :0.9125    Train Loss :0.00404170760884881    Test Loss :0.01469156239181757\n",
      "Epoch :0.925    Train Loss :0.0027951321098953485    Test Loss :0.009742912836372852\n",
      "Epoch :0.9375    Train Loss :0.003145280759781599    Test Loss :0.012552394531667233\n",
      "Epoch :0.95    Train Loss :0.0037954426370561123    Test Loss :0.010059271939098835\n",
      "Epoch :0.9625    Train Loss :0.0030146208591759205    Test Loss :0.007806498557329178\n",
      "Epoch :0.975    Train Loss :0.0027836195658892393    Test Loss :0.007617655210196972\n",
      "Epoch :0.9875    Train Loss :0.003168809227645397    Test Loss :0.01209584716707468\n",
      "Epoch :1.0    Train Loss :0.0032996225636452436    Test Loss :0.01839321292936802\n",
      "RMSE: 15.194372579965231\n",
      "MAE: 12.974823198803236\n",
      "MAPE: 11.185297279009868%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 400, 'hid_size': 50, 'lr': 0.01, 'num_layers': 2}\n",
      "total:  53.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.06358971446752548    Test Loss :0.38345634937286377\n",
      "Epoch :0.025    Train Loss :0.05755195766687393    Test Loss :0.24224527180194855\n",
      "Epoch :0.0375    Train Loss :0.053000859916210175    Test Loss :0.2503663897514343\n",
      "Epoch :0.05    Train Loss :0.051840342581272125    Test Loss :0.24071942269802094\n",
      "Epoch :0.0625    Train Loss :0.052171606570482254    Test Loss :0.2139991670846939\n",
      "Epoch :0.075    Train Loss :0.046028878539800644    Test Loss :0.2094409316778183\n",
      "Epoch :0.0875    Train Loss :0.025206217542290688    Test Loss :0.036463815718889236\n",
      "Epoch :0.1    Train Loss :0.02034662663936615    Test Loss :0.09024554491043091\n",
      "Epoch :0.1125    Train Loss :0.022624323144555092    Test Loss :0.09778524935245514\n",
      "Epoch :0.125    Train Loss :0.019764019176363945    Test Loss :0.020186450332403183\n",
      "Epoch :0.1375    Train Loss :0.01408187672495842    Test Loss :0.021959878504276276\n",
      "Epoch :0.15    Train Loss :0.015400268137454987    Test Loss :0.06701895594596863\n",
      "Epoch :0.1625    Train Loss :0.013812822289764881    Test Loss :0.03291482478380203\n",
      "Epoch :0.175    Train Loss :0.013848203234374523    Test Loss :0.016021545976400375\n",
      "Epoch :0.1875    Train Loss :0.010911282151937485    Test Loss :0.016279904171824455\n",
      "Epoch :0.2    Train Loss :0.01008380576968193    Test Loss :0.026599684730172157\n",
      "Epoch :0.2125    Train Loss :0.01057718601077795    Test Loss :0.024730924516916275\n",
      "Epoch :0.225    Train Loss :0.01113978773355484    Test Loss :0.015512075275182724\n",
      "Epoch :0.2375    Train Loss :0.009820892475545406    Test Loss :0.01867489516735077\n",
      "Epoch :0.25    Train Loss :0.008105804212391376    Test Loss :0.020481694489717484\n",
      "Epoch :0.2625    Train Loss :0.008910780772566795    Test Loss :0.01644790545105934\n",
      "Epoch :0.275    Train Loss :0.007624081801623106    Test Loss :0.0155155248939991\n",
      "Epoch :0.2875    Train Loss :0.008512402884662151    Test Loss :0.016925016418099403\n",
      "Epoch :0.3    Train Loss :0.009104212746024132    Test Loss :0.02315102517604828\n",
      "Epoch :0.3125    Train Loss :0.008219661191105843    Test Loss :0.019487962126731873\n",
      "Epoch :0.325    Train Loss :0.007797591853886843    Test Loss :0.016141513362526894\n",
      "Epoch :0.3375    Train Loss :0.008145814761519432    Test Loss :0.016848012804985046\n",
      "Epoch :0.35    Train Loss :0.007428032346069813    Test Loss :0.01700037531554699\n",
      "Epoch :0.3625    Train Loss :0.008336802013218403    Test Loss :0.016402408480644226\n",
      "Epoch :0.375    Train Loss :0.007072367239743471    Test Loss :0.0170554481446743\n",
      "Epoch :0.3875    Train Loss :0.0073208557441830635    Test Loss :0.02411886304616928\n",
      "Epoch :0.4    Train Loss :0.007831602357327938    Test Loss :0.02117581106722355\n",
      "Epoch :0.4125    Train Loss :0.007026277016848326    Test Loss :0.019137680530548096\n",
      "Epoch :0.425    Train Loss :0.007363305892795324    Test Loss :0.016245171427726746\n",
      "Epoch :0.4375    Train Loss :0.00698821758851409    Test Loss :0.018022669479250908\n",
      "Epoch :0.45    Train Loss :0.007334034889936447    Test Loss :0.0197304654866457\n",
      "Epoch :0.4625    Train Loss :0.007328173611313105    Test Loss :0.01863052137196064\n",
      "Epoch :0.475    Train Loss :0.0069676777347922325    Test Loss :0.019328463822603226\n",
      "Epoch :0.4875    Train Loss :0.0070271301083266735    Test Loss :0.02074385993182659\n",
      "Epoch :0.5    Train Loss :0.006690294947475195    Test Loss :0.01723266951739788\n",
      "Epoch :0.5125    Train Loss :0.0066575403325259686    Test Loss :0.018139932304620743\n",
      "Epoch :0.525    Train Loss :0.00722393486648798    Test Loss :0.020127592608332634\n",
      "Epoch :0.5375    Train Loss :0.006738817784935236    Test Loss :0.01834980398416519\n",
      "Epoch :0.55    Train Loss :0.005516257602721453    Test Loss :0.01666899025440216\n",
      "Epoch :0.5625    Train Loss :0.0066549633629620075    Test Loss :0.018987899646162987\n",
      "Epoch :0.575    Train Loss :0.0056349290534853935    Test Loss :0.0148352375254035\n",
      "Epoch :0.5875    Train Loss :0.006312130950391293    Test Loss :0.01977035216987133\n",
      "Epoch :0.6    Train Loss :0.006120964419096708    Test Loss :0.018942801281809807\n",
      "Epoch :0.6125    Train Loss :0.006114772520959377    Test Loss :0.019438644871115685\n",
      "Epoch :0.625    Train Loss :0.006458011455833912    Test Loss :0.01847803220152855\n",
      "Epoch :0.6375    Train Loss :0.00626409612596035    Test Loss :0.01988033950328827\n",
      "Epoch :0.65    Train Loss :0.005887580569833517    Test Loss :0.02003587782382965\n",
      "Epoch :0.6625    Train Loss :0.005718218628317118    Test Loss :0.01928192563354969\n",
      "Epoch :0.675    Train Loss :0.0064163352362811565    Test Loss :0.01646357960999012\n",
      "Epoch :0.6875    Train Loss :0.006472693756222725    Test Loss :0.020048590376973152\n",
      "Epoch :0.7    Train Loss :0.005787508096545935    Test Loss :0.01807398721575737\n",
      "Epoch :0.7125    Train Loss :0.006089040543884039    Test Loss :0.016716565936803818\n",
      "Epoch :0.725    Train Loss :0.006001790054142475    Test Loss :0.01716752164065838\n",
      "Epoch :0.7375    Train Loss :0.006039020139724016    Test Loss :0.020634813234210014\n",
      "Epoch :0.75    Train Loss :0.005538599099963903    Test Loss :0.014816184528172016\n",
      "Epoch :0.7625    Train Loss :0.006731819827109575    Test Loss :0.016451796516776085\n",
      "Epoch :0.775    Train Loss :0.005737262312322855    Test Loss :0.020676007494330406\n",
      "Epoch :0.7875    Train Loss :0.00560069689527154    Test Loss :0.012742672115564346\n",
      "Epoch :0.8    Train Loss :0.005125244613736868    Test Loss :0.015891240909695625\n",
      "Epoch :0.8125    Train Loss :0.005727601237595081    Test Loss :0.019366707652807236\n",
      "Epoch :0.825    Train Loss :0.006227638106793165    Test Loss :0.0156836099922657\n",
      "Epoch :0.8375    Train Loss :0.005325597245246172    Test Loss :0.016994357109069824\n",
      "Epoch :0.85    Train Loss :0.005356436595320702    Test Loss :0.017975129187107086\n",
      "Epoch :0.8625    Train Loss :0.006579962093383074    Test Loss :0.015015416778624058\n",
      "Epoch :0.875    Train Loss :0.005584288854151964    Test Loss :0.019289743155241013\n",
      "Epoch :0.8875    Train Loss :0.006166033446788788    Test Loss :0.019091440364718437\n",
      "Epoch :0.9    Train Loss :0.005285436287522316    Test Loss :0.01748582161962986\n",
      "Epoch :0.9125    Train Loss :0.005558765958994627    Test Loss :0.01705048233270645\n",
      "Epoch :0.925    Train Loss :0.005963692907243967    Test Loss :0.017250237986445427\n",
      "Epoch :0.9375    Train Loss :0.004948911257088184    Test Loss :0.017932379618287086\n",
      "Epoch :0.95    Train Loss :0.005533158313483    Test Loss :0.01817430555820465\n",
      "Epoch :0.9625    Train Loss :0.0056364573538303375    Test Loss :0.02258272096514702\n",
      "Epoch :0.975    Train Loss :0.005102998577058315    Test Loss :0.0194141436368227\n",
      "Epoch :0.9875    Train Loss :0.005664444994181395    Test Loss :0.023397933691740036\n",
      "Epoch :1.0    Train Loss :0.005014698952436447    Test Loss :0.016072265803813934\n",
      "RMSE: 11.235281043612083\n",
      "MAE: 9.125356751767228\n",
      "MAPE: 7.797809760886976%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 400, 'hid_size': 50, 'lr': 0.01, 'num_layers': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  56.00000000000001\n",
      "Epoch :0.0125    Train Loss :0.0844474658370018    Test Loss :0.4138977825641632\n",
      "Epoch :0.025    Train Loss :0.05704410374164581    Test Loss :0.1445961445569992\n",
      "Epoch :0.0375    Train Loss :0.04885343462228775    Test Loss :0.25352686643600464\n",
      "Epoch :0.05    Train Loss :0.03604007512331009    Test Loss :0.03903791308403015\n",
      "Epoch :0.0625    Train Loss :0.02397049404680729    Test Loss :0.10437584668397903\n",
      "Epoch :0.075    Train Loss :0.012600919231772423    Test Loss :0.026708198711276054\n",
      "Epoch :0.0875    Train Loss :0.011462178081274033    Test Loss :0.01861596293747425\n",
      "Epoch :0.1    Train Loss :0.00806598924100399    Test Loss :0.02359115704894066\n",
      "Epoch :0.1125    Train Loss :0.007723427843302488    Test Loss :0.015088289976119995\n",
      "Epoch :0.125    Train Loss :0.0076416125521063805    Test Loss :0.018007878214120865\n",
      "Epoch :0.1375    Train Loss :0.008070082403719425    Test Loss :0.01778307743370533\n",
      "Epoch :0.15    Train Loss :0.006308697164058685    Test Loss :0.01828771084547043\n",
      "Epoch :0.1625    Train Loss :0.006332563702017069    Test Loss :0.017446380108594894\n",
      "Epoch :0.175    Train Loss :0.005977993831038475    Test Loss :0.01258785929530859\n",
      "Epoch :0.1875    Train Loss :0.007075225468724966    Test Loss :0.014576497487723827\n",
      "Epoch :0.2    Train Loss :0.006112441886216402    Test Loss :0.012270945124328136\n",
      "Epoch :0.2125    Train Loss :0.00584548432379961    Test Loss :0.011441666632890701\n",
      "Epoch :0.225    Train Loss :0.005547764245420694    Test Loss :0.013352279551327229\n",
      "Epoch :0.2375    Train Loss :0.0050968388095498085    Test Loss :0.012218523770570755\n",
      "Epoch :0.25    Train Loss :0.005436925683170557    Test Loss :0.011709755286574364\n",
      "Epoch :0.2625    Train Loss :0.005349649116396904    Test Loss :0.007763157598674297\n",
      "Epoch :0.275    Train Loss :0.004572254605591297    Test Loss :0.008983897045254707\n",
      "Epoch :0.2875    Train Loss :0.005282612517476082    Test Loss :0.008220900781452656\n",
      "Epoch :0.3    Train Loss :0.005307844839990139    Test Loss :0.011890492402017117\n",
      "Epoch :0.3125    Train Loss :0.005061174277216196    Test Loss :0.01029256172478199\n",
      "Epoch :0.325    Train Loss :0.0050778863951563835    Test Loss :0.008698880672454834\n",
      "Epoch :0.3375    Train Loss :0.004626241512596607    Test Loss :0.009311012923717499\n",
      "Epoch :0.35    Train Loss :0.004954816773533821    Test Loss :0.01632421463727951\n",
      "Epoch :0.3625    Train Loss :0.004008966498076916    Test Loss :0.011493024416267872\n",
      "Epoch :0.375    Train Loss :0.004296671599149704    Test Loss :0.010355021804571152\n",
      "Epoch :0.3875    Train Loss :0.004187890328466892    Test Loss :0.011548723094165325\n",
      "Epoch :0.4    Train Loss :0.003639482893049717    Test Loss :0.013383012264966965\n",
      "Epoch :0.4125    Train Loss :0.005168986972421408    Test Loss :0.008631428703665733\n",
      "Epoch :0.425    Train Loss :0.0036215318832546473    Test Loss :0.013358665630221367\n",
      "Epoch :0.4375    Train Loss :0.004244068171828985    Test Loss :0.010539435781538486\n",
      "Epoch :0.45    Train Loss :0.0034238642547279596    Test Loss :0.008576717227697372\n",
      "Epoch :0.4625    Train Loss :0.004099296405911446    Test Loss :0.012040448375046253\n",
      "Epoch :0.475    Train Loss :0.003824058221653104    Test Loss :0.007507185451686382\n",
      "Epoch :0.4875    Train Loss :0.004538087639957666    Test Loss :0.015465058386325836\n",
      "Epoch :0.5    Train Loss :0.00377450417727232    Test Loss :0.008850500918924809\n",
      "Epoch :0.5125    Train Loss :0.004161424934864044    Test Loss :0.006750280037522316\n",
      "Epoch :0.525    Train Loss :0.004749170504510403    Test Loss :0.010012831538915634\n",
      "Epoch :0.5375    Train Loss :0.0037762990687042475    Test Loss :0.010059396736323833\n",
      "Epoch :0.55    Train Loss :0.004292189609259367    Test Loss :0.011377680115401745\n",
      "Epoch :0.5625    Train Loss :0.0034063123166561127    Test Loss :0.013878359459340572\n",
      "Epoch :0.575    Train Loss :0.003737295512109995    Test Loss :0.011350075714290142\n",
      "Epoch :0.5875    Train Loss :0.0029669939540326595    Test Loss :0.016090799123048782\n",
      "Epoch :0.6    Train Loss :0.004363030195236206    Test Loss :0.010217100381851196\n",
      "Epoch :0.6125    Train Loss :0.0035110951866954565    Test Loss :0.012750430032610893\n",
      "Epoch :0.625    Train Loss :0.004313935991376638    Test Loss :0.005311327986419201\n",
      "Epoch :0.6375    Train Loss :0.0032775886356830597    Test Loss :0.009837796911597252\n",
      "Epoch :0.65    Train Loss :0.004037267994135618    Test Loss :0.00660435575991869\n",
      "Epoch :0.6625    Train Loss :0.003316201502457261    Test Loss :0.007567036431282759\n",
      "Epoch :0.675    Train Loss :0.003766103880479932    Test Loss :0.007912245579063892\n",
      "Epoch :0.6875    Train Loss :0.004256139975041151    Test Loss :0.006062786560505629\n",
      "Epoch :0.7    Train Loss :0.0031960385385900736    Test Loss :0.010768373496830463\n",
      "Epoch :0.7125    Train Loss :0.004811141174286604    Test Loss :0.007438563276082277\n",
      "Epoch :0.725    Train Loss :0.004429956898093224    Test Loss :0.034445613622665405\n",
      "Epoch :0.7375    Train Loss :0.004011859651654959    Test Loss :0.009343392215669155\n",
      "Epoch :0.75    Train Loss :0.0036580325104296207    Test Loss :0.009827252477407455\n",
      "Epoch :0.7625    Train Loss :0.004772390238940716    Test Loss :0.007083032745867968\n",
      "Epoch :0.775    Train Loss :0.003120933659374714    Test Loss :0.007277106866240501\n",
      "Epoch :0.7875    Train Loss :0.003740090411156416    Test Loss :0.008073675446212292\n",
      "Epoch :0.8    Train Loss :0.0035611519124358892    Test Loss :0.011746355332434177\n",
      "Epoch :0.8125    Train Loss :0.003189664799720049    Test Loss :0.006634462159126997\n",
      "Epoch :0.825    Train Loss :0.003590659238398075    Test Loss :0.007927177473902702\n",
      "Epoch :0.8375    Train Loss :0.003697972046211362    Test Loss :0.006714935414493084\n",
      "Epoch :0.85    Train Loss :0.0030473233200609684    Test Loss :0.012415233999490738\n",
      "Epoch :0.8625    Train Loss :0.003164827125146985    Test Loss :0.009405619464814663\n",
      "Epoch :0.875    Train Loss :0.0033295643515884876    Test Loss :0.0253749992698431\n",
      "Epoch :0.8875    Train Loss :0.0031919588800519705    Test Loss :0.007092720828950405\n",
      "Epoch :0.9    Train Loss :0.0028764689341187477    Test Loss :0.008909613825380802\n",
      "Epoch :0.9125    Train Loss :0.0036555917467921972    Test Loss :0.00858487281948328\n",
      "Epoch :0.925    Train Loss :0.0033487870823591948    Test Loss :0.006151333451271057\n",
      "Epoch :0.9375    Train Loss :0.0030776087660342455    Test Loss :0.005215504206717014\n",
      "Epoch :0.95    Train Loss :0.0031179876532405615    Test Loss :0.015167446807026863\n",
      "Epoch :0.9625    Train Loss :0.0035104912240058184    Test Loss :0.012715931981801987\n",
      "Epoch :0.975    Train Loss :0.002261025132611394    Test Loss :0.012615595012903214\n",
      "Epoch :0.9875    Train Loss :0.0025761197321116924    Test Loss :0.00975818932056427\n",
      "Epoch :1.0    Train Loss :0.0029412894509732723    Test Loss :0.005426917225122452\n",
      "RMSE: 12.597857541829931\n",
      "MAE: 10.41107239322944\n",
      "MAPE: 9.018351928344956%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 400, 'hid_size': 50, 'lr': 0.02, 'num_layers': 2}\n",
      "total:  59.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.056539442390203476    Test Loss :0.1301148235797882\n",
      "Epoch :0.025    Train Loss :0.0555892139673233    Test Loss :0.2653104364871979\n",
      "Epoch :0.0375    Train Loss :0.05363824591040611    Test Loss :0.2244935929775238\n",
      "Epoch :0.05    Train Loss :0.05288096144795418    Test Loss :0.26058077812194824\n",
      "Epoch :0.0625    Train Loss :0.050956953316926956    Test Loss :0.2151215523481369\n",
      "Epoch :0.075    Train Loss :0.05268586054444313    Test Loss :0.24719130992889404\n",
      "Epoch :0.0875    Train Loss :0.051844771951436996    Test Loss :0.2347586303949356\n",
      "Epoch :0.1    Train Loss :0.05191913619637489    Test Loss :0.22948789596557617\n",
      "Epoch :0.1125    Train Loss :0.05081196501851082    Test Loss :0.239054337143898\n",
      "Epoch :0.125    Train Loss :0.051595769822597504    Test Loss :0.22675882279872894\n",
      "Epoch :0.1375    Train Loss :0.05252081900835037    Test Loss :0.23626747727394104\n",
      "Epoch :0.15    Train Loss :0.05130468308925629    Test Loss :0.23951749503612518\n",
      "Epoch :0.1625    Train Loss :0.05205603688955307    Test Loss :0.23118729889392853\n",
      "Epoch :0.175    Train Loss :0.05270441249012947    Test Loss :0.2367277890443802\n",
      "Epoch :0.1875    Train Loss :0.051793839782476425    Test Loss :0.2377438247203827\n",
      "Epoch :0.2    Train Loss :0.052307263016700745    Test Loss :0.23138844966888428\n",
      "Epoch :0.2125    Train Loss :0.05163310840725899    Test Loss :0.23229756951332092\n",
      "Epoch :0.225    Train Loss :0.05241050571203232    Test Loss :0.230980783700943\n",
      "Epoch :0.2375    Train Loss :0.05271375924348831    Test Loss :0.23456698656082153\n",
      "Epoch :0.25    Train Loss :0.05156116560101509    Test Loss :0.23302358388900757\n",
      "Epoch :0.2625    Train Loss :0.05132976546883583    Test Loss :0.23845180869102478\n",
      "Epoch :0.275    Train Loss :0.05105256289243698    Test Loss :0.23209455609321594\n",
      "Epoch :0.2875    Train Loss :0.051121387630701065    Test Loss :0.23432400822639465\n",
      "Epoch :0.3    Train Loss :0.05170110985636711    Test Loss :0.23925700783729553\n",
      "Epoch :0.3125    Train Loss :0.05183477699756622    Test Loss :0.2320941686630249\n",
      "Epoch :0.325    Train Loss :0.051880743354558945    Test Loss :0.23382346332073212\n",
      "Epoch :0.3375    Train Loss :0.05086418241262436    Test Loss :0.24504773318767548\n",
      "Epoch :0.35    Train Loss :0.05157512426376343    Test Loss :0.23369891941547394\n",
      "Epoch :0.3625    Train Loss :0.052383795380592346    Test Loss :0.2478639781475067\n",
      "Epoch :0.375    Train Loss :0.051337480545043945    Test Loss :0.23014333844184875\n",
      "Epoch :0.3875    Train Loss :0.0518171526491642    Test Loss :0.2366720288991928\n",
      "Epoch :0.4    Train Loss :0.05197461321949959    Test Loss :0.23102815449237823\n",
      "Epoch :0.4125    Train Loss :0.05159040167927742    Test Loss :0.23689581453800201\n",
      "Epoch :0.425    Train Loss :0.05134172365069389    Test Loss :0.23404359817504883\n",
      "Epoch :0.4375    Train Loss :0.05273226276040077    Test Loss :0.23687446117401123\n",
      "Epoch :0.45    Train Loss :0.05161694064736366    Test Loss :0.23298658430576324\n",
      "Epoch :0.4625    Train Loss :0.051660116761922836    Test Loss :0.23580217361450195\n",
      "Epoch :0.475    Train Loss :0.05183907970786095    Test Loss :0.23169516026973724\n",
      "Epoch :0.4875    Train Loss :0.05112505331635475    Test Loss :0.2378346025943756\n",
      "Epoch :0.5    Train Loss :0.05224666744470596    Test Loss :0.23359552025794983\n",
      "Epoch :0.5125    Train Loss :0.05226331949234009    Test Loss :0.2363014966249466\n",
      "Epoch :0.525    Train Loss :0.052835509181022644    Test Loss :0.23228251934051514\n",
      "Epoch :0.5375    Train Loss :0.05186448618769646    Test Loss :0.23255084455013275\n",
      "Epoch :0.55    Train Loss :0.05245593935251236    Test Loss :0.23285061120986938\n",
      "Epoch :0.5625    Train Loss :0.05278843268752098    Test Loss :0.236126109957695\n",
      "Epoch :0.575    Train Loss :0.051661550998687744    Test Loss :0.22923630475997925\n",
      "Epoch :0.5875    Train Loss :0.05167829245328903    Test Loss :0.23788005113601685\n",
      "Epoch :0.6    Train Loss :0.051120541989803314    Test Loss :0.23646825551986694\n",
      "Epoch :0.6125    Train Loss :0.05168366804718971    Test Loss :0.24078170955181122\n",
      "Epoch :0.625    Train Loss :0.051643844693899155    Test Loss :0.23341701924800873\n",
      "Epoch :0.6375    Train Loss :0.05135520175099373    Test Loss :0.23608706891536713\n",
      "Epoch :0.65    Train Loss :0.05218048393726349    Test Loss :0.2404625117778778\n",
      "Epoch :0.6625    Train Loss :0.05204692482948303    Test Loss :0.23186194896697998\n",
      "Epoch :0.675    Train Loss :0.05091537535190582    Test Loss :0.23570460081100464\n",
      "Epoch :0.6875    Train Loss :0.051749132573604584    Test Loss :0.24158817529678345\n",
      "Epoch :0.7    Train Loss :0.05181095376610756    Test Loss :0.23579156398773193\n",
      "Epoch :0.7125    Train Loss :0.0519057922065258    Test Loss :0.23992471396923065\n",
      "Epoch :0.725    Train Loss :0.05205638334155083    Test Loss :0.2338665872812271\n",
      "Epoch :0.7375    Train Loss :0.05173869803547859    Test Loss :0.23653805255889893\n",
      "Epoch :0.75    Train Loss :0.05219747871160507    Test Loss :0.229750856757164\n",
      "Epoch :0.7625    Train Loss :0.050844818353652954    Test Loss :0.2372916340827942\n",
      "Epoch :0.775    Train Loss :0.05133901908993721    Test Loss :0.2327900230884552\n",
      "Epoch :0.7875    Train Loss :0.05203748121857643    Test Loss :0.24360060691833496\n",
      "Epoch :0.8    Train Loss :0.052036602050065994    Test Loss :0.2299262434244156\n",
      "Epoch :0.8125    Train Loss :0.05182452127337456    Test Loss :0.2349950671195984\n",
      "Epoch :0.825    Train Loss :0.051362019032239914    Test Loss :0.2359304279088974\n",
      "Epoch :0.8375    Train Loss :0.05252736434340477    Test Loss :0.2415262758731842\n",
      "Epoch :0.85    Train Loss :0.051243532449007034    Test Loss :0.2296348214149475\n",
      "Epoch :0.8625    Train Loss :0.051485445350408554    Test Loss :0.2396043986082077\n",
      "Epoch :0.875    Train Loss :0.05170685052871704    Test Loss :0.2295113503932953\n",
      "Epoch :0.8875    Train Loss :0.051340486854314804    Test Loss :0.23731334507465363\n",
      "Epoch :0.9    Train Loss :0.05173955857753754    Test Loss :0.23471689224243164\n",
      "Epoch :0.9125    Train Loss :0.051581792533397675    Test Loss :0.23295658826828003\n",
      "Epoch :0.925    Train Loss :0.05123719945549965    Test Loss :0.22874662280082703\n",
      "Epoch :0.9375    Train Loss :0.05134674534201622    Test Loss :0.23589448630809784\n",
      "Epoch :0.95    Train Loss :0.05217750370502472    Test Loss :0.23853176832199097\n",
      "Epoch :0.9625    Train Loss :0.05147900432348251    Test Loss :0.23248735070228577\n",
      "Epoch :0.975    Train Loss :0.05144599825143814    Test Loss :0.23756442964076996\n",
      "Epoch :0.9875    Train Loss :0.05131389945745468    Test Loss :0.22838762402534485\n",
      "Epoch :1.0    Train Loss :0.05129362270236015    Test Loss :0.23890548944473267\n",
      "RMSE: 44.418946168322606\n",
      "MAE: 43.85337801431375\n",
      "MAPE: 38.65545458642908%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 400, 'hid_size': 50, 'lr': 0.02, 'num_layers': 5}\n",
      "total: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 62.0\n",
      "Epoch :0.0125    Train Loss :0.03282990679144859    Test Loss :0.1082659661769867\n",
      "Epoch :0.025    Train Loss :0.037055693566799164    Test Loss :0.09475763142108917\n",
      "Epoch :0.0375    Train Loss :0.0233019832521677    Test Loss :0.07840431481599808\n",
      "Epoch :0.05    Train Loss :0.016063036397099495    Test Loss :0.07279592752456665\n",
      "Epoch :0.0625    Train Loss :0.008944442495703697    Test Loss :0.012718566693365574\n",
      "Epoch :0.075    Train Loss :0.00954193715006113    Test Loss :0.01777162216603756\n",
      "Epoch :0.0875    Train Loss :0.009651249274611473    Test Loss :0.011620770208537579\n",
      "Epoch :0.1    Train Loss :0.008192497305572033    Test Loss :0.013590948656201363\n",
      "Epoch :0.1125    Train Loss :0.007131561171263456    Test Loss :0.00908348523080349\n",
      "Epoch :0.125    Train Loss :0.007494057063013315    Test Loss :0.011941242031753063\n",
      "Epoch :0.1375    Train Loss :0.0068530370481312275    Test Loss :0.00797360111027956\n",
      "Epoch :0.15    Train Loss :0.005725155584514141    Test Loss :0.006904159672558308\n",
      "Epoch :0.1625    Train Loss :0.00598077941685915    Test Loss :0.008500775322318077\n",
      "Epoch :0.175    Train Loss :0.005605645943433046    Test Loss :0.007222349755465984\n",
      "Epoch :0.1875    Train Loss :0.005853463429957628    Test Loss :0.008793413639068604\n",
      "Epoch :0.2    Train Loss :0.0050558969378471375    Test Loss :0.006566740572452545\n",
      "Epoch :0.2125    Train Loss :0.004611867945641279    Test Loss :0.007272771093994379\n",
      "Epoch :0.225    Train Loss :0.005389996338635683    Test Loss :0.007933580316603184\n",
      "Epoch :0.2375    Train Loss :0.005439841188490391    Test Loss :0.0065045529045164585\n",
      "Epoch :0.25    Train Loss :0.004053453449159861    Test Loss :0.009519481100142002\n",
      "Epoch :0.2625    Train Loss :0.00531757902354002    Test Loss :0.011427061632275581\n",
      "Epoch :0.275    Train Loss :0.00415894715115428    Test Loss :0.009570308960974216\n",
      "Epoch :0.2875    Train Loss :0.0045545343309640884    Test Loss :0.007047939579933882\n",
      "Epoch :0.3    Train Loss :0.005006818566471338    Test Loss :0.01084006018936634\n",
      "Epoch :0.3125    Train Loss :0.003880956443026662    Test Loss :0.009552014991641045\n",
      "Epoch :0.325    Train Loss :0.004589371383190155    Test Loss :0.006919422652572393\n",
      "Epoch :0.3375    Train Loss :0.004123832564800978    Test Loss :0.009961049072444439\n",
      "Epoch :0.35    Train Loss :0.003786316141486168    Test Loss :0.006550561636686325\n",
      "Epoch :0.3625    Train Loss :0.004473838489502668    Test Loss :0.008341081440448761\n",
      "Epoch :0.375    Train Loss :0.0035811166744679213    Test Loss :0.005024769809097052\n",
      "Epoch :0.3875    Train Loss :0.0037243550177663565    Test Loss :0.008278695866465569\n",
      "Epoch :0.4    Train Loss :0.004694614093750715    Test Loss :0.007536408491432667\n",
      "Epoch :0.4125    Train Loss :0.003690409706905484    Test Loss :0.006841724738478661\n",
      "Epoch :0.425    Train Loss :0.003978480584919453    Test Loss :0.006241354625672102\n",
      "Epoch :0.4375    Train Loss :0.003983460832387209    Test Loss :0.009801496751606464\n",
      "Epoch :0.45    Train Loss :0.0032117092050611973    Test Loss :0.009894597344100475\n",
      "Epoch :0.4625    Train Loss :0.0032483176328241825    Test Loss :0.011292017996311188\n",
      "Epoch :0.475    Train Loss :0.003493326483294368    Test Loss :0.008779613301157951\n",
      "Epoch :0.4875    Train Loss :0.003495753975585103    Test Loss :0.008400285616517067\n",
      "Epoch :0.5    Train Loss :0.00366540951654315    Test Loss :0.006489759776741266\n",
      "Epoch :0.5125    Train Loss :0.0029027676209807396    Test Loss :0.006120331585407257\n",
      "Epoch :0.525    Train Loss :0.0034873865079134703    Test Loss :0.0075033241882920265\n",
      "Epoch :0.5375    Train Loss :0.0034141915384680033    Test Loss :0.009666860103607178\n",
      "Epoch :0.55    Train Loss :0.003038043389096856    Test Loss :0.013211618177592754\n",
      "Epoch :0.5625    Train Loss :0.00281289080157876    Test Loss :0.01314451638609171\n",
      "Epoch :0.575    Train Loss :0.002830357290804386    Test Loss :0.009805561043322086\n",
      "Epoch :0.5875    Train Loss :0.0025953699368983507    Test Loss :0.007273366674780846\n",
      "Epoch :0.6    Train Loss :0.0032620993442833424    Test Loss :0.007109647151082754\n",
      "Epoch :0.6125    Train Loss :0.003498229430988431    Test Loss :0.007407899014651775\n",
      "Epoch :0.625    Train Loss :0.0033004421275109053    Test Loss :0.005636062938719988\n",
      "Epoch :0.6375    Train Loss :0.0034164634998887777    Test Loss :0.0057042865082621574\n",
      "Epoch :0.65    Train Loss :0.0031756905373185873    Test Loss :0.010429074987769127\n",
      "Epoch :0.6625    Train Loss :0.0032045533880591393    Test Loss :0.006236891262233257\n",
      "Epoch :0.675    Train Loss :0.0034636666532605886    Test Loss :0.010878489352762699\n",
      "Epoch :0.6875    Train Loss :0.003403429174795747    Test Loss :0.007130242418497801\n",
      "Epoch :0.7    Train Loss :0.0034224882256239653    Test Loss :0.009462266229093075\n",
      "Epoch :0.7125    Train Loss :0.0030982240568846464    Test Loss :0.01187459472566843\n",
      "Epoch :0.725    Train Loss :0.003091601189225912    Test Loss :0.008325988426804543\n",
      "Epoch :0.7375    Train Loss :0.0037906987126916647    Test Loss :0.00985343474894762\n",
      "Epoch :0.75    Train Loss :0.003023410914465785    Test Loss :0.00641209352761507\n",
      "Epoch :0.7625    Train Loss :0.002842389279976487    Test Loss :0.006800290197134018\n",
      "Epoch :0.775    Train Loss :0.0033056200481951237    Test Loss :0.006584406830370426\n",
      "Epoch :0.7875    Train Loss :0.0033213209826499224    Test Loss :0.006149234715849161\n",
      "Epoch :0.8    Train Loss :0.003116364125162363    Test Loss :0.009388848207890987\n",
      "Epoch :0.8125    Train Loss :0.003835417563095689    Test Loss :0.011741822585463524\n",
      "Epoch :0.825    Train Loss :0.0032312837429344654    Test Loss :0.00610380107536912\n",
      "Epoch :0.8375    Train Loss :0.0025912984274327755    Test Loss :0.010441754944622517\n",
      "Epoch :0.85    Train Loss :0.003026599297299981    Test Loss :0.004790285602211952\n",
      "Epoch :0.8625    Train Loss :0.0035733385011553764    Test Loss :0.007877402938902378\n",
      "Epoch :0.875    Train Loss :0.002647549845278263    Test Loss :0.008385011926293373\n",
      "Epoch :0.8875    Train Loss :0.003181211184710264    Test Loss :0.007061878219246864\n",
      "Epoch :0.9    Train Loss :0.00275779003277421    Test Loss :0.007031518034636974\n",
      "Epoch :0.9125    Train Loss :0.0031683037523180246    Test Loss :0.00663547869771719\n",
      "Epoch :0.925    Train Loss :0.0029988109599798918    Test Loss :0.009543290361762047\n",
      "Epoch :0.9375    Train Loss :0.002963573206216097    Test Loss :0.008072675205767155\n",
      "Epoch :0.95    Train Loss :0.002896711463108659    Test Loss :0.013257037848234177\n",
      "Epoch :0.9625    Train Loss :0.003443749388679862    Test Loss :0.007454404607415199\n",
      "Epoch :0.975    Train Loss :0.002733247820287943    Test Loss :0.011530676856637001\n",
      "Epoch :0.9875    Train Loss :0.0024272024165838957    Test Loss :0.016748595982789993\n",
      "Epoch :1.0    Train Loss :0.002974239643663168    Test Loss :0.014123968780040741\n",
      "RMSE: 18.425016251700498\n",
      "MAE: 15.747062030630829\n",
      "MAPE: 13.540225346730624%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 400, 'hid_size': 100, 'lr': 0.01, 'num_layers': 2}\n",
      "total:  66.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.053867679089307785    Test Loss :0.25818192958831787\n",
      "Epoch :0.025    Train Loss :0.047939714044332504    Test Loss :0.1718551516532898\n",
      "Epoch :0.0375    Train Loss :0.02342800423502922    Test Loss :0.027949552983045578\n",
      "Epoch :0.05    Train Loss :0.022540383040905    Test Loss :0.039879947900772095\n",
      "Epoch :0.0625    Train Loss :0.011443975381553173    Test Loss :0.01444170344620943\n",
      "Epoch :0.075    Train Loss :0.011544064618647099    Test Loss :0.033690422773361206\n",
      "Epoch :0.0875    Train Loss :0.012748215347528458    Test Loss :0.012910250574350357\n",
      "Epoch :0.1    Train Loss :0.008473374880850315    Test Loss :0.019733687862753868\n",
      "Epoch :0.1125    Train Loss :0.009364167228341103    Test Loss :0.02801203727722168\n",
      "Epoch :0.125    Train Loss :0.008248372003436089    Test Loss :0.017185360193252563\n",
      "Epoch :0.1375    Train Loss :0.008476841263473034    Test Loss :0.025852568447589874\n",
      "Epoch :0.15    Train Loss :0.007980660535395145    Test Loss :0.020694799721240997\n",
      "Epoch :0.1625    Train Loss :0.008320081047713757    Test Loss :0.014793504029512405\n",
      "Epoch :0.175    Train Loss :0.007551215589046478    Test Loss :0.01718754693865776\n",
      "Epoch :0.1875    Train Loss :0.008056196384131908    Test Loss :0.01210921723395586\n",
      "Epoch :0.2    Train Loss :0.0071435486897826195    Test Loss :0.01832822524011135\n",
      "Epoch :0.2125    Train Loss :0.00726123945787549    Test Loss :0.018717147409915924\n",
      "Epoch :0.225    Train Loss :0.00688400911167264    Test Loss :0.01813860423862934\n",
      "Epoch :0.2375    Train Loss :0.0062011233530938625    Test Loss :0.020638857036828995\n",
      "Epoch :0.25    Train Loss :0.006800052709877491    Test Loss :0.020730264484882355\n",
      "Epoch :0.2625    Train Loss :0.005845612846314907    Test Loss :0.015607710927724838\n",
      "Epoch :0.275    Train Loss :0.006553492974489927    Test Loss :0.02503843419253826\n",
      "Epoch :0.2875    Train Loss :0.0070505812764167786    Test Loss :0.013592025265097618\n",
      "Epoch :0.3    Train Loss :0.005692131817340851    Test Loss :0.013989085331559181\n",
      "Epoch :0.3125    Train Loss :0.006281311623752117    Test Loss :0.011014092713594437\n",
      "Epoch :0.325    Train Loss :0.007440430577844381    Test Loss :0.022534403949975967\n",
      "Epoch :0.3375    Train Loss :0.007594140712171793    Test Loss :0.015801193192601204\n",
      "Epoch :0.35    Train Loss :0.0056038531474769115    Test Loss :0.01261335238814354\n",
      "Epoch :0.3625    Train Loss :0.006233236752450466    Test Loss :0.011560278944671154\n",
      "Epoch :0.375    Train Loss :0.005892058368772268    Test Loss :0.012999363243579865\n",
      "Epoch :0.3875    Train Loss :0.005221087019890547    Test Loss :0.016236379742622375\n",
      "Epoch :0.4    Train Loss :0.006011967081576586    Test Loss :0.013551115058362484\n",
      "Epoch :0.4125    Train Loss :0.005587588530033827    Test Loss :0.012593613006174564\n",
      "Epoch :0.425    Train Loss :0.00548552256077528    Test Loss :0.012836959213018417\n",
      "Epoch :0.4375    Train Loss :0.005090954247862101    Test Loss :0.012902511283755302\n",
      "Epoch :0.45    Train Loss :0.005230726208537817    Test Loss :0.014558215625584126\n",
      "Epoch :0.4625    Train Loss :0.005388139747083187    Test Loss :0.021604284644126892\n",
      "Epoch :0.475    Train Loss :0.00519934156909585    Test Loss :0.016720639541745186\n",
      "Epoch :0.4875    Train Loss :0.005841458681970835    Test Loss :0.017642341554164886\n",
      "Epoch :0.5    Train Loss :0.005190008785575628    Test Loss :0.014378678984940052\n",
      "Epoch :0.5125    Train Loss :0.004470552783459425    Test Loss :0.01469117496162653\n",
      "Epoch :0.525    Train Loss :0.005294007249176502    Test Loss :0.01800793968141079\n",
      "Epoch :0.5375    Train Loss :0.004401045851409435    Test Loss :0.00953357107937336\n",
      "Epoch :0.55    Train Loss :0.004766491707414389    Test Loss :0.009203203022480011\n",
      "Epoch :0.5625    Train Loss :0.005484904162585735    Test Loss :0.0206928588449955\n",
      "Epoch :0.575    Train Loss :0.00553946103900671    Test Loss :0.010760552249848843\n",
      "Epoch :0.5875    Train Loss :0.004882552661001682    Test Loss :0.013836280442774296\n",
      "Epoch :0.6    Train Loss :0.00534777482971549    Test Loss :0.015242436900734901\n",
      "Epoch :0.6125    Train Loss :0.004499561153352261    Test Loss :0.012040491215884686\n",
      "Epoch :0.625    Train Loss :0.005052149761468172    Test Loss :0.009198599494993687\n",
      "Epoch :0.6375    Train Loss :0.005595872178673744    Test Loss :0.015883734449744225\n",
      "Epoch :0.65    Train Loss :0.004564561881124973    Test Loss :0.015965260565280914\n",
      "Epoch :0.6625    Train Loss :0.004753798712044954    Test Loss :0.014975897036492825\n",
      "Epoch :0.675    Train Loss :0.0034046608489006758    Test Loss :0.01176417712122202\n",
      "Epoch :0.6875    Train Loss :0.00414578802883625    Test Loss :0.01892280764877796\n",
      "Epoch :0.7    Train Loss :0.0040043373592197895    Test Loss :0.018313702195882797\n",
      "Epoch :0.7125    Train Loss :0.0041877334006130695    Test Loss :0.030391840264201164\n",
      "Epoch :0.725    Train Loss :0.00439314404502511    Test Loss :0.01223120465874672\n",
      "Epoch :0.7375    Train Loss :0.004101552069187164    Test Loss :0.01096989493817091\n",
      "Epoch :0.75    Train Loss :0.004295563790947199    Test Loss :0.011754661798477173\n",
      "Epoch :0.7625    Train Loss :0.0038845858070999384    Test Loss :0.014239242300391197\n",
      "Epoch :0.775    Train Loss :0.004095484036952257    Test Loss :0.010839945636689663\n",
      "Epoch :0.7875    Train Loss :0.004759959876537323    Test Loss :0.03737875074148178\n",
      "Epoch :0.8    Train Loss :0.0038802509661763906    Test Loss :0.04742874577641487\n",
      "Epoch :0.8125    Train Loss :0.003739116247743368    Test Loss :0.014625740237534046\n",
      "Epoch :0.825    Train Loss :0.003857060568407178    Test Loss :0.018180347979068756\n",
      "Epoch :0.8375    Train Loss :0.004169105552136898    Test Loss :0.01105338241904974\n",
      "Epoch :0.85    Train Loss :0.004353174474090338    Test Loss :0.018810000270605087\n",
      "Epoch :0.8625    Train Loss :0.0054766335524618626    Test Loss :0.04027925059199333\n",
      "Epoch :0.875    Train Loss :0.004899620544165373    Test Loss :0.015344508923590183\n",
      "Epoch :0.8875    Train Loss :0.0041013616137206554    Test Loss :0.012368115596473217\n",
      "Epoch :0.9    Train Loss :0.0039369999431073666    Test Loss :0.01526638213545084\n",
      "Epoch :0.9125    Train Loss :0.0033613606356084347    Test Loss :0.032532259821891785\n",
      "Epoch :0.925    Train Loss :0.0038950592279434204    Test Loss :0.037923555821180344\n",
      "Epoch :0.9375    Train Loss :0.0038807550445199013    Test Loss :0.020955685526132584\n",
      "Epoch :0.95    Train Loss :0.00423083221539855    Test Loss :0.06496962904930115\n",
      "Epoch :0.9625    Train Loss :0.003546615596860647    Test Loss :0.028832240030169487\n",
      "Epoch :0.975    Train Loss :0.003907377831637859    Test Loss :0.011094440706074238\n",
      "Epoch :0.9875    Train Loss :0.003790292888879776    Test Loss :0.01071494072675705\n",
      "Epoch :1.0    Train Loss :0.0049273781478405    Test Loss :0.0423445887863636\n",
      "RMSE: 18.635591937621896\n",
      "MAE: 14.594678942816298\n",
      "MAPE: 12.512129656743967%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 400, 'hid_size': 100, 'lr': 0.01, 'num_layers': 5}\n",
      "total:  69.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.08236236125230789    Test Loss :0.24077573418617249\n",
      "Epoch :0.025    Train Loss :0.037626296281814575    Test Loss :0.21533454954624176\n",
      "Epoch :0.0375    Train Loss :0.03391818329691887    Test Loss :0.05011766031384468\n",
      "Epoch :0.05    Train Loss :0.030454061925411224    Test Loss :0.1273869127035141\n",
      "Epoch :0.0625    Train Loss :0.017997613176703453    Test Loss :0.017358891665935516\n",
      "Epoch :0.075    Train Loss :0.008519833907485008    Test Loss :0.011409544385969639\n",
      "Epoch :0.0875    Train Loss :0.008474117144942284    Test Loss :0.022846398875117302\n",
      "Epoch :0.1    Train Loss :0.007593319285660982    Test Loss :0.008766131475567818\n",
      "Epoch :0.1125    Train Loss :0.006003193091601133    Test Loss :0.01818256825208664\n",
      "Epoch :0.125    Train Loss :0.006819061003625393    Test Loss :0.013844072818756104\n",
      "Epoch :0.1375    Train Loss :0.0057746791280806065    Test Loss :0.011278280057013035\n",
      "Epoch :0.15    Train Loss :0.00595506327226758    Test Loss :0.011629186570644379\n",
      "Epoch :0.1625    Train Loss :0.005871796514838934    Test Loss :0.01339438185095787\n",
      "Epoch :0.175    Train Loss :0.005618516821414232    Test Loss :0.016238296404480934\n",
      "Epoch :0.1875    Train Loss :0.005012551788240671    Test Loss :0.01182805560529232\n",
      "Epoch :0.2    Train Loss :0.00515417056158185    Test Loss :0.008388780057430267\n",
      "Epoch :0.2125    Train Loss :0.0051949950866401196    Test Loss :0.0114199323579669\n",
      "Epoch :0.225    Train Loss :0.0049206968396902084    Test Loss :0.009668742306530476\n",
      "Epoch :0.2375    Train Loss :0.005080978386104107    Test Loss :0.009066283702850342\n",
      "Epoch :0.25    Train Loss :0.004652004688978195    Test Loss :0.01300076674669981\n",
      "Epoch :0.2625    Train Loss :0.004320469219237566    Test Loss :0.013078595511615276\n",
      "Epoch :0.275    Train Loss :0.005112372804433107    Test Loss :0.00964006595313549\n",
      "Epoch :0.2875    Train Loss :0.004141977522522211    Test Loss :0.008871428668498993\n",
      "Epoch :0.3    Train Loss :0.004315332043915987    Test Loss :0.01360365841537714\n",
      "Epoch :0.3125    Train Loss :0.003813493764027953    Test Loss :0.009401144459843636\n",
      "Epoch :0.325    Train Loss :0.0033931962680071592    Test Loss :0.010739725083112717\n",
      "Epoch :0.3375    Train Loss :0.004630874842405319    Test Loss :0.011819441802799702\n",
      "Epoch :0.35    Train Loss :0.004261497873812914    Test Loss :0.009841205552220345\n",
      "Epoch :0.3625    Train Loss :0.004169089253991842    Test Loss :0.011686782352626324\n",
      "Epoch :0.375    Train Loss :0.003777919802814722    Test Loss :0.015732379630208015\n",
      "Epoch :0.3875    Train Loss :0.0035485585685819387    Test Loss :0.011273873969912529\n",
      "Epoch :0.4    Train Loss :0.004453583620488644    Test Loss :0.010434182360768318\n",
      "Epoch :0.4125    Train Loss :0.004420292563736439    Test Loss :0.008089607581496239\n",
      "Epoch :0.425    Train Loss :0.004589650314301252    Test Loss :0.012707317247986794\n",
      "Epoch :0.4375    Train Loss :0.003891000524163246    Test Loss :0.014604377560317516\n",
      "Epoch :0.45    Train Loss :0.004514054860919714    Test Loss :0.008589588105678558\n",
      "Epoch :0.4625    Train Loss :0.0034446262288838625    Test Loss :0.012322387658059597\n",
      "Epoch :0.475    Train Loss :0.0037137398030608892    Test Loss :0.008862542919814587\n",
      "Epoch :0.4875    Train Loss :0.0031511588022112846    Test Loss :0.008095720782876015\n",
      "Epoch :0.5    Train Loss :0.0032788459211587906    Test Loss :0.010131084360182285\n",
      "Epoch :0.5125    Train Loss :0.004106910899281502    Test Loss :0.009443399496376514\n",
      "Epoch :0.525    Train Loss :0.0038726467173546553    Test Loss :0.010606617666780949\n",
      "Epoch :0.5375    Train Loss :0.003117982065305114    Test Loss :0.008229528553783894\n",
      "Epoch :0.55    Train Loss :0.003247343935072422    Test Loss :0.00803180132061243\n",
      "Epoch :0.5625    Train Loss :0.003454845864325762    Test Loss :0.009075170382857323\n",
      "Epoch :0.575    Train Loss :0.0028693166095763445    Test Loss :0.006357694510370493\n",
      "Epoch :0.5875    Train Loss :0.0038676257245242596    Test Loss :0.007763576693832874\n",
      "Epoch :0.6    Train Loss :0.004060564562678337    Test Loss :0.012825749814510345\n",
      "Epoch :0.6125    Train Loss :0.003763827495276928    Test Loss :0.014325879514217377\n",
      "Epoch :0.625    Train Loss :0.003826226107776165    Test Loss :0.010506213642656803\n",
      "Epoch :0.6375    Train Loss :0.003717348212376237    Test Loss :0.0131539860740304\n",
      "Epoch :0.65    Train Loss :0.00363730825483799    Test Loss :0.008876901119947433\n",
      "Epoch :0.6625    Train Loss :0.004563871305435896    Test Loss :0.010305683128535748\n",
      "Epoch :0.675    Train Loss :0.0032904280815273523    Test Loss :0.007254267577081919\n",
      "Epoch :0.6875    Train Loss :0.0035800093319267035    Test Loss :0.007619209121912718\n",
      "Epoch :0.7    Train Loss :0.0037073700223118067    Test Loss :0.007588925771415234\n",
      "Epoch :0.7125    Train Loss :0.004199369810521603    Test Loss :0.01208432950079441\n",
      "Epoch :0.725    Train Loss :0.003389736171811819    Test Loss :0.006666878238320351\n",
      "Epoch :0.7375    Train Loss :0.003995743114501238    Test Loss :0.010767471976578236\n",
      "Epoch :0.75    Train Loss :0.003560939570888877    Test Loss :0.013849775306880474\n",
      "Epoch :0.7625    Train Loss :0.0034787205513566732    Test Loss :0.008208435960114002\n",
      "Epoch :0.775    Train Loss :0.0037636032793670893    Test Loss :0.007253924384713173\n",
      "Epoch :0.7875    Train Loss :0.003177283564582467    Test Loss :0.013135604560375214\n",
      "Epoch :0.8    Train Loss :0.0038580805994570255    Test Loss :0.008133400231599808\n",
      "Epoch :0.8125    Train Loss :0.003637674730271101    Test Loss :0.016386406496167183\n",
      "Epoch :0.825    Train Loss :0.00290492782369256    Test Loss :0.007195104379206896\n",
      "Epoch :0.8375    Train Loss :0.0034070392139256    Test Loss :0.01774294301867485\n",
      "Epoch :0.85    Train Loss :0.0033512767404317856    Test Loss :0.013178654946386814\n",
      "Epoch :0.8625    Train Loss :0.003049249527975917    Test Loss :0.009803751483559608\n",
      "Epoch :0.875    Train Loss :0.003300384385511279    Test Loss :0.009716055355966091\n",
      "Epoch :0.8875    Train Loss :0.003032033797353506    Test Loss :0.008578544482588768\n",
      "Epoch :0.9    Train Loss :0.0036005922593176365    Test Loss :0.01526915654540062\n",
      "Epoch :0.9125    Train Loss :0.003545258892700076    Test Loss :0.010814856737852097\n",
      "Epoch :0.925    Train Loss :0.003139290725812316    Test Loss :0.015823084861040115\n",
      "Epoch :0.9375    Train Loss :0.0038420120254158974    Test Loss :0.009856268763542175\n",
      "Epoch :0.95    Train Loss :0.0035186868626624346    Test Loss :0.007135380059480667\n",
      "Epoch :0.9625    Train Loss :0.003690683050081134    Test Loss :0.011882830411195755\n",
      "Epoch :0.975    Train Loss :0.003493385622277856    Test Loss :0.00935417041182518\n",
      "Epoch :0.9875    Train Loss :0.00371305993758142    Test Loss :0.00975849200040102\n",
      "Epoch :1.0    Train Loss :0.0036900986451655626    Test Loss :0.00903705507516861\n",
      "RMSE: 12.335186474655163\n",
      "MAE: 10.741257349354868\n",
      "MAPE: 9.32504521605668%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 400, 'hid_size': 100, 'lr': 0.02, 'num_layers': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  72.0\n",
      "Epoch :0.0125    Train Loss :0.21333573758602142    Test Loss :0.40060415863990784\n",
      "Epoch :0.025    Train Loss :0.059956539422273636    Test Loss :0.4287157356739044\n",
      "Epoch :0.0375    Train Loss :0.052632007747888565    Test Loss :0.2673276364803314\n",
      "Epoch :0.05    Train Loss :0.05782116577029228    Test Loss :0.22648844122886658\n",
      "Epoch :0.0625    Train Loss :0.053566526621580124    Test Loss :0.23570573329925537\n",
      "Epoch :0.075    Train Loss :0.05672561749815941    Test Loss :0.22129207849502563\n",
      "Epoch :0.0875    Train Loss :0.052424319088459015    Test Loss :0.2284162938594818\n",
      "Epoch :0.1    Train Loss :0.05178206413984299    Test Loss :0.2327328473329544\n",
      "Epoch :0.1125    Train Loss :0.053178638219833374    Test Loss :0.2537548542022705\n",
      "Epoch :0.125    Train Loss :0.051053375005722046    Test Loss :0.24322852492332458\n",
      "Epoch :0.1375    Train Loss :0.050581324845552444    Test Loss :0.21639908850193024\n",
      "Epoch :0.15    Train Loss :0.05081452429294586    Test Loss :0.21284785866737366\n",
      "Epoch :0.1625    Train Loss :0.05186459794640541    Test Loss :0.23042616248130798\n",
      "Epoch :0.175    Train Loss :0.05103062465786934    Test Loss :0.22777268290519714\n",
      "Epoch :0.1875    Train Loss :0.05137575417757034    Test Loss :0.22496303915977478\n",
      "Epoch :0.2    Train Loss :0.04625944048166275    Test Loss :0.2105497568845749\n",
      "Epoch :0.2125    Train Loss :0.04668567702174187    Test Loss :0.15866699814796448\n",
      "Epoch :0.225    Train Loss :0.024221127852797508    Test Loss :0.037504903972148895\n",
      "Epoch :0.2375    Train Loss :0.016457339748740196    Test Loss :0.024437617510557175\n",
      "Epoch :0.25    Train Loss :0.014948561787605286    Test Loss :0.02936769835650921\n",
      "Epoch :0.2625    Train Loss :0.025106560438871384    Test Loss :0.13602927327156067\n",
      "Epoch :0.275    Train Loss :0.02090265229344368    Test Loss :0.04800965636968613\n",
      "Epoch :0.2875    Train Loss :0.0224462803453207    Test Loss :0.06705012172460556\n",
      "Epoch :0.3    Train Loss :0.024071931838989258    Test Loss :0.012985128909349442\n",
      "Epoch :0.3125    Train Loss :0.016037296503782272    Test Loss :0.01594841107726097\n",
      "Epoch :0.325    Train Loss :0.013050145469605923    Test Loss :0.061207301914691925\n",
      "Epoch :0.3375    Train Loss :0.012137847021222115    Test Loss :0.013332676142454147\n",
      "Epoch :0.35    Train Loss :0.01554785668849945    Test Loss :0.022761665284633636\n",
      "Epoch :0.3625    Train Loss :0.010229040868580341    Test Loss :0.04020996764302254\n",
      "Epoch :0.375    Train Loss :0.010060284286737442    Test Loss :0.017263373360037804\n",
      "Epoch :0.3875    Train Loss :0.009196356870234013    Test Loss :0.025219598785042763\n",
      "Epoch :0.4    Train Loss :0.009585246443748474    Test Loss :0.02294238656759262\n",
      "Epoch :0.4125    Train Loss :0.00936898309737444    Test Loss :0.019836071878671646\n",
      "Epoch :0.425    Train Loss :0.009377598762512207    Test Loss :0.035633157938718796\n",
      "Epoch :0.4375    Train Loss :0.008366123773157597    Test Loss :0.01946306601166725\n",
      "Epoch :0.45    Train Loss :0.009032675065100193    Test Loss :0.02305695414543152\n",
      "Epoch :0.4625    Train Loss :0.009505527094006538    Test Loss :0.026203295215964317\n",
      "Epoch :0.475    Train Loss :0.008944659493863583    Test Loss :0.015671629458665848\n",
      "Epoch :0.4875    Train Loss :0.008920219726860523    Test Loss :0.018304189667105675\n",
      "Epoch :0.5    Train Loss :0.009391602128744125    Test Loss :0.028924353420734406\n",
      "Epoch :0.5125    Train Loss :0.010255150496959686    Test Loss :0.01268831267952919\n",
      "Epoch :0.525    Train Loss :0.008677862584590912    Test Loss :0.01947529800236225\n",
      "Epoch :0.5375    Train Loss :0.009596158750355244    Test Loss :0.029031170532107353\n",
      "Epoch :0.55    Train Loss :0.008805250748991966    Test Loss :0.023246273398399353\n",
      "Epoch :0.5625    Train Loss :0.008118993602693081    Test Loss :0.019425764679908752\n",
      "Epoch :0.575    Train Loss :0.009321365505456924    Test Loss :0.01880158670246601\n",
      "Epoch :0.5875    Train Loss :0.008108044043183327    Test Loss :0.026638854295015335\n",
      "Epoch :0.6    Train Loss :0.00976432766765356    Test Loss :0.016011593863368034\n",
      "Epoch :0.6125    Train Loss :0.00808714423328638    Test Loss :0.03114495985209942\n",
      "Epoch :0.625    Train Loss :0.007872057147324085    Test Loss :0.02642701007425785\n",
      "Epoch :0.6375    Train Loss :0.00750625180080533    Test Loss :0.017245255410671234\n",
      "Epoch :0.65    Train Loss :0.008599944412708282    Test Loss :0.02235931158065796\n",
      "Epoch :0.6625    Train Loss :0.006804155185818672    Test Loss :0.021525966003537178\n",
      "Epoch :0.675    Train Loss :0.007188910152763128    Test Loss :0.022306350991129875\n",
      "Epoch :0.6875    Train Loss :0.007676002569496632    Test Loss :0.019185015931725502\n",
      "Epoch :0.7    Train Loss :0.00776728056371212    Test Loss :0.020564554259181023\n",
      "Epoch :0.7125    Train Loss :0.0077670905739068985    Test Loss :0.02483699657022953\n",
      "Epoch :0.725    Train Loss :0.007830017246305943    Test Loss :0.022022390738129616\n",
      "Epoch :0.7375    Train Loss :0.006605066359043121    Test Loss :0.02133975923061371\n",
      "Epoch :0.75    Train Loss :0.007084470707923174    Test Loss :0.021149301901459694\n",
      "Epoch :0.7625    Train Loss :0.006995421834290028    Test Loss :0.01846044510602951\n",
      "Epoch :0.775    Train Loss :0.006292411126196384    Test Loss :0.014467546716332436\n",
      "Epoch :0.7875    Train Loss :0.007556631229817867    Test Loss :0.02026916854083538\n",
      "Epoch :0.8    Train Loss :0.007345527410507202    Test Loss :0.020975833758711815\n",
      "Epoch :0.8125    Train Loss :0.006772763095796108    Test Loss :0.01663743518292904\n",
      "Epoch :0.825    Train Loss :0.006872306577861309    Test Loss :0.01638357900083065\n",
      "Epoch :0.8375    Train Loss :0.0073589738458395    Test Loss :0.017362739890813828\n",
      "Epoch :0.85    Train Loss :0.0074166590347886086    Test Loss :0.018498722463846207\n",
      "Epoch :0.8625    Train Loss :0.006898835767060518    Test Loss :0.01891281269490719\n",
      "Epoch :0.875    Train Loss :0.007837584242224693    Test Loss :0.015676220878958702\n",
      "Epoch :0.8875    Train Loss :0.008020007982850075    Test Loss :0.024508118629455566\n",
      "Epoch :0.9    Train Loss :0.006820825394243002    Test Loss :0.015531188808381557\n",
      "Epoch :0.9125    Train Loss :0.006407740991562605    Test Loss :0.02413761615753174\n",
      "Epoch :0.925    Train Loss :0.007241593673825264    Test Loss :0.01506419200450182\n",
      "Epoch :0.9375    Train Loss :0.006883302237838507    Test Loss :0.02259826473891735\n",
      "Epoch :0.95    Train Loss :0.007177074905484915    Test Loss :0.019191507250070572\n",
      "Epoch :0.9625    Train Loss :0.007084293756633997    Test Loss :0.013599607162177563\n",
      "Epoch :0.975    Train Loss :0.006729749962687492    Test Loss :0.018410682678222656\n",
      "Epoch :0.9875    Train Loss :0.0065455990843474865    Test Loss :0.013345299288630486\n",
      "Epoch :1.0    Train Loss :0.006241287104785442    Test Loss :0.01817946508526802\n",
      "RMSE: 12.282406396865847\n",
      "MAE: 10.308917108087119\n",
      "MAPE: 8.820845165543826%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 400, 'hid_size': 100, 'lr': 0.02, 'num_layers': 5}\n",
      "total:  75.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.06109442934393883    Test Loss :0.41116759181022644\n",
      "Epoch :0.016666666666666666    Train Loss :0.06943023204803467    Test Loss :0.32151085138320923\n",
      "Epoch :0.025    Train Loss :0.05366520211100578    Test Loss :0.0949799194931984\n",
      "Epoch :0.03333333333333333    Train Loss :0.03888227790594101    Test Loss :0.1845715045928955\n",
      "Epoch :0.041666666666666664    Train Loss :0.022813156247138977    Test Loss :0.034488264471292496\n",
      "Epoch :0.05    Train Loss :0.014015564695000648    Test Loss :0.03140971064567566\n",
      "Epoch :0.058333333333333334    Train Loss :0.012295876629650593    Test Loss :0.022104760631918907\n",
      "Epoch :0.06666666666666667    Train Loss :0.011770818382501602    Test Loss :0.022629927843809128\n",
      "Epoch :0.075    Train Loss :0.00936025008559227    Test Loss :0.013878438621759415\n",
      "Epoch :0.08333333333333333    Train Loss :0.007678160443902016    Test Loss :0.014674544334411621\n",
      "Epoch :0.09166666666666666    Train Loss :0.010398424230515957    Test Loss :0.016390586271882057\n",
      "Epoch :0.1    Train Loss :0.007094749249517918    Test Loss :0.009044262580573559\n",
      "Epoch :0.10833333333333334    Train Loss :0.007389426231384277    Test Loss :0.012965490110218525\n",
      "Epoch :0.11666666666666667    Train Loss :0.007182594854384661    Test Loss :0.00974312424659729\n",
      "Epoch :0.125    Train Loss :0.007786729373037815    Test Loss :0.008986537344753742\n",
      "Epoch :0.13333333333333333    Train Loss :0.008450133726000786    Test Loss :0.00992509350180626\n",
      "Epoch :0.14166666666666666    Train Loss :0.005649416241794825    Test Loss :0.012368077412247658\n",
      "Epoch :0.15    Train Loss :0.007259469944983721    Test Loss :0.014285917393863201\n",
      "Epoch :0.15833333333333333    Train Loss :0.007392138708382845    Test Loss :0.012981163337826729\n",
      "Epoch :0.16666666666666666    Train Loss :0.005807029083371162    Test Loss :0.013370098546147346\n",
      "Epoch :0.175    Train Loss :0.006761198863387108    Test Loss :0.010199490934610367\n",
      "Epoch :0.18333333333333332    Train Loss :0.006106818560510874    Test Loss :0.009611185640096664\n",
      "Epoch :0.19166666666666668    Train Loss :0.006629399489611387    Test Loss :0.012933643534779549\n",
      "Epoch :0.2    Train Loss :0.005577148869633675    Test Loss :0.0127905597910285\n",
      "Epoch :0.20833333333333334    Train Loss :0.00648654717952013    Test Loss :0.008900181390345097\n",
      "Epoch :0.21666666666666667    Train Loss :0.005977365653961897    Test Loss :0.013502159155905247\n",
      "Epoch :0.225    Train Loss :0.005909335799515247    Test Loss :0.010800410062074661\n",
      "Epoch :0.23333333333333334    Train Loss :0.006212595384567976    Test Loss :0.016585111618041992\n",
      "Epoch :0.24166666666666667    Train Loss :0.004925934132188559    Test Loss :0.012363667599856853\n",
      "Epoch :0.25    Train Loss :0.005841095466166735    Test Loss :0.01028556190431118\n",
      "Epoch :0.25833333333333336    Train Loss :0.005799922160804272    Test Loss :0.01284157857298851\n",
      "Epoch :0.26666666666666666    Train Loss :0.005451524630188942    Test Loss :0.01233070157468319\n",
      "Epoch :0.275    Train Loss :0.005050082225352526    Test Loss :0.012576112523674965\n",
      "Epoch :0.2833333333333333    Train Loss :0.004896475933492184    Test Loss :0.012074335478246212\n",
      "Epoch :0.2916666666666667    Train Loss :0.0043977005407214165    Test Loss :0.011502639390528202\n",
      "Epoch :0.3    Train Loss :0.004828763660043478    Test Loss :0.015930132940411568\n",
      "Epoch :0.30833333333333335    Train Loss :0.004080344457179308    Test Loss :0.01768431067466736\n",
      "Epoch :0.31666666666666665    Train Loss :0.0043035545386374    Test Loss :0.00933031365275383\n",
      "Epoch :0.325    Train Loss :0.004359810147434473    Test Loss :0.009456627070903778\n",
      "Epoch :0.3333333333333333    Train Loss :0.004519165959209204    Test Loss :0.013921099714934826\n",
      "Epoch :0.3416666666666667    Train Loss :0.004808803088963032    Test Loss :0.014707505702972412\n",
      "Epoch :0.35    Train Loss :0.004422476515173912    Test Loss :0.016929123550653458\n",
      "Epoch :0.35833333333333334    Train Loss :0.004469241015613079    Test Loss :0.012169431895017624\n",
      "Epoch :0.36666666666666664    Train Loss :0.004594228230416775    Test Loss :0.013667295686900616\n",
      "Epoch :0.375    Train Loss :0.004829811863601208    Test Loss :0.012822790071368217\n",
      "Epoch :0.38333333333333336    Train Loss :0.003950863145291805    Test Loss :0.015332624316215515\n",
      "Epoch :0.39166666666666666    Train Loss :0.0047915019094944    Test Loss :0.01048904750496149\n",
      "Epoch :0.4    Train Loss :0.004350519739091396    Test Loss :0.010049848817288876\n",
      "Epoch :0.4083333333333333    Train Loss :0.005274957977235317    Test Loss :0.01776307262480259\n",
      "Epoch :0.4166666666666667    Train Loss :0.004403049126267433    Test Loss :0.010111678391695023\n",
      "Epoch :0.425    Train Loss :0.0040334188379347324    Test Loss :0.011555859819054604\n",
      "Epoch :0.43333333333333335    Train Loss :0.0030910270288586617    Test Loss :0.018927795812487602\n",
      "Epoch :0.44166666666666665    Train Loss :0.0035336166620254517    Test Loss :0.015941444784402847\n",
      "Epoch :0.45    Train Loss :0.003594361711293459    Test Loss :0.017690755426883698\n",
      "Epoch :0.4583333333333333    Train Loss :0.0036873293574899435    Test Loss :0.019178267568349838\n",
      "Epoch :0.4666666666666667    Train Loss :0.003807909321039915    Test Loss :0.016798842698335648\n",
      "Epoch :0.475    Train Loss :0.0042081051506102085    Test Loss :0.04029922932386398\n",
      "Epoch :0.48333333333333334    Train Loss :0.0048320963978767395    Test Loss :0.023821977898478508\n",
      "Epoch :0.49166666666666664    Train Loss :0.0036029962357133627    Test Loss :0.008803317323327065\n",
      "Epoch :0.5    Train Loss :0.0038518987130373716    Test Loss :0.016596157103776932\n",
      "Epoch :0.5083333333333333    Train Loss :0.0036958521232008934    Test Loss :0.016612401232123375\n",
      "Epoch :0.5166666666666667    Train Loss :0.0032551626209169626    Test Loss :0.015198977664113045\n",
      "Epoch :0.525    Train Loss :0.0037458338774740696    Test Loss :0.011723773553967476\n",
      "Epoch :0.5333333333333333    Train Loss :0.0034560621716082096    Test Loss :0.028198055922985077\n",
      "Epoch :0.5416666666666666    Train Loss :0.003056370886042714    Test Loss :0.01651809923350811\n",
      "Epoch :0.55    Train Loss :0.0038611104246228933    Test Loss :0.011357120238244534\n",
      "Epoch :0.5583333333333333    Train Loss :0.0032894525211304426    Test Loss :0.012942010536789894\n",
      "Epoch :0.5666666666666667    Train Loss :0.003942136187106371    Test Loss :0.022644832730293274\n",
      "Epoch :0.575    Train Loss :0.0031966527458280325    Test Loss :0.024882847443223\n",
      "Epoch :0.5833333333333334    Train Loss :0.0032871123403310776    Test Loss :0.01546366699039936\n",
      "Epoch :0.5916666666666667    Train Loss :0.0032093359623104334    Test Loss :0.021168487146496773\n",
      "Epoch :0.6    Train Loss :0.003621921641752124    Test Loss :0.019281750544905663\n",
      "Epoch :0.6083333333333333    Train Loss :0.003296450711786747    Test Loss :0.013438379392027855\n",
      "Epoch :0.6166666666666667    Train Loss :0.0038886191323399544    Test Loss :0.011024609208106995\n",
      "Epoch :0.625    Train Loss :0.0029395236633718014    Test Loss :0.012745381332933903\n",
      "Epoch :0.6333333333333333    Train Loss :0.0034714892972260714    Test Loss :0.018616385757923126\n",
      "Epoch :0.6416666666666667    Train Loss :0.002787355100736022    Test Loss :0.02000916376709938\n",
      "Epoch :0.65    Train Loss :0.003260451601818204    Test Loss :0.009905258193612099\n",
      "Epoch :0.6583333333333333    Train Loss :0.003213549265637994    Test Loss :0.016726670786738396\n",
      "Epoch :0.6666666666666666    Train Loss :0.0029189083725214005    Test Loss :0.0077594369649887085\n",
      "Epoch :0.675    Train Loss :0.004142739810049534    Test Loss :0.01620176061987877\n",
      "Epoch :0.6833333333333333    Train Loss :0.00299928174354136    Test Loss :0.019196534529328346\n",
      "Epoch :0.6916666666666667    Train Loss :0.003254776354879141    Test Loss :0.028530040755867958\n",
      "Epoch :0.7    Train Loss :0.003498126519843936    Test Loss :0.01893080770969391\n",
      "Epoch :0.7083333333333334    Train Loss :0.0025986675173044205    Test Loss :0.022635681554675102\n",
      "Epoch :0.7166666666666667    Train Loss :0.0029973413329571486    Test Loss :0.012470303103327751\n",
      "Epoch :0.725    Train Loss :0.0038327735383063555    Test Loss :0.01493129227310419\n",
      "Epoch :0.7333333333333333    Train Loss :0.003545445390045643    Test Loss :0.03424810618162155\n",
      "Epoch :0.7416666666666667    Train Loss :0.00290824961848557    Test Loss :0.028865743428468704\n",
      "Epoch :0.75    Train Loss :0.0032629843335598707    Test Loss :0.03288576379418373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7583333333333333    Train Loss :0.002835615538060665    Test Loss :0.019120821729302406\n",
      "Epoch :0.7666666666666667    Train Loss :0.0029011601582169533    Test Loss :0.01882646419107914\n",
      "Epoch :0.775    Train Loss :0.0030733379535377026    Test Loss :0.012913031503558159\n",
      "Epoch :0.7833333333333333    Train Loss :0.004328194539994001    Test Loss :0.01586126908659935\n",
      "Epoch :0.7916666666666666    Train Loss :0.0035255274269729853    Test Loss :0.007408555597066879\n",
      "Epoch :0.8    Train Loss :0.0025300001725554466    Test Loss :0.017284749075770378\n",
      "Epoch :0.8083333333333333    Train Loss :0.00347826792858541    Test Loss :0.04283241927623749\n",
      "Epoch :0.8166666666666667    Train Loss :0.0037375115789473057    Test Loss :0.007866719737648964\n",
      "Epoch :0.825    Train Loss :0.003780846018344164    Test Loss :0.006552387960255146\n",
      "Epoch :0.8333333333333334    Train Loss :0.003685646690428257    Test Loss :0.008305748924612999\n",
      "Epoch :0.8416666666666667    Train Loss :0.0033763956744223833    Test Loss :0.01678146980702877\n",
      "Epoch :0.85    Train Loss :0.0032467993441969156    Test Loss :0.007888131774961948\n",
      "Epoch :0.8583333333333333    Train Loss :0.0031725859735161066    Test Loss :0.01119675301015377\n",
      "Epoch :0.8666666666666667    Train Loss :0.0029974819626659155    Test Loss :0.015039721503853798\n",
      "Epoch :0.875    Train Loss :0.002829083940014243    Test Loss :0.012334167025983334\n",
      "Epoch :0.8833333333333333    Train Loss :0.004145805258303881    Test Loss :0.00952982623130083\n",
      "Epoch :0.8916666666666667    Train Loss :0.0030725591350346804    Test Loss :0.02857045643031597\n",
      "Epoch :0.9    Train Loss :0.00394402677193284    Test Loss :0.020021557807922363\n",
      "Epoch :0.9083333333333333    Train Loss :0.0030972894746810198    Test Loss :0.022016022354364395\n",
      "Epoch :0.9166666666666666    Train Loss :0.0036434419453144073    Test Loss :0.014692450873553753\n",
      "Epoch :0.925    Train Loss :0.00317242625169456    Test Loss :0.022708380594849586\n",
      "Epoch :0.9333333333333333    Train Loss :0.0024532817769795656    Test Loss :0.03810712695121765\n",
      "Epoch :0.9416666666666667    Train Loss :0.0032864289823919535    Test Loss :0.03655104339122772\n",
      "Epoch :0.95    Train Loss :0.0030897953547537327    Test Loss :0.04528149217367172\n",
      "Epoch :0.9583333333333334    Train Loss :0.002944580977782607    Test Loss :0.021013792604207993\n",
      "Epoch :0.9666666666666667    Train Loss :0.002898130100220442    Test Loss :0.01324717327952385\n",
      "Epoch :0.975    Train Loss :0.0030577850993722677    Test Loss :0.013599592261016369\n",
      "Epoch :0.9833333333333333    Train Loss :0.0029710172675549984    Test Loss :0.0198320671916008\n",
      "Epoch :0.9916666666666667    Train Loss :0.0031236372888088226    Test Loss :0.022679731249809265\n",
      "Epoch :1.0    Train Loss :0.003426952753216028    Test Loss :0.030291413888335228\n",
      "RMSE: 19.89721674919844\n",
      "MAE: 17.976110435160237\n",
      "MAPE: 15.620169617867463%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 50, 'lr': 0.01, 'num_layers': 2}\n",
      "total: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 78.0\n",
      "Epoch :0.008333333333333333    Train Loss :0.05660523846745491    Test Loss :0.31541746854782104\n",
      "Epoch :0.016666666666666666    Train Loss :0.05213453620672226    Test Loss :0.2353406548500061\n",
      "Epoch :0.025    Train Loss :0.05241621285676956    Test Loss :0.22458034753799438\n",
      "Epoch :0.03333333333333333    Train Loss :0.051530394703149796    Test Loss :0.2419181764125824\n",
      "Epoch :0.041666666666666664    Train Loss :0.0494767464697361    Test Loss :0.2089473009109497\n",
      "Epoch :0.05    Train Loss :0.02147369645535946    Test Loss :0.062495309859514236\n",
      "Epoch :0.058333333333333334    Train Loss :0.020785387605428696    Test Loss :0.03246332332491875\n",
      "Epoch :0.06666666666666667    Train Loss :0.017011601477861404    Test Loss :0.07844215631484985\n",
      "Epoch :0.075    Train Loss :0.013146068900823593    Test Loss :0.023073367774486542\n",
      "Epoch :0.08333333333333333    Train Loss :0.012804023921489716    Test Loss :0.028086790814995766\n",
      "Epoch :0.09166666666666666    Train Loss :0.013009295798838139    Test Loss :0.025717487558722496\n",
      "Epoch :0.1    Train Loss :0.012299366295337677    Test Loss :0.03045453131198883\n",
      "Epoch :0.10833333333333334    Train Loss :0.012036717496812344    Test Loss :0.04324452579021454\n",
      "Epoch :0.11666666666666667    Train Loss :0.010194375179708004    Test Loss :0.029935508966445923\n",
      "Epoch :0.125    Train Loss :0.008575358428061008    Test Loss :0.011062485165894032\n",
      "Epoch :0.13333333333333333    Train Loss :0.01060351263731718    Test Loss :0.026808802038431168\n",
      "Epoch :0.14166666666666666    Train Loss :0.010245158337056637    Test Loss :0.022875193506479263\n",
      "Epoch :0.15    Train Loss :0.00893852673470974    Test Loss :0.023063797503709793\n",
      "Epoch :0.15833333333333333    Train Loss :0.008742321282625198    Test Loss :0.016244031488895416\n",
      "Epoch :0.16666666666666666    Train Loss :0.008840278722345829    Test Loss :0.018281664699316025\n",
      "Epoch :0.175    Train Loss :0.007883871905505657    Test Loss :0.017387505620718002\n",
      "Epoch :0.18333333333333332    Train Loss :0.008397550322115421    Test Loss :0.0198415145277977\n",
      "Epoch :0.19166666666666668    Train Loss :0.007697991095483303    Test Loss :0.014274884946644306\n",
      "Epoch :0.2    Train Loss :0.007680890150368214    Test Loss :0.019099485129117966\n",
      "Epoch :0.20833333333333334    Train Loss :0.008087154477834702    Test Loss :0.020130399614572525\n",
      "Epoch :0.21666666666666667    Train Loss :0.007645357400178909    Test Loss :0.02046503871679306\n",
      "Epoch :0.225    Train Loss :0.007549621630460024    Test Loss :0.014245405793190002\n",
      "Epoch :0.23333333333333334    Train Loss :0.008405548520386219    Test Loss :0.01599350944161415\n",
      "Epoch :0.24166666666666667    Train Loss :0.007703219074755907    Test Loss :0.02004634402692318\n",
      "Epoch :0.25    Train Loss :0.007808659225702286    Test Loss :0.016876881942152977\n",
      "Epoch :0.25833333333333336    Train Loss :0.007895008660852909    Test Loss :0.015933793038129807\n",
      "Epoch :0.26666666666666666    Train Loss :0.007197592407464981    Test Loss :0.01780521497130394\n",
      "Epoch :0.275    Train Loss :0.007033537607640028    Test Loss :0.018433716148138046\n",
      "Epoch :0.2833333333333333    Train Loss :0.006367535796016455    Test Loss :0.016149595379829407\n",
      "Epoch :0.2916666666666667    Train Loss :0.007808403577655554    Test Loss :0.018826071172952652\n",
      "Epoch :0.3    Train Loss :0.006816148292273283    Test Loss :0.018417619168758392\n",
      "Epoch :0.30833333333333335    Train Loss :0.007106056436896324    Test Loss :0.017587793990969658\n",
      "Epoch :0.31666666666666665    Train Loss :0.006623487453907728    Test Loss :0.01893179677426815\n",
      "Epoch :0.325    Train Loss :0.006628280505537987    Test Loss :0.017892973497509956\n",
      "Epoch :0.3333333333333333    Train Loss :0.006766917649656534    Test Loss :0.017939794808626175\n",
      "Epoch :0.3416666666666667    Train Loss :0.007069872692227364    Test Loss :0.017631739377975464\n",
      "Epoch :0.35    Train Loss :0.006356141995638609    Test Loss :0.01775483228266239\n",
      "Epoch :0.35833333333333334    Train Loss :0.006535270716995001    Test Loss :0.018306836485862732\n",
      "Epoch :0.36666666666666664    Train Loss :0.006273063365370035    Test Loss :0.01703491248190403\n",
      "Epoch :0.375    Train Loss :0.0066505130380392075    Test Loss :0.01990601420402527\n",
      "Epoch :0.38333333333333336    Train Loss :0.005724165588617325    Test Loss :0.017213167622685432\n",
      "Epoch :0.39166666666666666    Train Loss :0.006665869150310755    Test Loss :0.015342802740633488\n",
      "Epoch :0.4    Train Loss :0.006213676184415817    Test Loss :0.017929714173078537\n",
      "Epoch :0.4083333333333333    Train Loss :0.006765374913811684    Test Loss :0.0158308707177639\n",
      "Epoch :0.4166666666666667    Train Loss :0.006351341959089041    Test Loss :0.016146663576364517\n",
      "Epoch :0.425    Train Loss :0.005896460264921188    Test Loss :0.019307337701320648\n",
      "Epoch :0.43333333333333335    Train Loss :0.00585732189938426    Test Loss :0.01836516335606575\n",
      "Epoch :0.44166666666666665    Train Loss :0.005915416404604912    Test Loss :0.017482850700616837\n",
      "Epoch :0.45    Train Loss :0.0059265997260808945    Test Loss :0.022833464667201042\n",
      "Epoch :0.4583333333333333    Train Loss :0.006481080781668425    Test Loss :0.01436495129019022\n",
      "Epoch :0.4666666666666667    Train Loss :0.006228992249816656    Test Loss :0.01625959761440754\n",
      "Epoch :0.475    Train Loss :0.005787018686532974    Test Loss :0.01931857131421566\n",
      "Epoch :0.48333333333333334    Train Loss :0.006598868872970343    Test Loss :0.01447000727057457\n",
      "Epoch :0.49166666666666664    Train Loss :0.0055688428692519665    Test Loss :0.015656238421797752\n",
      "Epoch :0.5    Train Loss :0.006053556688129902    Test Loss :0.018644891679286957\n",
      "Epoch :0.5083333333333333    Train Loss :0.006071682088077068    Test Loss :0.017312096431851387\n",
      "Epoch :0.5166666666666667    Train Loss :0.006388713605701923    Test Loss :0.01933782920241356\n",
      "Epoch :0.525    Train Loss :0.006245758850127459    Test Loss :0.017714377492666245\n",
      "Epoch :0.5333333333333333    Train Loss :0.005399970803409815    Test Loss :0.014814320020377636\n",
      "Epoch :0.5416666666666666    Train Loss :0.005447994917631149    Test Loss :0.01688690297305584\n",
      "Epoch :0.55    Train Loss :0.006823550444096327    Test Loss :0.015854045748710632\n",
      "Epoch :0.5583333333333333    Train Loss :0.0055957818403840065    Test Loss :0.015588930808007717\n",
      "Epoch :0.5666666666666667    Train Loss :0.005555837880820036    Test Loss :0.01754508726298809\n",
      "Epoch :0.575    Train Loss :0.005739604588598013    Test Loss :0.016563324257731438\n",
      "Epoch :0.5833333333333334    Train Loss :0.005279548931866884    Test Loss :0.017578652128577232\n",
      "Epoch :0.5916666666666667    Train Loss :0.005700068548321724    Test Loss :0.017378443852066994\n",
      "Epoch :0.6    Train Loss :0.005790099035948515    Test Loss :0.017180681228637695\n",
      "Epoch :0.6083333333333333    Train Loss :0.0057902466505765915    Test Loss :0.016894252970814705\n",
      "Epoch :0.6166666666666667    Train Loss :0.0056981188245117664    Test Loss :0.017881236970424652\n",
      "Epoch :0.625    Train Loss :0.005339095368981361    Test Loss :0.017366040498018265\n",
      "Epoch :0.6333333333333333    Train Loss :0.005476582329720259    Test Loss :0.016660233959555626\n",
      "Epoch :0.6416666666666667    Train Loss :0.005541479680687189    Test Loss :0.017728330567479134\n",
      "Epoch :0.65    Train Loss :0.0059776161797344685    Test Loss :0.015128476545214653\n",
      "Epoch :0.6583333333333333    Train Loss :0.006174028385430574    Test Loss :0.018434613943099976\n",
      "Epoch :0.6666666666666666    Train Loss :0.0057450239546597    Test Loss :0.014757048338651657\n",
      "Epoch :0.675    Train Loss :0.005196576472371817    Test Loss :0.013741621747612953\n",
      "Epoch :0.6833333333333333    Train Loss :0.005200833547860384    Test Loss :0.01929406449198723\n",
      "Epoch :0.6916666666666667    Train Loss :0.005893449764698744    Test Loss :0.01742617040872574\n",
      "Epoch :0.7    Train Loss :0.0049418299458920956    Test Loss :0.018153218552470207\n",
      "Epoch :0.7083333333333334    Train Loss :0.006458712741732597    Test Loss :0.016011083498597145\n",
      "Epoch :0.7166666666666667    Train Loss :0.004985582083463669    Test Loss :0.01596018113195896\n",
      "Epoch :0.725    Train Loss :0.0054402281530201435    Test Loss :0.016059691086411476\n",
      "Epoch :0.7333333333333333    Train Loss :0.005896939896047115    Test Loss :0.0162564255297184\n",
      "Epoch :0.7416666666666667    Train Loss :0.004917818587273359    Test Loss :0.017130279913544655\n",
      "Epoch :0.75    Train Loss :0.005407293327152729    Test Loss :0.018809223547577858\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7583333333333333    Train Loss :0.004778555128723383    Test Loss :0.012910213321447372\n",
      "Epoch :0.7666666666666667    Train Loss :0.005429685581475496    Test Loss :0.01901647448539734\n",
      "Epoch :0.775    Train Loss :0.00497009139508009    Test Loss :0.015823382884263992\n",
      "Epoch :0.7833333333333333    Train Loss :0.005491109564900398    Test Loss :0.018948659300804138\n",
      "Epoch :0.7916666666666666    Train Loss :0.005059155635535717    Test Loss :0.019115719944238663\n",
      "Epoch :0.8    Train Loss :0.005774429999291897    Test Loss :0.019820312038064003\n",
      "Epoch :0.8083333333333333    Train Loss :0.005008647684007883    Test Loss :0.016142187640070915\n",
      "Epoch :0.8166666666666667    Train Loss :0.00451111514121294    Test Loss :0.01634315401315689\n",
      "Epoch :0.825    Train Loss :0.005019389092922211    Test Loss :0.015987183898687363\n",
      "Epoch :0.8333333333333334    Train Loss :0.0049071768298745155    Test Loss :0.016379861161112785\n",
      "Epoch :0.8416666666666667    Train Loss :0.0049940976314246655    Test Loss :0.0181456096470356\n",
      "Epoch :0.85    Train Loss :0.005415929015725851    Test Loss :0.01612170599400997\n",
      "Epoch :0.8583333333333333    Train Loss :0.004442671779543161    Test Loss :0.019771970808506012\n",
      "Epoch :0.8666666666666667    Train Loss :0.005470653995871544    Test Loss :0.023944206535816193\n",
      "Epoch :0.875    Train Loss :0.005089175421744585    Test Loss :0.016475966200232506\n",
      "Epoch :0.8833333333333333    Train Loss :0.005476243793964386    Test Loss :0.02090100198984146\n",
      "Epoch :0.8916666666666667    Train Loss :0.004778335802257061    Test Loss :0.016348741948604584\n",
      "Epoch :0.9    Train Loss :0.004833064042031765    Test Loss :0.02060185931622982\n",
      "Epoch :0.9083333333333333    Train Loss :0.005086733028292656    Test Loss :0.017833463847637177\n",
      "Epoch :0.9166666666666666    Train Loss :0.004748658277094364    Test Loss :0.020376374945044518\n",
      "Epoch :0.925    Train Loss :0.004887768533080816    Test Loss :0.016330452635884285\n",
      "Epoch :0.9333333333333333    Train Loss :0.004786144942045212    Test Loss :0.020066792145371437\n",
      "Epoch :0.9416666666666667    Train Loss :0.005283532664179802    Test Loss :0.020489538088440895\n",
      "Epoch :0.95    Train Loss :0.004077978432178497    Test Loss :0.018106546252965927\n",
      "Epoch :0.9583333333333334    Train Loss :0.004768787417560816    Test Loss :0.036079101264476776\n",
      "Epoch :0.9666666666666667    Train Loss :0.005135393235832453    Test Loss :0.0288890078663826\n",
      "Epoch :0.975    Train Loss :0.00478492584079504    Test Loss :0.03759639337658882\n",
      "Epoch :0.9833333333333333    Train Loss :0.005568327382206917    Test Loss :0.01856839284300804\n",
      "Epoch :0.9916666666666667    Train Loss :0.005750901997089386    Test Loss :0.02623748406767845\n",
      "Epoch :1.0    Train Loss :0.004704163875430822    Test Loss :0.018809640780091286\n",
      "RMSE: 12.603278583148533\n",
      "MAE: 10.495436157944846\n",
      "MAPE: 8.972709818020649%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 50, 'lr': 0.01, 'num_layers': 5}\n",
      "total:  81.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.045720793306827545    Test Loss :0.13234981894493103\n",
      "Epoch :0.016666666666666666    Train Loss :0.013122087344527245    Test Loss :0.2217046022415161\n",
      "Epoch :0.025    Train Loss :0.019014785066246986    Test Loss :0.06953216344118118\n",
      "Epoch :0.03333333333333333    Train Loss :0.018549373373389244    Test Loss :0.05641050636768341\n",
      "Epoch :0.041666666666666664    Train Loss :0.012566027231514454    Test Loss :0.027528144419193268\n",
      "Epoch :0.05    Train Loss :0.007193032652139664    Test Loss :0.027856208384037018\n",
      "Epoch :0.058333333333333334    Train Loss :0.008342444896697998    Test Loss :0.027416573837399483\n",
      "Epoch :0.06666666666666667    Train Loss :0.007648075465112925    Test Loss :0.010510165244340897\n",
      "Epoch :0.075    Train Loss :0.006031678058207035    Test Loss :0.015161329880356789\n",
      "Epoch :0.08333333333333333    Train Loss :0.006132593844085932    Test Loss :0.015217349864542484\n",
      "Epoch :0.09166666666666666    Train Loss :0.006241543218493462    Test Loss :0.008622045628726482\n",
      "Epoch :0.1    Train Loss :0.007075605448335409    Test Loss :0.011507767252624035\n",
      "Epoch :0.10833333333333334    Train Loss :0.005414022132754326    Test Loss :0.01204737089574337\n",
      "Epoch :0.11666666666666667    Train Loss :0.005110520403832197    Test Loss :0.009522929787635803\n",
      "Epoch :0.125    Train Loss :0.005204278044402599    Test Loss :0.01239993330091238\n",
      "Epoch :0.13333333333333333    Train Loss :0.005335981026291847    Test Loss :0.013354064896702766\n",
      "Epoch :0.14166666666666666    Train Loss :0.005473391618579626    Test Loss :0.011730656959116459\n",
      "Epoch :0.15    Train Loss :0.005615660455077887    Test Loss :0.011785084381699562\n",
      "Epoch :0.15833333333333333    Train Loss :0.004351664800196886    Test Loss :0.006712385453283787\n",
      "Epoch :0.16666666666666666    Train Loss :0.005048627033829689    Test Loss :0.009820433333516121\n",
      "Epoch :0.175    Train Loss :0.0048789167776703835    Test Loss :0.007017407100647688\n",
      "Epoch :0.18333333333333332    Train Loss :0.004987902473658323    Test Loss :0.008777711540460587\n",
      "Epoch :0.19166666666666668    Train Loss :0.005144037306308746    Test Loss :0.005552912596613169\n",
      "Epoch :0.2    Train Loss :0.0036754868924617767    Test Loss :0.00984654575586319\n",
      "Epoch :0.20833333333333334    Train Loss :0.004178324714303017    Test Loss :0.008136816322803497\n",
      "Epoch :0.21666666666666667    Train Loss :0.005209203809499741    Test Loss :0.006800719536840916\n",
      "Epoch :0.225    Train Loss :0.0036519316490739584    Test Loss :0.008395642973482609\n",
      "Epoch :0.23333333333333334    Train Loss :0.003558685537427664    Test Loss :0.009706035256385803\n",
      "Epoch :0.24166666666666667    Train Loss :0.0042270636186003685    Test Loss :0.009948767721652985\n",
      "Epoch :0.25    Train Loss :0.004665340296924114    Test Loss :0.013591812923550606\n",
      "Epoch :0.25833333333333336    Train Loss :0.005049895960837603    Test Loss :0.010609125718474388\n",
      "Epoch :0.26666666666666666    Train Loss :0.004710300359874964    Test Loss :0.008375737816095352\n",
      "Epoch :0.275    Train Loss :0.003718205727636814    Test Loss :0.010847182944417\n",
      "Epoch :0.2833333333333333    Train Loss :0.004089795518666506    Test Loss :0.006897974759340286\n",
      "Epoch :0.2916666666666667    Train Loss :0.0034354471135884523    Test Loss :0.008199788630008698\n",
      "Epoch :0.3    Train Loss :0.003523036604747176    Test Loss :0.007802113890647888\n",
      "Epoch :0.30833333333333335    Train Loss :0.003981149755418301    Test Loss :0.0075926086865365505\n",
      "Epoch :0.31666666666666665    Train Loss :0.004324601497501135    Test Loss :0.009760715998709202\n",
      "Epoch :0.325    Train Loss :0.0046679358929395676    Test Loss :0.010256304405629635\n",
      "Epoch :0.3333333333333333    Train Loss :0.003184340661391616    Test Loss :0.006330028176307678\n",
      "Epoch :0.3416666666666667    Train Loss :0.004081943072378635    Test Loss :0.012517794966697693\n",
      "Epoch :0.35    Train Loss :0.0036407269071787596    Test Loss :0.005553069524466991\n",
      "Epoch :0.35833333333333334    Train Loss :0.003467586822807789    Test Loss :0.007500659208744764\n",
      "Epoch :0.36666666666666664    Train Loss :0.0031700225081294775    Test Loss :0.011163270100951195\n",
      "Epoch :0.375    Train Loss :0.0036110621877014637    Test Loss :0.008862325921654701\n",
      "Epoch :0.38333333333333336    Train Loss :0.0031242636032402515    Test Loss :0.009204288013279438\n",
      "Epoch :0.39166666666666666    Train Loss :0.003080552676692605    Test Loss :0.008225884288549423\n",
      "Epoch :0.4    Train Loss :0.003195479279384017    Test Loss :0.006610196083784103\n",
      "Epoch :0.4083333333333333    Train Loss :0.002777039771899581    Test Loss :0.0074093700386583805\n",
      "Epoch :0.4166666666666667    Train Loss :0.0036325433757156134    Test Loss :0.01042008027434349\n",
      "Epoch :0.425    Train Loss :0.0031012296676635742    Test Loss :0.008302919566631317\n",
      "Epoch :0.43333333333333335    Train Loss :0.0032046029809862375    Test Loss :0.009305180981755257\n",
      "Epoch :0.44166666666666665    Train Loss :0.0031823713798075914    Test Loss :0.01243679504841566\n",
      "Epoch :0.45    Train Loss :0.0035332408733665943    Test Loss :0.011245755478739738\n",
      "Epoch :0.4583333333333333    Train Loss :0.00263731530867517    Test Loss :0.012090807780623436\n",
      "Epoch :0.4666666666666667    Train Loss :0.003806178690865636    Test Loss :0.008672908879816532\n",
      "Epoch :0.475    Train Loss :0.0029750815592706203    Test Loss :0.008795628324151039\n",
      "Epoch :0.48333333333333334    Train Loss :0.0031986997928470373    Test Loss :0.01771489903330803\n",
      "Epoch :0.49166666666666664    Train Loss :0.00287102279253304    Test Loss :0.02989903837442398\n",
      "Epoch :0.5    Train Loss :0.0030196865554898977    Test Loss :0.01448769960552454\n",
      "Epoch :0.5083333333333333    Train Loss :0.004012514837086201    Test Loss :0.022774089127779007\n",
      "Epoch :0.5166666666666667    Train Loss :0.0030309665016829967    Test Loss :0.015649860724806786\n",
      "Epoch :0.525    Train Loss :0.003952587954699993    Test Loss :0.026834476739168167\n",
      "Epoch :0.5333333333333333    Train Loss :0.003910244442522526    Test Loss :0.009480074979364872\n",
      "Epoch :0.5416666666666666    Train Loss :0.0038251285441219807    Test Loss :0.0259312242269516\n",
      "Epoch :0.55    Train Loss :0.0028419422451406717    Test Loss :0.04492967575788498\n",
      "Epoch :0.5583333333333333    Train Loss :0.0031737564131617546    Test Loss :0.010787975043058395\n",
      "Epoch :0.5666666666666667    Train Loss :0.0041931988671422005    Test Loss :0.007910080254077911\n",
      "Epoch :0.575    Train Loss :0.0036232827696949244    Test Loss :0.007986551150679588\n",
      "Epoch :0.5833333333333334    Train Loss :0.003493656637147069    Test Loss :0.007444819435477257\n",
      "Epoch :0.5916666666666667    Train Loss :0.003598949871957302    Test Loss :0.0064744530245661736\n",
      "Epoch :0.6    Train Loss :0.0027638250030577183    Test Loss :0.007071956060826778\n",
      "Epoch :0.6083333333333333    Train Loss :0.003274178598076105    Test Loss :0.010714394971728325\n",
      "Epoch :0.6166666666666667    Train Loss :0.0036125320475548506    Test Loss :0.02246732823550701\n",
      "Epoch :0.625    Train Loss :0.003047371283173561    Test Loss :0.010554727166891098\n",
      "Epoch :0.6333333333333333    Train Loss :0.0029114580247551203    Test Loss :0.013870501890778542\n",
      "Epoch :0.6416666666666667    Train Loss :0.0038542053662240505    Test Loss :0.011722011491656303\n",
      "Epoch :0.65    Train Loss :0.003284985199570656    Test Loss :0.01275209803134203\n",
      "Epoch :0.6583333333333333    Train Loss :0.003368168603628874    Test Loss :0.007522359490394592\n",
      "Epoch :0.6666666666666666    Train Loss :0.0026839454658329487    Test Loss :0.00876566581428051\n",
      "Epoch :0.675    Train Loss :0.0027452462818473577    Test Loss :0.01923348940908909\n",
      "Epoch :0.6833333333333333    Train Loss :0.003389656310901046    Test Loss :0.010791721753776073\n",
      "Epoch :0.6916666666666667    Train Loss :0.003001932520419359    Test Loss :0.01826501078903675\n",
      "Epoch :0.7    Train Loss :0.002697642194107175    Test Loss :0.007509516552090645\n",
      "Epoch :0.7083333333333334    Train Loss :0.0027048927731812    Test Loss :0.02035905420780182\n",
      "Epoch :0.7166666666666667    Train Loss :0.0034133046865463257    Test Loss :0.04458761587738991\n",
      "Epoch :0.725    Train Loss :0.0030783391557633877    Test Loss :0.013931754045188427\n",
      "Epoch :0.7333333333333333    Train Loss :0.0027519213035702705    Test Loss :0.01635035127401352\n",
      "Epoch :0.7416666666666667    Train Loss :0.0033225449733436108    Test Loss :0.028089048340916634\n",
      "Epoch :0.75    Train Loss :0.0027531320229172707    Test Loss :0.009338031522929668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7583333333333333    Train Loss :0.0030945162288844585    Test Loss :0.00968550331890583\n",
      "Epoch :0.7666666666666667    Train Loss :0.0030445691663771868    Test Loss :0.012261531315743923\n",
      "Epoch :0.775    Train Loss :0.0031919681932777166    Test Loss :0.01382392831146717\n",
      "Epoch :0.7833333333333333    Train Loss :0.0026675958652049303    Test Loss :0.015821829438209534\n",
      "Epoch :0.7916666666666666    Train Loss :0.0028373838867992163    Test Loss :0.03613336756825447\n",
      "Epoch :0.8    Train Loss :0.002624430228024721    Test Loss :0.008367730304598808\n",
      "Epoch :0.8083333333333333    Train Loss :0.002773293061181903    Test Loss :0.01947299763560295\n",
      "Epoch :0.8166666666666667    Train Loss :0.002570668701082468    Test Loss :0.017728911712765694\n",
      "Epoch :0.825    Train Loss :0.0027642028871923685    Test Loss :0.0524350143969059\n",
      "Epoch :0.8333333333333334    Train Loss :0.0027343914844095707    Test Loss :0.013833263888955116\n",
      "Epoch :0.8416666666666667    Train Loss :0.0026872989255934954    Test Loss :0.06011523678898811\n",
      "Epoch :0.85    Train Loss :0.0032448892015963793    Test Loss :0.07834982126951218\n",
      "Epoch :0.8583333333333333    Train Loss :0.002868998795747757    Test Loss :0.008396869525313377\n",
      "Epoch :0.8666666666666667    Train Loss :0.0022042186465114355    Test Loss :0.022705065086483955\n",
      "Epoch :0.875    Train Loss :0.002747283549979329    Test Loss :0.026667358353734016\n",
      "Epoch :0.8833333333333333    Train Loss :0.002985761733725667    Test Loss :0.018495265394449234\n",
      "Epoch :0.8916666666666667    Train Loss :0.00258585624396801    Test Loss :0.019700314849615097\n",
      "Epoch :0.9    Train Loss :0.0020193373784422874    Test Loss :0.022431176155805588\n",
      "Epoch :0.9083333333333333    Train Loss :0.0028492826968431473    Test Loss :0.03003695048391819\n",
      "Epoch :0.9166666666666666    Train Loss :0.0022205093409866095    Test Loss :0.018363846465945244\n",
      "Epoch :0.925    Train Loss :0.002511437749490142    Test Loss :0.04500162601470947\n",
      "Epoch :0.9333333333333333    Train Loss :0.0024118630681186914    Test Loss :0.007875400595366955\n",
      "Epoch :0.9416666666666667    Train Loss :0.002892030868679285    Test Loss :0.021061118692159653\n",
      "Epoch :0.95    Train Loss :0.0024322709068655968    Test Loss :0.027602769434452057\n",
      "Epoch :0.9583333333333334    Train Loss :0.002307641552761197    Test Loss :0.04062585160136223\n",
      "Epoch :0.9666666666666667    Train Loss :0.002720464253798127    Test Loss :0.06988665461540222\n",
      "Epoch :0.975    Train Loss :0.001959541579708457    Test Loss :0.06891622394323349\n",
      "Epoch :0.9833333333333333    Train Loss :0.0017594343516975641    Test Loss :0.03177221864461899\n",
      "Epoch :0.9916666666666667    Train Loss :0.002222502138465643    Test Loss :0.015997502952814102\n",
      "Epoch :1.0    Train Loss :0.0022032605484128    Test Loss :0.026672935113310814\n",
      "RMSE: 22.197235853932064\n",
      "MAE: 18.002573291802555\n",
      "MAPE: 15.75916509261724%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 50, 'lr': 0.02, 'num_layers': 2}\n",
      "total:  84.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.06458167731761932    Test Loss :0.21261738240718842\n",
      "Epoch :0.016666666666666666    Train Loss :0.05058669298887253    Test Loss :0.14854219555854797\n",
      "Epoch :0.025    Train Loss :0.08473075181245804    Test Loss :0.34375035762786865\n",
      "Epoch :0.03333333333333333    Train Loss :0.05407167598605156    Test Loss :0.2556570768356323\n",
      "Epoch :0.041666666666666664    Train Loss :0.054623208940029144    Test Loss :0.2322634905576706\n",
      "Epoch :0.05    Train Loss :0.05299421399831772    Test Loss :0.2254904955625534\n",
      "Epoch :0.058333333333333334    Train Loss :0.052481699734926224    Test Loss :0.24554921686649323\n",
      "Epoch :0.06666666666666667    Train Loss :0.05389918014407158    Test Loss :0.2363663911819458\n",
      "Epoch :0.075    Train Loss :0.05255455523729324    Test Loss :0.2344856858253479\n",
      "Epoch :0.08333333333333333    Train Loss :0.05265262350440025    Test Loss :0.23358504474163055\n",
      "Epoch :0.09166666666666666    Train Loss :0.05177338793873787    Test Loss :0.2359694242477417\n",
      "Epoch :0.1    Train Loss :0.05108407884836197    Test Loss :0.23937799036502838\n",
      "Epoch :0.10833333333333334    Train Loss :0.05155283957719803    Test Loss :0.24270369112491608\n",
      "Epoch :0.11666666666666667    Train Loss :0.052385952323675156    Test Loss :0.23422561585903168\n",
      "Epoch :0.125    Train Loss :0.049906015396118164    Test Loss :0.23485080897808075\n",
      "Epoch :0.13333333333333333    Train Loss :0.05255470424890518    Test Loss :0.23098687827587128\n",
      "Epoch :0.14166666666666666    Train Loss :0.05210377648472786    Test Loss :0.23457027971744537\n",
      "Epoch :0.15    Train Loss :0.052625250071287155    Test Loss :0.23694351315498352\n",
      "Epoch :0.15833333333333333    Train Loss :0.05213218554854393    Test Loss :0.2340681105852127\n",
      "Epoch :0.16666666666666666    Train Loss :0.05069638788700104    Test Loss :0.23551887273788452\n",
      "Epoch :0.175    Train Loss :0.052094411104917526    Test Loss :0.2346995323896408\n",
      "Epoch :0.18333333333333332    Train Loss :0.051284048706293106    Test Loss :0.2299375981092453\n",
      "Epoch :0.19166666666666668    Train Loss :0.05135481059551239    Test Loss :0.2399701029062271\n",
      "Epoch :0.2    Train Loss :0.05104463919997215    Test Loss :0.23168796300888062\n",
      "Epoch :0.20833333333333334    Train Loss :0.052048034965991974    Test Loss :0.23412597179412842\n",
      "Epoch :0.21666666666666667    Train Loss :0.05306148901581764    Test Loss :0.2364031970500946\n",
      "Epoch :0.225    Train Loss :0.05169191211462021    Test Loss :0.23180720210075378\n",
      "Epoch :0.23333333333333334    Train Loss :0.05219077318906784    Test Loss :0.2343568652868271\n",
      "Epoch :0.24166666666666667    Train Loss :0.05114370957016945    Test Loss :0.23275090754032135\n",
      "Epoch :0.25    Train Loss :0.051284849643707275    Test Loss :0.23253822326660156\n",
      "Epoch :0.25833333333333336    Train Loss :0.05125727131962776    Test Loss :0.23644700646400452\n",
      "Epoch :0.26666666666666666    Train Loss :0.05223225802183151    Test Loss :0.233684241771698\n",
      "Epoch :0.275    Train Loss :0.052202414721250534    Test Loss :0.23481236398220062\n",
      "Epoch :0.2833333333333333    Train Loss :0.051255982369184494    Test Loss :0.2340388298034668\n",
      "Epoch :0.2916666666666667    Train Loss :0.05207851901650429    Test Loss :0.23520036041736603\n",
      "Epoch :0.3    Train Loss :0.05137215182185173    Test Loss :0.2343226671218872\n",
      "Epoch :0.30833333333333335    Train Loss :0.05232113599777222    Test Loss :0.2282700538635254\n",
      "Epoch :0.31666666666666665    Train Loss :0.0509280301630497    Test Loss :0.237714022397995\n",
      "Epoch :0.325    Train Loss :0.052127640694379807    Test Loss :0.23411332070827484\n",
      "Epoch :0.3333333333333333    Train Loss :0.05171487107872963    Test Loss :0.2308562695980072\n",
      "Epoch :0.3416666666666667    Train Loss :0.05240405723452568    Test Loss :0.2358194887638092\n",
      "Epoch :0.35    Train Loss :0.051239900290966034    Test Loss :0.2383195012807846\n",
      "Epoch :0.35833333333333334    Train Loss :0.05131886899471283    Test Loss :0.23608188331127167\n",
      "Epoch :0.36666666666666664    Train Loss :0.05133581906557083    Test Loss :0.23878535628318787\n",
      "Epoch :0.375    Train Loss :0.052095748484134674    Test Loss :0.23348625004291534\n",
      "Epoch :0.38333333333333336    Train Loss :0.05162652209401131    Test Loss :0.23385439813137054\n",
      "Epoch :0.39166666666666666    Train Loss :0.051528558135032654    Test Loss :0.23521289229393005\n",
      "Epoch :0.4    Train Loss :0.05148855969309807    Test Loss :0.2317458689212799\n",
      "Epoch :0.4083333333333333    Train Loss :0.05207754671573639    Test Loss :0.23198586702346802\n",
      "Epoch :0.4166666666666667    Train Loss :0.05204438790678978    Test Loss :0.23848649859428406\n",
      "Epoch :0.425    Train Loss :0.05153719335794449    Test Loss :0.23204949498176575\n",
      "Epoch :0.43333333333333335    Train Loss :0.052021898329257965    Test Loss :0.23758992552757263\n",
      "Epoch :0.44166666666666665    Train Loss :0.05085517838597298    Test Loss :0.23766645789146423\n",
      "Epoch :0.45    Train Loss :0.05126124247908592    Test Loss :0.23342439532279968\n",
      "Epoch :0.4583333333333333    Train Loss :0.052462249994277954    Test Loss :0.23623350262641907\n",
      "Epoch :0.4666666666666667    Train Loss :0.051055505871772766    Test Loss :0.23485717177391052\n",
      "Epoch :0.475    Train Loss :0.05081074684858322    Test Loss :0.23624670505523682\n",
      "Epoch :0.48333333333333334    Train Loss :0.05211557075381279    Test Loss :0.23190143704414368\n",
      "Epoch :0.49166666666666664    Train Loss :0.05207892879843712    Test Loss :0.23392219841480255\n",
      "Epoch :0.5    Train Loss :0.051339756697416306    Test Loss :0.23288558423519135\n",
      "Epoch :0.5083333333333333    Train Loss :0.05140779912471771    Test Loss :0.22992393374443054\n",
      "Epoch :0.5166666666666667    Train Loss :0.05217990651726723    Test Loss :0.23398858308792114\n",
      "Epoch :0.525    Train Loss :0.051842398941516876    Test Loss :0.23508338630199432\n",
      "Epoch :0.5333333333333333    Train Loss :0.05178581923246384    Test Loss :0.23331372439861298\n",
      "Epoch :0.5416666666666666    Train Loss :0.05188463628292084    Test Loss :0.2344299852848053\n",
      "Epoch :0.55    Train Loss :0.052176009863615036    Test Loss :0.23813201487064362\n",
      "Epoch :0.5583333333333333    Train Loss :0.051891524344682693    Test Loss :0.23483359813690186\n",
      "Epoch :0.5666666666666667    Train Loss :0.05147365480661392    Test Loss :0.23366804420948029\n",
      "Epoch :0.575    Train Loss :0.05155880004167557    Test Loss :0.23643171787261963\n",
      "Epoch :0.5833333333333334    Train Loss :0.051542676985263824    Test Loss :0.23375503718852997\n",
      "Epoch :0.5916666666666667    Train Loss :0.05168537050485611    Test Loss :0.23276713490486145\n",
      "Epoch :0.6    Train Loss :0.05170547589659691    Test Loss :0.23210221529006958\n",
      "Epoch :0.6083333333333333    Train Loss :0.05166221782565117    Test Loss :0.233873188495636\n",
      "Epoch :0.6166666666666667    Train Loss :0.051850877702236176    Test Loss :0.23549452424049377\n",
      "Epoch :0.625    Train Loss :0.051459215581417084    Test Loss :0.23378731310367584\n",
      "Epoch :0.6333333333333333    Train Loss :0.0518103688955307    Test Loss :0.23451299965381622\n",
      "Epoch :0.6416666666666667    Train Loss :0.051447343081235886    Test Loss :0.23539689183235168\n",
      "Epoch :0.65    Train Loss :0.051817938685417175    Test Loss :0.2343052625656128\n",
      "Epoch :0.6583333333333333    Train Loss :0.05178699642419815    Test Loss :0.23279039561748505\n",
      "Epoch :0.6666666666666666    Train Loss :0.05161995813250542    Test Loss :0.23334988951683044\n",
      "Epoch :0.675    Train Loss :0.05163709819316864    Test Loss :0.2331322878599167\n",
      "Epoch :0.6833333333333333    Train Loss :0.051402222365140915    Test Loss :0.2326097935438156\n",
      "Epoch :0.6916666666666667    Train Loss :0.051804959774017334    Test Loss :0.23288917541503906\n",
      "Epoch :0.7    Train Loss :0.05188698321580887    Test Loss :0.23339435458183289\n",
      "Epoch :0.7083333333333334    Train Loss :0.05113696679472923    Test Loss :0.2317529171705246\n",
      "Epoch :0.7166666666666667    Train Loss :0.05148787423968315    Test Loss :0.23235481977462769\n",
      "Epoch :0.725    Train Loss :0.05176663398742676    Test Loss :0.23571892082691193\n",
      "Epoch :0.7333333333333333    Train Loss :0.05148826912045479    Test Loss :0.23472937941551208\n",
      "Epoch :0.7416666666666667    Train Loss :0.05167728662490845    Test Loss :0.23535822331905365\n",
      "Epoch :0.75    Train Loss :0.051606833934783936    Test Loss :0.23350602388381958\n",
      "Epoch :0.7583333333333333    Train Loss :0.051454778760671616    Test Loss :0.23498712480068207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7666666666666667    Train Loss :0.05150885134935379    Test Loss :0.23367996513843536\n",
      "Epoch :0.775    Train Loss :0.051485825330019    Test Loss :0.2339114248752594\n",
      "Epoch :0.7833333333333333    Train Loss :0.05091679468750954    Test Loss :0.23466739058494568\n",
      "Epoch :0.7916666666666666    Train Loss :0.0517711341381073    Test Loss :0.23527514934539795\n",
      "Epoch :0.8    Train Loss :0.051719892770051956    Test Loss :0.2346811592578888\n",
      "Epoch :0.8083333333333333    Train Loss :0.05149883031845093    Test Loss :0.2353527545928955\n",
      "Epoch :0.8166666666666667    Train Loss :0.051632486283779144    Test Loss :0.23460233211517334\n",
      "Epoch :0.825    Train Loss :0.051548782736063004    Test Loss :0.23318669199943542\n",
      "Epoch :0.8333333333333334    Train Loss :0.051502447575330734    Test Loss :0.23416301608085632\n",
      "Epoch :0.8416666666666667    Train Loss :0.051673609763383865    Test Loss :0.2340133935213089\n",
      "Epoch :0.85    Train Loss :0.051578063517808914    Test Loss :0.2335648089647293\n",
      "Epoch :0.8583333333333333    Train Loss :0.05170639604330063    Test Loss :0.23262321949005127\n",
      "Epoch :0.8666666666666667    Train Loss :0.05163029208779335    Test Loss :0.2332175374031067\n",
      "Epoch :0.875    Train Loss :0.05174042657017708    Test Loss :0.2347017377614975\n",
      "Epoch :0.8833333333333333    Train Loss :0.051606375724077225    Test Loss :0.23315441608428955\n",
      "Epoch :0.8916666666666667    Train Loss :0.051570549607276917    Test Loss :0.23315371572971344\n",
      "Epoch :0.9    Train Loss :0.051598478108644485    Test Loss :0.23502373695373535\n",
      "Epoch :0.9083333333333333    Train Loss :0.051479730755090714    Test Loss :0.23365874588489532\n",
      "Epoch :0.9166666666666666    Train Loss :0.05168978124856949    Test Loss :0.23278740048408508\n",
      "Epoch :0.925    Train Loss :0.05164768919348717    Test Loss :0.23449786007404327\n",
      "Epoch :0.9333333333333333    Train Loss :0.051460109651088715    Test Loss :0.23451274633407593\n",
      "Epoch :0.9416666666666667    Train Loss :0.05154259130358696    Test Loss :0.23286303877830505\n",
      "Epoch :0.95    Train Loss :0.051764849573373795    Test Loss :0.233198344707489\n",
      "Epoch :0.9583333333333334    Train Loss :0.051785048097372055    Test Loss :0.23088952898979187\n",
      "Epoch :0.9666666666666667    Train Loss :0.051581405103206635    Test Loss :0.2307383418083191\n",
      "Epoch :0.975    Train Loss :0.051655109971761703    Test Loss :0.23362071812152863\n",
      "Epoch :0.9833333333333333    Train Loss :0.05166285112500191    Test Loss :0.23526734113693237\n",
      "Epoch :0.9916666666666667    Train Loss :0.051331810653209686    Test Loss :0.23414668440818787\n",
      "Epoch :1.0    Train Loss :0.05192989110946655    Test Loss :0.23505713045597076\n",
      "RMSE: 43.986179217795154\n",
      "MAE: 43.421593903947084\n",
      "MAPE: 38.268741253545926%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 50, 'lr': 0.02, 'num_layers': 5}\n",
      "total:  88.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.06413332372903824    Test Loss :0.3131193220615387\n",
      "Epoch :0.016666666666666666    Train Loss :0.04324502870440483    Test Loss :0.06342750042676926\n",
      "Epoch :0.025    Train Loss :0.03767738863825798    Test Loss :0.035310037434101105\n",
      "Epoch :0.03333333333333333    Train Loss :0.015828821808099747    Test Loss :0.011481909081339836\n",
      "Epoch :0.041666666666666664    Train Loss :0.013399885967373848    Test Loss :0.025657128542661667\n",
      "Epoch :0.05    Train Loss :0.009412627667188644    Test Loss :0.02017877995967865\n",
      "Epoch :0.058333333333333334    Train Loss :0.009055482223629951    Test Loss :0.036183472722768784\n",
      "Epoch :0.06666666666666667    Train Loss :0.007917487993836403    Test Loss :0.008856533095240593\n",
      "Epoch :0.075    Train Loss :0.008487668819725513    Test Loss :0.013479097746312618\n",
      "Epoch :0.08333333333333333    Train Loss :0.00743871508166194    Test Loss :0.011465948075056076\n",
      "Epoch :0.09166666666666666    Train Loss :0.007034098729491234    Test Loss :0.007485629059374332\n",
      "Epoch :0.1    Train Loss :0.006152263376861811    Test Loss :0.010367780923843384\n",
      "Epoch :0.10833333333333334    Train Loss :0.005596432369202375    Test Loss :0.009122378192842007\n",
      "Epoch :0.11666666666666667    Train Loss :0.005525742191821337    Test Loss :0.00827602669596672\n",
      "Epoch :0.125    Train Loss :0.0054479558020830154    Test Loss :0.009148926474153996\n",
      "Epoch :0.13333333333333333    Train Loss :0.0057988762855529785    Test Loss :0.009366472251713276\n",
      "Epoch :0.14166666666666666    Train Loss :0.004931681323796511    Test Loss :0.008966237306594849\n",
      "Epoch :0.15    Train Loss :0.004904347471892834    Test Loss :0.008711704984307289\n",
      "Epoch :0.15833333333333333    Train Loss :0.005218781065195799    Test Loss :0.016325710341334343\n",
      "Epoch :0.16666666666666666    Train Loss :0.005243587773293257    Test Loss :0.01489011850208044\n",
      "Epoch :0.175    Train Loss :0.004521865397691727    Test Loss :0.011432469822466373\n",
      "Epoch :0.18333333333333332    Train Loss :0.004728733561933041    Test Loss :0.01310222502797842\n",
      "Epoch :0.19166666666666668    Train Loss :0.004347698297351599    Test Loss :0.015040013939142227\n",
      "Epoch :0.2    Train Loss :0.004333477467298508    Test Loss :0.01516625378280878\n",
      "Epoch :0.20833333333333334    Train Loss :0.004019115585833788    Test Loss :0.022897768765687943\n",
      "Epoch :0.21666666666666667    Train Loss :0.004269715398550034    Test Loss :0.01899724081158638\n",
      "Epoch :0.225    Train Loss :0.004249237477779388    Test Loss :0.01572391390800476\n",
      "Epoch :0.23333333333333334    Train Loss :0.004406956490129232    Test Loss :0.014246196486055851\n",
      "Epoch :0.24166666666666667    Train Loss :0.004062717314809561    Test Loss :0.015123797580599785\n",
      "Epoch :0.25    Train Loss :0.003242038656026125    Test Loss :0.016222544014453888\n",
      "Epoch :0.25833333333333336    Train Loss :0.003022567369043827    Test Loss :0.015505003742873669\n",
      "Epoch :0.26666666666666666    Train Loss :0.004108560737222433    Test Loss :0.026637447997927666\n",
      "Epoch :0.275    Train Loss :0.0035963125992566347    Test Loss :0.03583083301782608\n",
      "Epoch :0.2833333333333333    Train Loss :0.00419085007160902    Test Loss :0.025644998997449875\n",
      "Epoch :0.2916666666666667    Train Loss :0.0037523875944316387    Test Loss :0.013410027138888836\n",
      "Epoch :0.3    Train Loss :0.0027369698509573936    Test Loss :0.018825234845280647\n",
      "Epoch :0.30833333333333335    Train Loss :0.00321578118018806    Test Loss :0.012712417170405388\n",
      "Epoch :0.31666666666666665    Train Loss :0.0033414552453905344    Test Loss :0.008467169478535652\n",
      "Epoch :0.325    Train Loss :0.0032917868811637163    Test Loss :0.023671582341194153\n",
      "Epoch :0.3333333333333333    Train Loss :0.0038563560228794813    Test Loss :0.01647014170885086\n",
      "Epoch :0.3416666666666667    Train Loss :0.0025370814837515354    Test Loss :0.007595292292535305\n",
      "Epoch :0.35    Train Loss :0.002492602216079831    Test Loss :0.024347582831978798\n",
      "Epoch :0.35833333333333334    Train Loss :0.00333800888620317    Test Loss :0.01612663082778454\n",
      "Epoch :0.36666666666666664    Train Loss :0.0024571858812123537    Test Loss :0.018798889592289925\n",
      "Epoch :0.375    Train Loss :0.0035247551277279854    Test Loss :0.02108541689813137\n",
      "Epoch :0.38333333333333336    Train Loss :0.002936946926638484    Test Loss :0.024649741128087044\n",
      "Epoch :0.39166666666666666    Train Loss :0.0027009083423763514    Test Loss :0.03274960070848465\n",
      "Epoch :0.4    Train Loss :0.00274828658439219    Test Loss :0.030074337497353554\n",
      "Epoch :0.4083333333333333    Train Loss :0.0036605934146791697    Test Loss :0.020827773958444595\n",
      "Epoch :0.4166666666666667    Train Loss :0.0031270193867385387    Test Loss :0.011558049358427525\n",
      "Epoch :0.425    Train Loss :0.0034413740504533052    Test Loss :0.016930198296904564\n",
      "Epoch :0.43333333333333335    Train Loss :0.0029410014394670725    Test Loss :0.01994824782013893\n",
      "Epoch :0.44166666666666665    Train Loss :0.0031066727824509144    Test Loss :0.021681372076272964\n",
      "Epoch :0.45    Train Loss :0.0028470437973737717    Test Loss :0.025102905929088593\n",
      "Epoch :0.4583333333333333    Train Loss :0.003095677588135004    Test Loss :0.01326037384569645\n",
      "Epoch :0.4666666666666667    Train Loss :0.003761250525712967    Test Loss :0.018128855153918266\n",
      "Epoch :0.475    Train Loss :0.003640434704720974    Test Loss :0.0183713398873806\n",
      "Epoch :0.48333333333333334    Train Loss :0.00351535901427269    Test Loss :0.019674012437462807\n",
      "Epoch :0.49166666666666664    Train Loss :0.0029032311867922544    Test Loss :0.010903880000114441\n",
      "Epoch :0.5    Train Loss :0.003161961678415537    Test Loss :0.02410299703478813\n",
      "Epoch :0.5083333333333333    Train Loss :0.0028393890243023634    Test Loss :0.011368947103619576\n",
      "Epoch :0.5166666666666667    Train Loss :0.003784377360716462    Test Loss :0.021436305716633797\n",
      "Epoch :0.525    Train Loss :0.003257148899137974    Test Loss :0.017270274460315704\n",
      "Epoch :0.5333333333333333    Train Loss :0.00269910367205739    Test Loss :0.024316489696502686\n",
      "Epoch :0.5416666666666666    Train Loss :0.0024431459605693817    Test Loss :0.021299678832292557\n",
      "Epoch :0.55    Train Loss :0.0030001101549714804    Test Loss :0.014646071009337902\n",
      "Epoch :0.5583333333333333    Train Loss :0.003005027072504163    Test Loss :0.023839034140110016\n",
      "Epoch :0.5666666666666667    Train Loss :0.0027651828713715076    Test Loss :0.023930024355649948\n",
      "Epoch :0.575    Train Loss :0.002932218136265874    Test Loss :0.0261479914188385\n",
      "Epoch :0.5833333333333334    Train Loss :0.002619536593556404    Test Loss :0.018936265259981155\n",
      "Epoch :0.5916666666666667    Train Loss :0.002726925304159522    Test Loss :0.021507352590560913\n",
      "Epoch :0.6    Train Loss :0.002667353255674243    Test Loss :0.014889195561408997\n",
      "Epoch :0.6083333333333333    Train Loss :0.0028936618473380804    Test Loss :0.014761708676815033\n",
      "Epoch :0.6166666666666667    Train Loss :0.00280605792067945    Test Loss :0.018234197050333023\n",
      "Epoch :0.625    Train Loss :0.002788164187222719    Test Loss :0.019401559606194496\n",
      "Epoch :0.6333333333333333    Train Loss :0.0031511627603322268    Test Loss :0.016795219853520393\n",
      "Epoch :0.6416666666666667    Train Loss :0.002629352966323495    Test Loss :0.01686922274529934\n",
      "Epoch :0.65    Train Loss :0.002967194188386202    Test Loss :0.017778780311346054\n",
      "Epoch :0.6583333333333333    Train Loss :0.0029837379697710276    Test Loss :0.022868527099490166\n",
      "Epoch :0.6666666666666666    Train Loss :0.002255768049508333    Test Loss :0.030637672170996666\n",
      "Epoch :0.675    Train Loss :0.0023296878207474947    Test Loss :0.02206832356750965\n",
      "Epoch :0.6833333333333333    Train Loss :0.002594521502032876    Test Loss :0.019014742225408554\n",
      "Epoch :0.6916666666666667    Train Loss :0.002964176470413804    Test Loss :0.012453623116016388\n",
      "Epoch :0.7    Train Loss :0.0029183693695813417    Test Loss :0.024193724617362022\n",
      "Epoch :0.7083333333333334    Train Loss :0.0034469927195459604    Test Loss :0.03113321214914322\n",
      "Epoch :0.7166666666666667    Train Loss :0.003113220212981105    Test Loss :0.013574722222983837\n",
      "Epoch :0.725    Train Loss :0.0029269875958561897    Test Loss :0.019107352942228317\n",
      "Epoch :0.7333333333333333    Train Loss :0.0030762574169784784    Test Loss :0.015499318018555641\n",
      "Epoch :0.7416666666666667    Train Loss :0.002368381479755044    Test Loss :0.016707461327314377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.75    Train Loss :0.0025781288277357817    Test Loss :0.01700272597372532\n",
      "Epoch :0.7583333333333333    Train Loss :0.002712597604840994    Test Loss :0.02741149812936783\n",
      "Epoch :0.7666666666666667    Train Loss :0.0026037616189569235    Test Loss :0.020611725747585297\n",
      "Epoch :0.775    Train Loss :0.0020160216372460127    Test Loss :0.017486775293946266\n",
      "Epoch :0.7833333333333333    Train Loss :0.003059238428249955    Test Loss :0.021069200709462166\n",
      "Epoch :0.7916666666666666    Train Loss :0.002407487016171217    Test Loss :0.028676673769950867\n",
      "Epoch :0.8    Train Loss :0.0027668834663927555    Test Loss :0.015977878123521805\n",
      "Epoch :0.8083333333333333    Train Loss :0.0021584921050816774    Test Loss :0.02464061975479126\n",
      "Epoch :0.8166666666666667    Train Loss :0.0031609325669705868    Test Loss :0.031078781932592392\n",
      "Epoch :0.825    Train Loss :0.002818745095282793    Test Loss :0.021448129788041115\n",
      "Epoch :0.8333333333333334    Train Loss :0.002692878246307373    Test Loss :0.029743699356913567\n",
      "Epoch :0.8416666666666667    Train Loss :0.0023641157895326614    Test Loss :0.025826411321759224\n",
      "Epoch :0.85    Train Loss :0.002846023067831993    Test Loss :0.01600555330514908\n",
      "Epoch :0.8583333333333333    Train Loss :0.00327727640978992    Test Loss :0.01609555259346962\n",
      "Epoch :0.8666666666666667    Train Loss :0.0026259205769747496    Test Loss :0.016093403100967407\n",
      "Epoch :0.875    Train Loss :0.0024310932494699955    Test Loss :0.018899599090218544\n",
      "Epoch :0.8833333333333333    Train Loss :0.0029206709004938602    Test Loss :0.016221720725297928\n",
      "Epoch :0.8916666666666667    Train Loss :0.0021209490951150656    Test Loss :0.02665785700082779\n",
      "Epoch :0.9    Train Loss :0.0028585223481059074    Test Loss :0.025253901258111\n",
      "Epoch :0.9083333333333333    Train Loss :0.002685003913938999    Test Loss :0.024352705106139183\n",
      "Epoch :0.9166666666666666    Train Loss :0.002827183110639453    Test Loss :0.020241061225533485\n",
      "Epoch :0.925    Train Loss :0.002551328856498003    Test Loss :0.026186665520071983\n",
      "Epoch :0.9333333333333333    Train Loss :0.0025161560624837875    Test Loss :0.019719336181879044\n",
      "Epoch :0.9416666666666667    Train Loss :0.002295290119946003    Test Loss :0.018911005929112434\n",
      "Epoch :0.95    Train Loss :0.002495723543688655    Test Loss :0.026242459192872047\n",
      "Epoch :0.9583333333333334    Train Loss :0.002346825087442994    Test Loss :0.017414873465895653\n",
      "Epoch :0.9666666666666667    Train Loss :0.002481234259903431    Test Loss :0.015291539020836353\n",
      "Epoch :0.975    Train Loss :0.002403320511803031    Test Loss :0.020274750888347626\n",
      "Epoch :0.9833333333333333    Train Loss :0.00203437521122396    Test Loss :0.02224520407617092\n",
      "Epoch :0.9916666666666667    Train Loss :0.0027894682716578245    Test Loss :0.018663369119167328\n",
      "Epoch :1.0    Train Loss :0.0023278952576220036    Test Loss :0.017050746828317642\n",
      "RMSE: 14.694395025490495\n",
      "MAE: 11.905666698390283\n",
      "MAPE: 10.275989117608756%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 100, 'lr': 0.01, 'num_layers': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  91.0\n",
      "Epoch :0.008333333333333333    Train Loss :0.05479058995842934    Test Loss :0.1855284422636032\n",
      "Epoch :0.016666666666666666    Train Loss :0.05179095268249512    Test Loss :0.25345125794410706\n",
      "Epoch :0.025    Train Loss :0.0519673228263855    Test Loss :0.2170853316783905\n",
      "Epoch :0.03333333333333333    Train Loss :0.04385828971862793    Test Loss :0.1185506209731102\n",
      "Epoch :0.041666666666666664    Train Loss :0.027976099401712418    Test Loss :0.06833351403474808\n",
      "Epoch :0.05    Train Loss :0.021324044093489647    Test Loss :0.046021830290555954\n",
      "Epoch :0.058333333333333334    Train Loss :0.018172405660152435    Test Loss :0.04055104777216911\n",
      "Epoch :0.06666666666666667    Train Loss :0.015507949516177177    Test Loss :0.056098245084285736\n",
      "Epoch :0.075    Train Loss :0.009643159806728363    Test Loss :0.008641913533210754\n",
      "Epoch :0.08333333333333333    Train Loss :0.009929297491908073    Test Loss :0.023256925866007805\n",
      "Epoch :0.09166666666666666    Train Loss :0.00948866456747055    Test Loss :0.02810230851173401\n",
      "Epoch :0.1    Train Loss :0.0084867924451828    Test Loss :0.011123471893370152\n",
      "Epoch :0.10833333333333334    Train Loss :0.008788937702775002    Test Loss :0.01307800505310297\n",
      "Epoch :0.11666666666666667    Train Loss :0.008252897299826145    Test Loss :0.024824991822242737\n",
      "Epoch :0.125    Train Loss :0.007400233298540115    Test Loss :0.03133155405521393\n",
      "Epoch :0.13333333333333333    Train Loss :0.006785665638744831    Test Loss :0.013051515445113182\n",
      "Epoch :0.14166666666666666    Train Loss :0.00714827049523592    Test Loss :0.019941437989473343\n",
      "Epoch :0.15    Train Loss :0.007967721670866013    Test Loss :0.017776500433683395\n",
      "Epoch :0.15833333333333333    Train Loss :0.008519411087036133    Test Loss :0.017001092433929443\n",
      "Epoch :0.16666666666666666    Train Loss :0.007134078536182642    Test Loss :0.02017209678888321\n",
      "Epoch :0.175    Train Loss :0.0068495552986860275    Test Loss :0.01606837846338749\n",
      "Epoch :0.18333333333333332    Train Loss :0.007673997897654772    Test Loss :0.017658330500125885\n",
      "Epoch :0.19166666666666668    Train Loss :0.008560167625546455    Test Loss :0.015333184041082859\n",
      "Epoch :0.2    Train Loss :0.006991783156991005    Test Loss :0.01617458648979664\n",
      "Epoch :0.20833333333333334    Train Loss :0.007048909552395344    Test Loss :0.017985474318265915\n",
      "Epoch :0.21666666666666667    Train Loss :0.006165813188999891    Test Loss :0.016604991629719734\n",
      "Epoch :0.225    Train Loss :0.006077245343476534    Test Loss :0.0164017416536808\n",
      "Epoch :0.23333333333333334    Train Loss :0.0065338704735040665    Test Loss :0.018011407926678658\n",
      "Epoch :0.24166666666666667    Train Loss :0.0066701145842671394    Test Loss :0.016765279695391655\n",
      "Epoch :0.25    Train Loss :0.005938062444329262    Test Loss :0.014133676886558533\n",
      "Epoch :0.25833333333333336    Train Loss :0.0059763966128230095    Test Loss :0.013964686542749405\n",
      "Epoch :0.26666666666666666    Train Loss :0.0062443530187010765    Test Loss :0.018996233120560646\n",
      "Epoch :0.275    Train Loss :0.00613046670332551    Test Loss :0.018198195844888687\n",
      "Epoch :0.2833333333333333    Train Loss :0.005793299525976181    Test Loss :0.015362489968538284\n",
      "Epoch :0.2916666666666667    Train Loss :0.005484109278768301    Test Loss :0.012273467145860195\n",
      "Epoch :0.3    Train Loss :0.00598169956356287    Test Loss :0.020819757133722305\n",
      "Epoch :0.30833333333333335    Train Loss :0.0059191761538386345    Test Loss :0.022044867277145386\n",
      "Epoch :0.31666666666666665    Train Loss :0.005277711898088455    Test Loss :0.014866090379655361\n",
      "Epoch :0.325    Train Loss :0.005941071081906557    Test Loss :0.0129972193390131\n",
      "Epoch :0.3333333333333333    Train Loss :0.005976674146950245    Test Loss :0.01709720306098461\n",
      "Epoch :0.3416666666666667    Train Loss :0.005518308840692043    Test Loss :0.01234242133796215\n",
      "Epoch :0.35    Train Loss :0.004843000788241625    Test Loss :0.008494618348777294\n",
      "Epoch :0.35833333333333334    Train Loss :0.005465964321047068    Test Loss :0.010770091786980629\n",
      "Epoch :0.36666666666666664    Train Loss :0.004828955978155136    Test Loss :0.01946733519434929\n",
      "Epoch :0.375    Train Loss :0.006065494380891323    Test Loss :0.009471669793128967\n",
      "Epoch :0.38333333333333336    Train Loss :0.006533219013363123    Test Loss :0.015596688725054264\n",
      "Epoch :0.39166666666666666    Train Loss :0.004444177728146315    Test Loss :0.012627151794731617\n",
      "Epoch :0.4    Train Loss :0.004812619648873806    Test Loss :0.010535471141338348\n",
      "Epoch :0.4083333333333333    Train Loss :0.005693335551768541    Test Loss :0.015266580507159233\n",
      "Epoch :0.4166666666666667    Train Loss :0.005048925522714853    Test Loss :0.012492754496634007\n",
      "Epoch :0.425    Train Loss :0.005577079951763153    Test Loss :0.010239770635962486\n",
      "Epoch :0.43333333333333335    Train Loss :0.004539851564913988    Test Loss :0.014047525823116302\n",
      "Epoch :0.44166666666666665    Train Loss :0.004290932789444923    Test Loss :0.01787860319018364\n",
      "Epoch :0.45    Train Loss :0.005215898621827364    Test Loss :0.011603008024394512\n",
      "Epoch :0.4583333333333333    Train Loss :0.0054060425609350204    Test Loss :0.008862181566655636\n",
      "Epoch :0.4666666666666667    Train Loss :0.005445260088890791    Test Loss :0.02140207029879093\n",
      "Epoch :0.475    Train Loss :0.005514942575246096    Test Loss :0.013399221003055573\n",
      "Epoch :0.48333333333333334    Train Loss :0.00485688541084528    Test Loss :0.010689638555049896\n",
      "Epoch :0.49166666666666664    Train Loss :0.0040689874440431595    Test Loss :0.01743297465145588\n",
      "Epoch :0.5    Train Loss :0.0046239811927080154    Test Loss :0.015282920561730862\n",
      "Epoch :0.5083333333333333    Train Loss :0.004190857522189617    Test Loss :0.018976999446749687\n",
      "Epoch :0.5166666666666667    Train Loss :0.004662642255425453    Test Loss :0.01701081544160843\n",
      "Epoch :0.525    Train Loss :0.004134158603847027    Test Loss :0.014785067178308964\n",
      "Epoch :0.5333333333333333    Train Loss :0.004001645836979151    Test Loss :0.012846947647631168\n",
      "Epoch :0.5416666666666666    Train Loss :0.004365750588476658    Test Loss :0.022245990112423897\n",
      "Epoch :0.55    Train Loss :0.004368980415165424    Test Loss :0.020035626366734505\n",
      "Epoch :0.5583333333333333    Train Loss :0.004653886891901493    Test Loss :0.01806224137544632\n",
      "Epoch :0.5666666666666667    Train Loss :0.004141692537814379    Test Loss :0.016123274341225624\n",
      "Epoch :0.575    Train Loss :0.004274910315871239    Test Loss :0.017157698050141335\n",
      "Epoch :0.5833333333333334    Train Loss :0.004013605881482363    Test Loss :0.011285546235740185\n",
      "Epoch :0.5916666666666667    Train Loss :0.0038969023153185844    Test Loss :0.013034990057349205\n",
      "Epoch :0.6    Train Loss :0.004824475385248661    Test Loss :0.01648135297000408\n",
      "Epoch :0.6083333333333333    Train Loss :0.004610813222825527    Test Loss :0.008172677829861641\n",
      "Epoch :0.6166666666666667    Train Loss :0.004543632734566927    Test Loss :0.012012687511742115\n",
      "Epoch :0.625    Train Loss :0.004183418117463589    Test Loss :0.019630856812000275\n",
      "Epoch :0.6333333333333333    Train Loss :0.003157601458951831    Test Loss :0.014224426820874214\n",
      "Epoch :0.6416666666666667    Train Loss :0.004216376692056656    Test Loss :0.03554574027657509\n",
      "Epoch :0.65    Train Loss :0.003360479138791561    Test Loss :0.013391646556556225\n",
      "Epoch :0.6583333333333333    Train Loss :0.004061571788042784    Test Loss :0.021354271098971367\n",
      "Epoch :0.6666666666666666    Train Loss :0.0043952809646725655    Test Loss :0.02853996306657791\n",
      "Epoch :0.675    Train Loss :0.004604583606123924    Test Loss :0.02615077793598175\n",
      "Epoch :0.6833333333333333    Train Loss :0.003192424774169922    Test Loss :0.022450942546129227\n",
      "Epoch :0.6916666666666667    Train Loss :0.00317373382858932    Test Loss :0.02602190338075161\n",
      "Epoch :0.7    Train Loss :0.003459468949586153    Test Loss :0.020883316174149513\n",
      "Epoch :0.7083333333333334    Train Loss :0.004366225562989712    Test Loss :0.023766737431287766\n",
      "Epoch :0.7166666666666667    Train Loss :0.003968890756368637    Test Loss :0.052209191024303436\n",
      "Epoch :0.725    Train Loss :0.003760834224522114    Test Loss :0.01887594349682331\n",
      "Epoch :0.7333333333333333    Train Loss :0.003819043515250087    Test Loss :0.032487694174051285\n",
      "Epoch :0.7416666666666667    Train Loss :0.004837637301534414    Test Loss :0.024111758917570114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.75    Train Loss :0.00390021875500679    Test Loss :0.01919185183942318\n",
      "Epoch :0.7583333333333333    Train Loss :0.004603407345712185    Test Loss :0.019124481827020645\n",
      "Epoch :0.7666666666666667    Train Loss :0.004275395534932613    Test Loss :0.018269114196300507\n",
      "Epoch :0.775    Train Loss :0.003654979635030031    Test Loss :0.032008010894060135\n",
      "Epoch :0.7833333333333333    Train Loss :0.004637294914573431    Test Loss :0.02721032314002514\n",
      "Epoch :0.7916666666666666    Train Loss :0.005237172357738018    Test Loss :0.029761087149381638\n",
      "Epoch :0.8    Train Loss :0.00375611730851233    Test Loss :0.025615457445383072\n",
      "Epoch :0.8083333333333333    Train Loss :0.003197904909029603    Test Loss :0.059623606503009796\n",
      "Epoch :0.8166666666666667    Train Loss :0.004357045516371727    Test Loss :0.034575629979372025\n",
      "Epoch :0.825    Train Loss :0.0032635449897497892    Test Loss :0.04265988618135452\n",
      "Epoch :0.8333333333333334    Train Loss :0.003703590715304017    Test Loss :0.016940947622060776\n",
      "Epoch :0.8416666666666667    Train Loss :0.00391963729634881    Test Loss :0.05771775543689728\n",
      "Epoch :0.85    Train Loss :0.0034764190204441547    Test Loss :0.02205389179289341\n",
      "Epoch :0.8583333333333333    Train Loss :0.004021898377686739    Test Loss :0.02494552917778492\n",
      "Epoch :0.8666666666666667    Train Loss :0.003654651576653123    Test Loss :0.04316125810146332\n",
      "Epoch :0.875    Train Loss :0.004267450422048569    Test Loss :0.017557382583618164\n",
      "Epoch :0.8833333333333333    Train Loss :0.0034486015792936087    Test Loss :0.009105594828724861\n",
      "Epoch :0.8916666666666667    Train Loss :0.004055224359035492    Test Loss :0.05001396685838699\n",
      "Epoch :0.9    Train Loss :0.0038721056189388037    Test Loss :0.015155860222876072\n",
      "Epoch :0.9083333333333333    Train Loss :0.003652080660685897    Test Loss :0.038464970886707306\n",
      "Epoch :0.9166666666666666    Train Loss :0.003886328311637044    Test Loss :0.014621206559240818\n",
      "Epoch :0.925    Train Loss :0.002832203172147274    Test Loss :0.022684140130877495\n",
      "Epoch :0.9333333333333333    Train Loss :0.0034168791025877    Test Loss :0.032472074031829834\n",
      "Epoch :0.9416666666666667    Train Loss :0.002973556751385331    Test Loss :0.02074296586215496\n",
      "Epoch :0.95    Train Loss :0.004407096654176712    Test Loss :0.021565420553088188\n",
      "Epoch :0.9583333333333334    Train Loss :0.0026236134581267834    Test Loss :0.02541280724108219\n",
      "Epoch :0.9666666666666667    Train Loss :0.002989244181662798    Test Loss :0.024078194051980972\n",
      "Epoch :0.975    Train Loss :0.00286330608651042    Test Loss :0.06273133307695389\n",
      "Epoch :0.9833333333333333    Train Loss :0.003400390036404133    Test Loss :0.047652482986450195\n",
      "Epoch :0.9916666666666667    Train Loss :0.002413528971374035    Test Loss :0.03102959133684635\n",
      "Epoch :1.0    Train Loss :0.0029896844644099474    Test Loss :0.05208875238895416\n",
      "RMSE: 16.166743483565153\n",
      "MAE: 13.551713256360156\n",
      "MAPE: 11.859746142513963%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 100, 'lr': 0.01, 'num_layers': 5}\n",
      "total:  94.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.008333333333333333    Train Loss :0.19609472155570984    Test Loss :0.5798997282981873\n",
      "Epoch :0.016666666666666666    Train Loss :0.05478660389780998    Test Loss :0.23438142240047455\n",
      "Epoch :0.025    Train Loss :0.046739768236875534    Test Loss :0.1592407524585724\n",
      "Epoch :0.03333333333333333    Train Loss :0.03886933624744415    Test Loss :0.06123919412493706\n",
      "Epoch :0.041666666666666664    Train Loss :0.04250122606754303    Test Loss :0.17052771151065826\n",
      "Epoch :0.05    Train Loss :0.01893918216228485    Test Loss :0.02904014103114605\n",
      "Epoch :0.058333333333333334    Train Loss :0.013073272071778774    Test Loss :0.03955911844968796\n",
      "Epoch :0.06666666666666667    Train Loss :0.013991891406476498    Test Loss :0.024800583720207214\n",
      "Epoch :0.075    Train Loss :0.011066445149481297    Test Loss :0.014456412754952908\n",
      "Epoch :0.08333333333333333    Train Loss :0.009481225162744522    Test Loss :0.015303128398954868\n",
      "Epoch :0.09166666666666666    Train Loss :0.009774377569556236    Test Loss :0.0208017248660326\n",
      "Epoch :0.1    Train Loss :0.008307662792503834    Test Loss :0.010814573615789413\n",
      "Epoch :0.10833333333333334    Train Loss :0.007711208891123533    Test Loss :0.017154138535261154\n",
      "Epoch :0.11666666666666667    Train Loss :0.007809879723936319    Test Loss :0.051407672464847565\n",
      "Epoch :0.125    Train Loss :0.007125745061784983    Test Loss :0.03280199319124222\n",
      "Epoch :0.13333333333333333    Train Loss :0.007631911430507898    Test Loss :0.03046269156038761\n",
      "Epoch :0.14166666666666666    Train Loss :0.005950664635747671    Test Loss :0.05652964115142822\n",
      "Epoch :0.15    Train Loss :0.006702720187604427    Test Loss :0.053333815187215805\n",
      "Epoch :0.15833333333333333    Train Loss :0.005552016664296389    Test Loss :0.018954824656248093\n",
      "Epoch :0.16666666666666666    Train Loss :0.007702086586505175    Test Loss :0.03031776286661625\n",
      "Epoch :0.175    Train Loss :0.005492604803293943    Test Loss :0.0711069256067276\n",
      "Epoch :0.18333333333333332    Train Loss :0.006521147210150957    Test Loss :0.05801938846707344\n",
      "Epoch :0.19166666666666668    Train Loss :0.005499399267137051    Test Loss :0.05096907541155815\n",
      "Epoch :0.2    Train Loss :0.0061728619039058685    Test Loss :0.065106101334095\n",
      "Epoch :0.20833333333333334    Train Loss :0.0048389192670583725    Test Loss :0.051950469613075256\n",
      "Epoch :0.21666666666666667    Train Loss :0.005516823381185532    Test Loss :0.05147388204932213\n",
      "Epoch :0.225    Train Loss :0.005586756393313408    Test Loss :0.05568511784076691\n",
      "Epoch :0.23333333333333334    Train Loss :0.0056250798515975475    Test Loss :0.07469892501831055\n",
      "Epoch :0.24166666666666667    Train Loss :0.005881792865693569    Test Loss :0.05418776720762253\n",
      "Epoch :0.25    Train Loss :0.005131283309310675    Test Loss :0.05020182579755783\n",
      "Epoch :0.25833333333333336    Train Loss :0.00605873903259635    Test Loss :0.09507279098033905\n",
      "Epoch :0.26666666666666666    Train Loss :0.00487958500161767    Test Loss :0.0742788165807724\n",
      "Epoch :0.275    Train Loss :0.005578326061367989    Test Loss :0.0784001424908638\n",
      "Epoch :0.2833333333333333    Train Loss :0.004262120928615332    Test Loss :0.08875890076160431\n",
      "Epoch :0.2916666666666667    Train Loss :0.005166952032595873    Test Loss :0.0584673248231411\n",
      "Epoch :0.3    Train Loss :0.004903707653284073    Test Loss :0.1334160715341568\n",
      "Epoch :0.30833333333333335    Train Loss :0.005099497269839048    Test Loss :0.10779634863138199\n",
      "Epoch :0.31666666666666665    Train Loss :0.004598625469952822    Test Loss :0.1458718478679657\n",
      "Epoch :0.325    Train Loss :0.004921284969896078    Test Loss :0.12555694580078125\n",
      "Epoch :0.3333333333333333    Train Loss :0.005283141974359751    Test Loss :0.07211310416460037\n",
      "Epoch :0.3416666666666667    Train Loss :0.004303750582039356    Test Loss :0.07025466114282608\n",
      "Epoch :0.35    Train Loss :0.003868530038744211    Test Loss :0.08370411396026611\n",
      "Epoch :0.35833333333333334    Train Loss :0.004600907675921917    Test Loss :0.06714864820241928\n",
      "Epoch :0.36666666666666664    Train Loss :0.005348320119082928    Test Loss :0.08616328239440918\n",
      "Epoch :0.375    Train Loss :0.004719963762909174    Test Loss :0.07856029272079468\n",
      "Epoch :0.38333333333333336    Train Loss :0.005156935192644596    Test Loss :0.07059172540903091\n",
      "Epoch :0.39166666666666666    Train Loss :0.0044639515690505505    Test Loss :0.08824319392442703\n",
      "Epoch :0.4    Train Loss :0.004451152868568897    Test Loss :0.0854174941778183\n",
      "Epoch :0.4083333333333333    Train Loss :0.004593015182763338    Test Loss :0.08852162957191467\n",
      "Epoch :0.4166666666666667    Train Loss :0.004363260231912136    Test Loss :0.0833805501461029\n",
      "Epoch :0.425    Train Loss :0.00384892919100821    Test Loss :0.07867909967899323\n",
      "Epoch :0.43333333333333335    Train Loss :0.00391090102493763    Test Loss :0.07283058762550354\n",
      "Epoch :0.44166666666666665    Train Loss :0.004770164843648672    Test Loss :0.082667775452137\n",
      "Epoch :0.45    Train Loss :0.00449558487161994    Test Loss :0.10527368634939194\n",
      "Epoch :0.4583333333333333    Train Loss :0.0037146774120628834    Test Loss :0.1019265428185463\n",
      "Epoch :0.4666666666666667    Train Loss :0.004348436836153269    Test Loss :0.1046089231967926\n",
      "Epoch :0.475    Train Loss :0.004809631500393152    Test Loss :0.09896133840084076\n",
      "Epoch :0.48333333333333334    Train Loss :0.0035057661589235067    Test Loss :0.05679825320839882\n",
      "Epoch :0.49166666666666664    Train Loss :0.0038853632286190987    Test Loss :0.054468609392642975\n",
      "Epoch :0.5    Train Loss :0.004712612368166447    Test Loss :0.08086082339286804\n",
      "Epoch :0.5083333333333333    Train Loss :0.004110159818083048    Test Loss :0.06119152903556824\n",
      "Epoch :0.5166666666666667    Train Loss :0.0050337347202003    Test Loss :0.046223800629377365\n",
      "Epoch :0.525    Train Loss :0.0032078318763524294    Test Loss :0.0779782310128212\n",
      "Epoch :0.5333333333333333    Train Loss :0.0039479671977460384    Test Loss :0.0797555223107338\n",
      "Epoch :0.5416666666666666    Train Loss :0.0034591725561767817    Test Loss :0.07028896361589432\n",
      "Epoch :0.55    Train Loss :0.0037277410738170147    Test Loss :0.057044729590415955\n",
      "Epoch :0.5583333333333333    Train Loss :0.0035980907268822193    Test Loss :0.06736193597316742\n",
      "Epoch :0.5666666666666667    Train Loss :0.0030568884685635567    Test Loss :0.06539998203516006\n",
      "Epoch :0.575    Train Loss :0.0035864124074578285    Test Loss :0.09087167680263519\n",
      "Epoch :0.5833333333333334    Train Loss :0.003218992380425334    Test Loss :0.08491931855678558\n",
      "Epoch :0.5916666666666667    Train Loss :0.0035305130295455456    Test Loss :0.08967194706201553\n",
      "Epoch :0.6    Train Loss :0.0035838403273373842    Test Loss :0.09669718891382217\n",
      "Epoch :0.6083333333333333    Train Loss :0.0031167950946837664    Test Loss :0.0926593467593193\n",
      "Epoch :0.6166666666666667    Train Loss :0.0040242173708975315    Test Loss :0.09896339476108551\n",
      "Epoch :0.625    Train Loss :0.0038096995558589697    Test Loss :0.09406986087560654\n",
      "Epoch :0.6333333333333333    Train Loss :0.003232210874557495    Test Loss :0.08306293189525604\n",
      "Epoch :0.6416666666666667    Train Loss :0.0036285577807575464    Test Loss :0.08149056881666183\n",
      "Epoch :0.65    Train Loss :0.003500494407489896    Test Loss :0.062378037720918655\n",
      "Epoch :0.6583333333333333    Train Loss :0.004886215087026358    Test Loss :0.05289602652192116\n",
      "Epoch :0.6666666666666666    Train Loss :0.0031066483352333307    Test Loss :0.03943851590156555\n",
      "Epoch :0.675    Train Loss :0.003923897165805101    Test Loss :0.057631734758615494\n",
      "Epoch :0.6833333333333333    Train Loss :0.0033850069157779217    Test Loss :0.030681269243359566\n",
      "Epoch :0.6916666666666667    Train Loss :0.003272570203989744    Test Loss :0.0701722577214241\n",
      "Epoch :0.7    Train Loss :0.0031815937254577875    Test Loss :0.07246765494346619\n",
      "Epoch :0.7083333333333334    Train Loss :0.0037983120419085026    Test Loss :0.08862251043319702\n",
      "Epoch :0.7166666666666667    Train Loss :0.003569825319573283    Test Loss :0.06690984964370728\n",
      "Epoch :0.725    Train Loss :0.0030546654015779495    Test Loss :0.05161811038851738\n",
      "Epoch :0.7333333333333333    Train Loss :0.0030808160081505775    Test Loss :0.07252350449562073\n",
      "Epoch :0.7416666666666667    Train Loss :0.0029206578619778156    Test Loss :0.07796184718608856\n",
      "Epoch :0.75    Train Loss :0.0033713250886648893    Test Loss :0.08108770102262497\n",
      "Epoch :0.7583333333333333    Train Loss :0.002566308481618762    Test Loss :0.050286389887332916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.7666666666666667    Train Loss :0.002901780651882291    Test Loss :0.05826830118894577\n",
      "Epoch :0.775    Train Loss :0.0032123499549925327    Test Loss :0.07588709890842438\n",
      "Epoch :0.7833333333333333    Train Loss :0.002805879805237055    Test Loss :0.09585398435592651\n",
      "Epoch :0.7916666666666666    Train Loss :0.003729620948433876    Test Loss :0.06254186481237411\n",
      "Epoch :0.8    Train Loss :0.0034356736578047276    Test Loss :0.05868291109800339\n",
      "Epoch :0.8083333333333333    Train Loss :0.00345309148542583    Test Loss :0.08480577170848846\n",
      "Epoch :0.8166666666666667    Train Loss :0.003077027853578329    Test Loss :0.093468576669693\n",
      "Epoch :0.825    Train Loss :0.0032753036357462406    Test Loss :0.07364927232265472\n",
      "Epoch :0.8333333333333334    Train Loss :0.003007461316883564    Test Loss :0.0616161935031414\n",
      "Epoch :0.8416666666666667    Train Loss :0.003952504135668278    Test Loss :0.045236438512802124\n",
      "Epoch :0.85    Train Loss :0.0029300411697477102    Test Loss :0.04316522181034088\n",
      "Epoch :0.8583333333333333    Train Loss :0.0037129241973161697    Test Loss :0.05461083725094795\n",
      "Epoch :0.8666666666666667    Train Loss :0.0031337731052190065    Test Loss :0.04941672086715698\n",
      "Epoch :0.875    Train Loss :0.003464231500402093    Test Loss :0.054332435131073\n",
      "Epoch :0.8833333333333333    Train Loss :0.0034982392098754644    Test Loss :0.0371941477060318\n",
      "Epoch :0.8916666666666667    Train Loss :0.0031130656134337187    Test Loss :0.04286475479602814\n",
      "Epoch :0.9    Train Loss :0.0026440841611474752    Test Loss :0.04915335029363632\n",
      "Epoch :0.9083333333333333    Train Loss :0.0032527658622711897    Test Loss :0.05391069874167442\n",
      "Epoch :0.9166666666666666    Train Loss :0.003362315008416772    Test Loss :0.038417134433984756\n",
      "Epoch :0.925    Train Loss :0.0027051412034779787    Test Loss :0.040591202676296234\n",
      "Epoch :0.9333333333333333    Train Loss :0.004211112391203642    Test Loss :0.06328807026147842\n",
      "Epoch :0.9416666666666667    Train Loss :0.002631308976560831    Test Loss :0.045629389584064484\n",
      "Epoch :0.95    Train Loss :0.0031921162735670805    Test Loss :0.05144849419593811\n",
      "Epoch :0.9583333333333334    Train Loss :0.002951035276055336    Test Loss :0.03670518845319748\n",
      "Epoch :0.9666666666666667    Train Loss :0.002389113185927272    Test Loss :0.047358084470033646\n",
      "Epoch :0.975    Train Loss :0.0036022430285811424    Test Loss :0.039887141436338425\n",
      "Epoch :0.9833333333333333    Train Loss :0.0026893431786447763    Test Loss :0.05204322189092636\n",
      "Epoch :0.9916666666666667    Train Loss :0.003019872587174177    Test Loss :0.04607153683900833\n",
      "Epoch :1.0    Train Loss :0.0026379236951470375    Test Loss :0.04816945269703865\n",
      "RMSE: 18.03427093353848\n",
      "MAE: 14.069513733825364\n",
      "MAPE: 12.074334711952126%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 100, 'lr': 0.02, 'num_layers': 2}\n",
      "total: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 97.0\n",
      "Epoch :0.008333333333333333    Train Loss :0.14541825652122498    Test Loss :1.374518871307373\n",
      "Epoch :0.016666666666666666    Train Loss :0.06205430254340172    Test Loss :0.4413036108016968\n",
      "Epoch :0.025    Train Loss :0.0634407252073288    Test Loss :0.2563707232475281\n",
      "Epoch :0.03333333333333333    Train Loss :0.04733946919441223    Test Loss :0.1929008662700653\n",
      "Epoch :0.041666666666666664    Train Loss :0.040980953723192215    Test Loss :0.1834021359682083\n",
      "Epoch :0.05    Train Loss :0.027403075248003006    Test Loss :0.020240964367985725\n",
      "Epoch :0.058333333333333334    Train Loss :0.016583824530243874    Test Loss :0.054128073155879974\n",
      "Epoch :0.06666666666666667    Train Loss :0.013965984806418419    Test Loss :0.029553435742855072\n",
      "Epoch :0.075    Train Loss :0.011841853149235249    Test Loss :0.021526791155338287\n",
      "Epoch :0.08333333333333333    Train Loss :0.014526688493788242    Test Loss :0.028631573542952538\n",
      "Epoch :0.09166666666666666    Train Loss :0.0128755122423172    Test Loss :0.02584194764494896\n",
      "Epoch :0.1    Train Loss :0.012237122282385826    Test Loss :0.029575111344456673\n",
      "Epoch :0.10833333333333334    Train Loss :0.011280610226094723    Test Loss :0.019229363650083542\n",
      "Epoch :0.11666666666666667    Train Loss :0.012459691613912582    Test Loss :0.022084295749664307\n",
      "Epoch :0.125    Train Loss :0.010081262327730656    Test Loss :0.03585430979728699\n",
      "Epoch :0.13333333333333333    Train Loss :0.01152805332094431    Test Loss :0.020392833277583122\n",
      "Epoch :0.14166666666666666    Train Loss :0.010682680644094944    Test Loss :0.0149273332208395\n",
      "Epoch :0.15    Train Loss :0.010095352306962013    Test Loss :0.02198442444205284\n",
      "Epoch :0.15833333333333333    Train Loss :0.009265454486012459    Test Loss :0.020158879458904266\n",
      "Epoch :0.16666666666666666    Train Loss :0.008799845352768898    Test Loss :0.014061501249670982\n",
      "Epoch :0.175    Train Loss :0.008741452358663082    Test Loss :0.022343700751662254\n",
      "Epoch :0.18333333333333332    Train Loss :0.00983088742941618    Test Loss :0.027219828218221664\n",
      "Epoch :0.19166666666666668    Train Loss :0.008702848106622696    Test Loss :0.020476922392845154\n",
      "Epoch :0.2    Train Loss :0.00946726556867361    Test Loss :0.020326221361756325\n",
      "Epoch :0.20833333333333334    Train Loss :0.008888530544936657    Test Loss :0.017619458958506584\n",
      "Epoch :0.21666666666666667    Train Loss :0.008125683292746544    Test Loss :0.021808728575706482\n",
      "Epoch :0.225    Train Loss :0.008766882121562958    Test Loss :0.018213702365756035\n",
      "Epoch :0.23333333333333334    Train Loss :0.009113123640418053    Test Loss :0.01995173655450344\n",
      "Epoch :0.24166666666666667    Train Loss :0.007974611595273018    Test Loss :0.022222809493541718\n",
      "Epoch :0.25    Train Loss :0.009252889081835747    Test Loss :0.017119869589805603\n",
      "Epoch :0.25833333333333336    Train Loss :0.008033206686377525    Test Loss :0.015640219673514366\n",
      "Epoch :0.26666666666666666    Train Loss :0.00798562727868557    Test Loss :0.019017111510038376\n",
      "Epoch :0.275    Train Loss :0.007369932718575001    Test Loss :0.015030132606625557\n",
      "Epoch :0.2833333333333333    Train Loss :0.007657312788069248    Test Loss :0.014655058272182941\n",
      "Epoch :0.2916666666666667    Train Loss :0.008129713125526905    Test Loss :0.017079045996069908\n",
      "Epoch :0.3    Train Loss :0.007500946056097746    Test Loss :0.016321241855621338\n",
      "Epoch :0.30833333333333335    Train Loss :0.006639557890594006    Test Loss :0.016478372737765312\n",
      "Epoch :0.31666666666666665    Train Loss :0.007483616936951876    Test Loss :0.021995607763528824\n",
      "Epoch :0.325    Train Loss :0.008168951608240604    Test Loss :0.01874636858701706\n",
      "Epoch :0.3333333333333333    Train Loss :0.007280309218913317    Test Loss :0.015376999974250793\n",
      "Epoch :0.3416666666666667    Train Loss :0.00761184748262167    Test Loss :0.020671913400292397\n",
      "Epoch :0.35    Train Loss :0.008603112772107124    Test Loss :0.01594400778412819\n",
      "Epoch :0.35833333333333334    Train Loss :0.006352790165692568    Test Loss :0.024321533739566803\n",
      "Epoch :0.36666666666666664    Train Loss :0.006988914217799902    Test Loss :0.020202722400426865\n",
      "Epoch :0.375    Train Loss :0.00784477312117815    Test Loss :0.01713624782860279\n",
      "Epoch :0.38333333333333336    Train Loss :0.007493015378713608    Test Loss :0.022068345919251442\n",
      "Epoch :0.39166666666666666    Train Loss :0.007834400050342083    Test Loss :0.015561340376734734\n",
      "Epoch :0.4    Train Loss :0.007812126073986292    Test Loss :0.022432927042245865\n",
      "Epoch :0.4083333333333333    Train Loss :0.007127473596483469    Test Loss :0.018545174971222878\n",
      "Epoch :0.4166666666666667    Train Loss :0.007459667511284351    Test Loss :0.021278157830238342\n",
      "Epoch :0.425    Train Loss :0.007525159511715174    Test Loss :0.014628540724515915\n",
      "Epoch :0.43333333333333335    Train Loss :0.0070040225982666016    Test Loss :0.01986067369580269\n",
      "Epoch :0.44166666666666665    Train Loss :0.008435540832579136    Test Loss :0.011983227916061878\n",
      "Epoch :0.45    Train Loss :0.006714759394526482    Test Loss :0.01889798603951931\n",
      "Epoch :0.4583333333333333    Train Loss :0.007304654456675053    Test Loss :0.010195457376539707\n",
      "Epoch :0.4666666666666667    Train Loss :0.006979568861424923    Test Loss :0.02517521195113659\n",
      "Epoch :0.475    Train Loss :0.007035326678305864    Test Loss :0.014865180477499962\n",
      "Epoch :0.48333333333333334    Train Loss :0.006502362433820963    Test Loss :0.021981552243232727\n",
      "Epoch :0.49166666666666664    Train Loss :0.006810000166296959    Test Loss :0.014721086248755455\n",
      "Epoch :0.5    Train Loss :0.006684522144496441    Test Loss :0.017587710171937943\n",
      "Epoch :0.5083333333333333    Train Loss :0.006639300845563412    Test Loss :0.014225351624190807\n",
      "Epoch :0.5166666666666667    Train Loss :0.0064301444217562675    Test Loss :0.02007085084915161\n",
      "Epoch :0.525    Train Loss :0.006064333952963352    Test Loss :0.015426836907863617\n",
      "Epoch :0.5333333333333333    Train Loss :0.006161171477288008    Test Loss :0.01807522587478161\n",
      "Epoch :0.5416666666666666    Train Loss :0.006246278528124094    Test Loss :0.01455743983387947\n",
      "Epoch :0.55    Train Loss :0.0059357802383601665    Test Loss :0.020959576591849327\n",
      "Epoch :0.5583333333333333    Train Loss :0.0057108765468001366    Test Loss :0.01484115794301033\n",
      "Epoch :0.5666666666666667    Train Loss :0.006402221042662859    Test Loss :0.01672874018549919\n",
      "Epoch :0.575    Train Loss :0.006544044241309166    Test Loss :0.017951352521777153\n",
      "Epoch :0.5833333333333334    Train Loss :0.005824122112244368    Test Loss :0.012987117283046246\n",
      "Epoch :0.5916666666666667    Train Loss :0.006698689889162779    Test Loss :0.018167395144701004\n",
      "Epoch :0.6    Train Loss :0.0072338515892624855    Test Loss :0.015779299661517143\n",
      "Epoch :0.6083333333333333    Train Loss :0.010668393224477768    Test Loss :0.018230708315968513\n",
      "Epoch :0.6166666666666667    Train Loss :0.00821759831160307    Test Loss :0.020833147689700127\n",
      "Epoch :0.625    Train Loss :0.007239806931465864    Test Loss :0.025773201137781143\n",
      "Epoch :0.6333333333333333    Train Loss :0.007156970910727978    Test Loss :0.014565049670636654\n",
      "Epoch :0.6416666666666667    Train Loss :0.006490688771009445    Test Loss :0.018678806722164154\n",
      "Epoch :0.65    Train Loss :0.006818368099629879    Test Loss :0.018562253564596176\n",
      "Epoch :0.6583333333333333    Train Loss :0.0065928311087191105    Test Loss :0.014800713397562504\n",
      "Epoch :0.6666666666666666    Train Loss :0.007046383339911699    Test Loss :0.01935184933245182\n",
      "Epoch :0.675    Train Loss :0.007063526194542646    Test Loss :0.017527377232909203\n",
      "Epoch :0.6833333333333333    Train Loss :0.006297720596194267    Test Loss :0.01305166445672512\n",
      "Epoch :0.6916666666666667    Train Loss :0.006359423045068979    Test Loss :0.0217377170920372\n",
      "Epoch :0.7    Train Loss :0.006381251383572817    Test Loss :0.015350965782999992\n",
      "Epoch :0.7083333333333334    Train Loss :0.006029026582837105    Test Loss :0.01928800344467163\n",
      "Epoch :0.7166666666666667    Train Loss :0.006937975063920021    Test Loss :0.023711126297712326\n",
      "Epoch :0.725    Train Loss :0.006027879659086466    Test Loss :0.015736639499664307\n",
      "Epoch :0.7333333333333333    Train Loss :0.006240170914679766    Test Loss :0.020069923251867294\n",
      "Epoch :0.7416666666666667    Train Loss :0.005948739591985941    Test Loss :0.015922853723168373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.75    Train Loss :0.006616572383791208    Test Loss :0.021412700414657593\n",
      "Epoch :0.7583333333333333    Train Loss :0.005757135339081287    Test Loss :0.015656454488635063\n",
      "Epoch :0.7666666666666667    Train Loss :0.006293569225817919    Test Loss :0.023886337876319885\n",
      "Epoch :0.775    Train Loss :0.006229826714843512    Test Loss :0.011392194777727127\n",
      "Epoch :0.7833333333333333    Train Loss :0.0055215065367519855    Test Loss :0.023157885298132896\n",
      "Epoch :0.7916666666666666    Train Loss :0.005963869392871857    Test Loss :0.013716701418161392\n",
      "Epoch :0.8    Train Loss :0.007112489081919193    Test Loss :0.017378760501742363\n",
      "Epoch :0.8083333333333333    Train Loss :0.005348469130694866    Test Loss :0.02048603817820549\n",
      "Epoch :0.8166666666666667    Train Loss :0.005650247912853956    Test Loss :0.011443855240941048\n",
      "Epoch :0.825    Train Loss :0.0059069800190627575    Test Loss :0.021102316677570343\n",
      "Epoch :0.8333333333333334    Train Loss :0.005710321478545666    Test Loss :0.013678274117410183\n",
      "Epoch :0.8416666666666667    Train Loss :0.006146114319562912    Test Loss :0.01802019029855728\n",
      "Epoch :0.85    Train Loss :0.005644469987601042    Test Loss :0.018081625923514366\n",
      "Epoch :0.8583333333333333    Train Loss :0.005011235363781452    Test Loss :0.013649780303239822\n",
      "Epoch :0.8666666666666667    Train Loss :0.0054134223610162735    Test Loss :0.01722712069749832\n",
      "Epoch :0.875    Train Loss :0.005595972295850515    Test Loss :0.016767490655183792\n",
      "Epoch :0.8833333333333333    Train Loss :0.004920196253806353    Test Loss :0.015093488618731499\n",
      "Epoch :0.8916666666666667    Train Loss :0.005757669452577829    Test Loss :0.019541146233677864\n",
      "Epoch :0.9    Train Loss :0.005384389311075211    Test Loss :0.018043847754597664\n",
      "Epoch :0.9083333333333333    Train Loss :0.0054198866710066795    Test Loss :0.015459517017006874\n",
      "Epoch :0.9166666666666666    Train Loss :0.004792443942278624    Test Loss :0.01867484487593174\n",
      "Epoch :0.925    Train Loss :0.0054073939099907875    Test Loss :0.015700729563832283\n",
      "Epoch :0.9333333333333333    Train Loss :0.005383115727454424    Test Loss :0.017816783860325813\n",
      "Epoch :0.9416666666666667    Train Loss :0.00529737351462245    Test Loss :0.012806909158825874\n",
      "Epoch :0.95    Train Loss :0.005163340363651514    Test Loss :0.013508085161447525\n",
      "Epoch :0.9583333333333334    Train Loss :0.005524097476154566    Test Loss :0.02155962586402893\n",
      "Epoch :0.9666666666666667    Train Loss :0.005456569138914347    Test Loss :0.013173995539546013\n",
      "Epoch :0.975    Train Loss :0.005663261283189058    Test Loss :0.01873129978775978\n",
      "Epoch :0.9833333333333333    Train Loss :0.004895014688372612    Test Loss :0.013975805602967739\n",
      "Epoch :0.9916666666666667    Train Loss :0.004917396232485771    Test Loss :0.01801249384880066\n",
      "Epoch :1.0    Train Loss :0.00557565176859498    Test Loss :0.01656179316341877\n",
      "RMSE: 11.729051463767398\n",
      "MAE: 9.712508552141774\n",
      "MAPE: 8.299083885933328%\n",
      "parametros: {'dropout_rate': 0.8, 'epochs': 600, 'hid_size': 100, 'lr': 0.02, 'num_layers': 5}\n",
      "total:  100.0\n",
      "Tempo total de execução: 907.5527322292328 segundos\n",
      "RMSE: 9.326625729650555\n",
      "MAE: 7.036730136911734\n",
      "MAPE: 6.0227364034298585\n",
      "{'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 100, 'lr': 0.02, 'num_layers': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex\\AppData\\Local\\Temp\\ipykernel_12708\\1047888194.py:84: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "params_grid = {'num_layers':[2,5],\n",
    "               'hid_size':[50,100],\n",
    "              'lr': [0.01,0.02],\n",
    "               'epochs':[400,600],\n",
    "              'dropout_rate':[0.5,0.8]}\n",
    "grid = ParameterGrid(params_grid)\n",
    "cnt = 0\n",
    "for p in grid:\n",
    "    cnt = cnt+1\n",
    "\n",
    "print('Total Possible Models',cnt)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model_parameters = pd.DataFrame(columns = ['RMSE','Parameters'])\n",
    "best_rmse = np.inf\n",
    "best_prediction=None\n",
    "count = 0\n",
    "\n",
    "for p in grid:\n",
    "    \n",
    "    lstm = LSTM(in_dim = x.shape[-1],\n",
    "                hid_dim = p['hid_size'],\n",
    "                out_dim = x.shape[-1],\n",
    "                num_layers = p['num_layers'], \n",
    "                dropout_rate= p['dropout_rate'])\n",
    "    \n",
    "    optimizer = torch.optim.Adam(lstm.parameters(), lr=p['lr'])\n",
    "    \n",
    "    loss_fun = nn.MSELoss()\n",
    "    \n",
    "    train_loss, test_loss = train_model(lstm,\n",
    "               loss_fun,\n",
    "               optimizer,\n",
    "               train_x,\n",
    "               test_x,\n",
    "               train_y,\n",
    "               test_y,\n",
    "               epochs=p['epochs'])\n",
    "\n",
    "    \n",
    "    # testing the predction model on multiple time series\n",
    "    last_x = train_x[-1].view(entradas)\n",
    "\n",
    "    prediction_val = []\n",
    "\n",
    "    while len(prediction_val)<len(test_y):\n",
    "        prediction = lstm(last_x.view(1,entradas,1))\n",
    "        prediction_val.append(prediction[0,0].item())\n",
    "\n",
    "\n",
    "        ## replace the predicted value in last x\n",
    "        last_x = torch.cat((last_x[1:],prediction[0]))\n",
    "\n",
    "    # plot the result\n",
    "    train_y_cp = scale.inverse_transform(train_y.detach().numpy())\n",
    "    test_y_cp = scale.inverse_transform(test_y.detach().numpy())\n",
    "    prediction_val = scale.inverse_transform(np.asarray(prediction_val).reshape(-1,1))\n",
    "\n",
    "    y_true = test_y_cp\n",
    "    y_pred = prediction_val\n",
    "\n",
    "    RMSE = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    print(f'RMSE: {RMSE}')\n",
    "\n",
    "    MAE = mean_absolute_error(y_true, y_pred)\n",
    "    print(f'MAE: {MAE}')\n",
    "\n",
    "    MAPE = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    print(f'MAPE: {MAPE}%')\n",
    "    \n",
    "    print(f'parametros: {p}')\n",
    "    \n",
    "    if RMSE < best_rmse:\n",
    "        best_rmse = RMSE\n",
    "        best_mae = MAE\n",
    "        best_mape = MAPE\n",
    "        best_prediction2 = y_pred\n",
    "        best_parameters2 = p\n",
    "    \n",
    "    model_parameters = model_parameters.append({'RMSE':RMSE,'MAE':MAE,'MAPE':MAPE,'Parameters':p},ignore_index=True)\n",
    "    count += 1\n",
    "    print(\"total: \" ,round(count/cnt,2)*100)\n",
    "    \n",
    "end_time = time.time()\n",
    "print(f\"Tempo total de execução: {end_time - start_time} segundos\")\n",
    "\n",
    "parameters = model_parameters.sort_values(by=['RMSE'])\n",
    "parameters = parameters.reset_index(drop=True)\n",
    "print('RMSE:',parameters.loc[0, 'RMSE'])\n",
    "print('MAE:',parameters.loc[0, 'MAE'])\n",
    "print('MAPE:',parameters.loc[0, 'MAPE'])\n",
    "print(parameters.loc[0, 'Parameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b3022d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 9.326625729650555\n",
      "MAE: 7.036730136911734\n",
      "MAPE: 6.0227364034298585\n",
      "{'dropout_rate': 0.5, 'epochs': 400, 'hid_size': 100, 'lr': 0.02, 'num_layers': 2}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameters = model_parameters.sort_values(by=['RMSE'])\n",
    "parameters = parameters.reset_index(drop=True)\n",
    "print('RMSE:',parameters.loc[0, 'RMSE'])\n",
    "print('MAE:',parameters.loc[0, 'MAE'])\n",
    "print('MAPE:',parameters.loc[0, 'MAPE'])\n",
    "print(parameters.loc[0, 'Parameters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4ebf3078",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropout_rate': 0.5,\n",
       " 'epochs': 400,\n",
       " 'hid_size': 100,\n",
       " 'lr': 0.02,\n",
       " 'num_layers': 2}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_parameters2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9d140b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_6 = pd.concat([treino_mensal, teste_mensal, previsao_mensal])\n",
    "df_treino = pd.concat([treino_mensal, teste_mensal])\n",
    "entradas = 5\n",
    "x,y,train_x,train_y,test_x,test_y = escalonar(df_6,df_treino, entradas,scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b6e14934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :0.0125    Train Loss :0.12461134791374207    Test Loss :0.5340958833694458\n",
      "Epoch :0.025    Train Loss :0.05397188290953636    Test Loss :0.3153873682022095\n",
      "Epoch :0.0375    Train Loss :0.04674431309103966    Test Loss :0.05259029567241669\n",
      "Epoch :0.05    Train Loss :0.01944100484251976    Test Loss :0.02649158053100109\n",
      "Epoch :0.0625    Train Loss :0.011639428324997425    Test Loss :0.05836619809269905\n",
      "Epoch :0.075    Train Loss :0.006904996931552887    Test Loss :0.008396023884415627\n",
      "Epoch :0.0875    Train Loss :0.007120877504348755    Test Loss :0.0274211298674345\n",
      "Epoch :0.1    Train Loss :0.008029219694435596    Test Loss :0.03518369793891907\n",
      "Epoch :0.1125    Train Loss :0.006647803820669651    Test Loss :0.013339623808860779\n",
      "Epoch :0.125    Train Loss :0.0053853061981499195    Test Loss :0.023101747035980225\n",
      "Epoch :0.1375    Train Loss :0.005502123385667801    Test Loss :0.034197401255369186\n",
      "Epoch :0.15    Train Loss :0.005573246628046036    Test Loss :0.021939856931567192\n",
      "Epoch :0.1625    Train Loss :0.005472500808537006    Test Loss :0.01901359297335148\n",
      "Epoch :0.175    Train Loss :0.005535585805773735    Test Loss :0.015769774094223976\n",
      "Epoch :0.1875    Train Loss :0.004705400671809912    Test Loss :0.0281621515750885\n",
      "Epoch :0.2    Train Loss :0.004905833397060633    Test Loss :0.018574371933937073\n",
      "Epoch :0.2125    Train Loss :0.004573290701955557    Test Loss :0.014332798309624195\n",
      "Epoch :0.225    Train Loss :0.005044138990342617    Test Loss :0.012457597069442272\n",
      "Epoch :0.2375    Train Loss :0.004542980343103409    Test Loss :0.01865088753402233\n",
      "Epoch :0.25    Train Loss :0.004817795939743519    Test Loss :0.021764187142252922\n",
      "Epoch :0.2625    Train Loss :0.0048158979043364525    Test Loss :0.02663589082658291\n",
      "Epoch :0.275    Train Loss :0.003922733012586832    Test Loss :0.028016669675707817\n",
      "Epoch :0.2875    Train Loss :0.0039358437061309814    Test Loss :0.020325249060988426\n",
      "Epoch :0.3    Train Loss :0.00408176938071847    Test Loss :0.016865193843841553\n",
      "Epoch :0.3125    Train Loss :0.0033749721478670835    Test Loss :0.017674501985311508\n",
      "Epoch :0.325    Train Loss :0.003763922257348895    Test Loss :0.019939878955483437\n",
      "Epoch :0.3375    Train Loss :0.002947712317109108    Test Loss :0.023659178987145424\n",
      "Epoch :0.35    Train Loss :0.0032780098263174295    Test Loss :0.012717977166175842\n",
      "Epoch :0.3625    Train Loss :0.0032915184274315834    Test Loss :0.01630546897649765\n",
      "Epoch :0.375    Train Loss :0.002901849104091525    Test Loss :0.012580398470163345\n",
      "Epoch :0.3875    Train Loss :0.003136808518320322    Test Loss :0.008693859912455082\n",
      "Epoch :0.4    Train Loss :0.003392965067178011    Test Loss :0.01566476933658123\n",
      "Epoch :0.4125    Train Loss :0.0032196789979934692    Test Loss :0.019212203100323677\n",
      "Epoch :0.425    Train Loss :0.0031475245486944914    Test Loss :0.02486640401184559\n",
      "Epoch :0.4375    Train Loss :0.002369325840845704    Test Loss :0.009785798378288746\n",
      "Epoch :0.45    Train Loss :0.0024319086223840714    Test Loss :0.007280758116394281\n",
      "Epoch :0.4625    Train Loss :0.0025205686688423157    Test Loss :0.013834971934556961\n",
      "Epoch :0.475    Train Loss :0.0025711823254823685    Test Loss :0.007305522914975882\n",
      "Epoch :0.4875    Train Loss :0.003107401542365551    Test Loss :0.00980343297123909\n",
      "Epoch :0.5    Train Loss :0.0028349130880087614    Test Loss :0.011289160698652267\n",
      "Epoch :0.5125    Train Loss :0.0018220413476228714    Test Loss :0.012490046210587025\n",
      "Epoch :0.525    Train Loss :0.0021488142665475607    Test Loss :0.017687523737549782\n",
      "Epoch :0.5375    Train Loss :0.002690990222617984    Test Loss :0.01375869382172823\n",
      "Epoch :0.55    Train Loss :0.002653452567756176    Test Loss :0.015005909837782383\n",
      "Epoch :0.5625    Train Loss :0.002639013109728694    Test Loss :0.01558851171284914\n",
      "Epoch :0.575    Train Loss :0.0023698091972619295    Test Loss :0.004281332716345787\n",
      "Epoch :0.5875    Train Loss :0.0023836384061723948    Test Loss :0.009512840770184994\n",
      "Epoch :0.6    Train Loss :0.002395445480942726    Test Loss :0.010171708650887012\n",
      "Epoch :0.6125    Train Loss :0.0022545766551047564    Test Loss :0.008023441769182682\n",
      "Epoch :0.625    Train Loss :0.002175780013203621    Test Loss :0.003971263766288757\n",
      "Epoch :0.6375    Train Loss :0.0023251590318977833    Test Loss :0.01714175194501877\n",
      "Epoch :0.65    Train Loss :0.0024856620002537966    Test Loss :0.009614079259335995\n",
      "Epoch :0.6625    Train Loss :0.002399587305262685    Test Loss :0.011587361805140972\n",
      "Epoch :0.675    Train Loss :0.001995683880522847    Test Loss :0.01294352114200592\n",
      "Epoch :0.6875    Train Loss :0.002389466157183051    Test Loss :0.008670484647154808\n",
      "Epoch :0.7    Train Loss :0.0023446246050298214    Test Loss :0.009586281143128872\n",
      "Epoch :0.7125    Train Loss :0.0023662748280912638    Test Loss :0.005820813123136759\n",
      "Epoch :0.725    Train Loss :0.002160552889108658    Test Loss :0.002755800960585475\n",
      "Epoch :0.7375    Train Loss :0.00217982055619359    Test Loss :0.003090834943577647\n",
      "Epoch :0.75    Train Loss :0.0024281691294163465    Test Loss :0.0033803742844611406\n",
      "Epoch :0.7625    Train Loss :0.0022725954186171293    Test Loss :0.01375157292932272\n",
      "Epoch :0.775    Train Loss :0.00236182427033782    Test Loss :0.014764646999537945\n",
      "Epoch :0.7875    Train Loss :0.0020873115863651037    Test Loss :0.010804247111082077\n",
      "Epoch :0.8    Train Loss :0.0023937132209539413    Test Loss :0.008171617984771729\n",
      "Epoch :0.8125    Train Loss :0.0024773902259767056    Test Loss :0.011807591654360294\n",
      "Epoch :0.825    Train Loss :0.0019737512338906527    Test Loss :0.010311140678822994\n",
      "Epoch :0.8375    Train Loss :0.0020797853358089924    Test Loss :0.0042117987759411335\n",
      "Epoch :0.85    Train Loss :0.0020310855470597744    Test Loss :0.006391179282218218\n",
      "Epoch :0.8625    Train Loss :0.0024202605709433556    Test Loss :0.012473559938371181\n",
      "Epoch :0.875    Train Loss :0.0023428392596542835    Test Loss :0.009699426591396332\n",
      "Epoch :0.8875    Train Loss :0.0024398821406066418    Test Loss :0.006826536729931831\n",
      "Epoch :0.9    Train Loss :0.002131104003638029    Test Loss :0.006446802522987127\n",
      "Epoch :0.9125    Train Loss :0.002116217277944088    Test Loss :0.0023629663046449423\n",
      "Epoch :0.925    Train Loss :0.002388244727626443    Test Loss :0.0024101147428154945\n",
      "Epoch :0.9375    Train Loss :0.0021852042991667986    Test Loss :0.01128325890749693\n",
      "Epoch :0.95    Train Loss :0.0023127279710024595    Test Loss :0.007097398396581411\n",
      "Epoch :0.9625    Train Loss :0.002387955319136381    Test Loss :0.006956664379686117\n",
      "Epoch :0.975    Train Loss :0.0021245696116238832    Test Loss :0.00837813038378954\n",
      "Epoch :0.9875    Train Loss :0.0022728920448571444    Test Loss :0.007246352732181549\n",
      "Epoch :1.0    Train Loss :0.0020480488892644644    Test Loss :0.005769169423729181\n",
      "RMSE: 13.310330172724878\n",
      "MAE: 12.96505851902466\n",
      "MAPE: 10.002661316681381%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "lstm = LSTM(in_dim = x.shape[-1],\n",
    "            hid_dim = best_parameters2['hid_size'],\n",
    "            out_dim = x.shape[-1],\n",
    "            num_layers =best_parameters2['num_layers'], \n",
    "            dropout_rate= best_parameters2['dropout_rate'])\n",
    "\n",
    "optimizer = torch.optim.Adam(lstm.parameters(), lr=best_parameters2['lr'])\n",
    "\n",
    "loss_fun = nn.MSELoss()\n",
    "\n",
    "train_loss, test_loss = train_model(lstm,\n",
    "           loss_fun,\n",
    "           optimizer,\n",
    "           train_x,\n",
    "           test_x,\n",
    "           train_y,\n",
    "           test_y,\n",
    "           epochs=best_parameters2['epochs'])\n",
    "\n",
    "\n",
    "# testing the predction model on multiple time series\n",
    "last_x = train_x[-1].view(entradas)\n",
    "\n",
    "prediction_val = []\n",
    "\n",
    "while len(prediction_val)<len(test_y):\n",
    "    prediction = lstm(last_x.view(1,entradas,1))\n",
    "    prediction_val.append(prediction[0,0].item())\n",
    "\n",
    "\n",
    "    ## replace the predicted value in last x\n",
    "    last_x = torch.cat((last_x[1:],prediction[0]))\n",
    "\n",
    "# plot the result\n",
    "train_y_cp = scale.inverse_transform(train_y.detach().numpy())\n",
    "test_y_cp = scale.inverse_transform(test_y.detach().numpy())\n",
    "prediction_val = scale.inverse_transform(np.asarray(prediction_val).reshape(-1,1))\n",
    "\n",
    "y_true = test_y_cp\n",
    "y_pred = prediction_val\n",
    "\n",
    "\n",
    "RMSE = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "print(f'RMSE: {RMSE}')\n",
    "\n",
    "MAE = mean_absolute_error(y_true, y_pred)\n",
    "print(f'MAE: {MAE}')\n",
    "\n",
    "MAPE = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "print(f'MAPE: {MAPE}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18d929b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(previsao_mensal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "a0a9c9d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotando os resultados\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Linha da série temporal\n",
    "plt.plot(treino_mensal.index, treino_mensal['valor'],color='blue', label='Dados de treino')\n",
    "plt.plot(teste_mensal.index, teste_mensal['valor'],color='green', label='Dados de teste')\n",
    "plt.plot(previsao_mensal.index, previsao_mensal['valor'],color='red', label='Dados futuros reais')\n",
    "\n",
    "\n",
    "# Linha das previsões\n",
    "plt.plot(teste_mensal[2:].index,best_prediction2,label='Previsões futuras',color = 'orange')\n",
    "plt.plot(previsao_mensal.index,prediction_val,label='Previsões testes',color = 'cyan')\n",
    "\n",
    "\n",
    "plt.ylabel('Valores', fontsize=12)\n",
    "plt.xlabel('Datas', fontsize=12)\n",
    "plt.title('Índice Ibovespa mensal previsto com o modelo LSTM')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ffaab73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotando os resultados\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# Linha da série temporal\n",
    "#plt.plot(treino_mensal.index, treino_mensal['valor'],color='blue', label='Dados de treino')\n",
    "plt.plot(teste_mensal.index, teste_mensal['valor'],color='green', label='Dados de teste')\n",
    "plt.plot(previsao_mensal.index, previsao_mensal['valor'],color='red', label='Dados futuros reais')\n",
    "\n",
    "\n",
    "# Linha das previsões\n",
    "plt.plot(teste_mensal[2:].index,best_prediction2,label='Previsões futuras',color = 'orange')\n",
    "plt.plot(previsao_mensal.index,prediction_val,label='Previsões testes',color = 'cyan')\n",
    "\n",
    "\n",
    "plt.ylabel('Valores', fontsize=12)\n",
    "plt.xlabel('Datas', fontsize=12)\n",
    "plt.title('Índice Ibovespa mensal previsto com o modelo LSTM')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8e3afe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
